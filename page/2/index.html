<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Wenhua">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Wenhua">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Wenhua">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>Wenhua</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wenhua</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/precision和accuracy的区别/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/precision和accuracy的区别/" itemprop="url">precision和accuracy的区别</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-12T11:38:19+08:00">
                2019-04-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="confusion-matrix"><a href="#confusion-matrix" class="headerlink" title="confusion matrix"></a>confusion matrix</h3><p><img src="/precision和accuracy的区别/微信图片_20190411171859.png" alt=""></p>
<p>True positive(TP), 真正例, 被正确的划分为正例的个数, 即实际为正例且被分类器划分为正例的实例数/样本数<br>True negative(TN), 真负例, 被正确的划分为负例的个数, 即实际为负例且被分类器划分为负例的实例数<br>False positive(FP), 假正例, 被错误的划分为正例的个数, 即实际为负例且被分类器划分为正例的实例数<br>False negative(FN), 假负例, 被错误的划分为负例的个数, 即实际为正例且被分类器划分为负例的实例数<br><a href="https://www.cnblogs.com/mxp-neu/articles/5316989.html" target="_blank" rel="noopener">参考</a></p>
<p>实际值为正例的是True positive(TP)和False negative(FN)<br>实际值为负例的是True negative(TN)和False positive(FP)<br>预测值为正例的是True positive(TP)和False positive(FP)<br>预测值为负例的是True negative(TN)和False negative(FN)</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th>预测值</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr>
<td>实际值</td>
<td>正例</td>
<td>true positive(TP)真正例</td>
<td>false negative(FN)假负例</td>
</tr>
<tr>
<td>实际值</td>
<td>负例</td>
<td>false positive(FP)假正例</td>
<td>true negative(TN)真负例</td>
</tr>
</tbody>
</table>
</div>
<p>还可以用如下的方式进行理解<br>正例记为+1, 负例记为-1, true用+1表示, false用-1表示, 那么</p>
<script type="math/tex; mode=display">True positive(TP) = 1 * 1 = 1, 实际值为正例</script><script type="math/tex; mode=display">True negative(TN) = 1 * (-1) = -1, 实际值为负例</script><script type="math/tex; mode=display">False positive(FP) = (-1) * 1 = -1, 实际值为负例</script><script type="math/tex; mode=display">False negative(FN) = (-1) * (-1) = 1, 实际值为正例</script><h3 id="difference-between-accuracy-and-precision"><a href="#difference-between-accuracy-and-precision" class="headerlink" title="difference between accuracy and precision"></a>difference between accuracy and precision</h3><p><img src="/precision和accuracy的区别/微信图片_20190412151536.png" width="300px"></p>
<p>In the fields of engineering, industry and statistics, the accuracy of a measurement system is the degree of closeness of measurements of aquantity to its actual (true) value. The precision of a measurement system, also called reproducibility or repeatability, is the degree to which repeated measurements under unchanged conditions show the same results.[1] Although the two words can be synonymous in colloquial use, they are deliberately contrasted in the context of the scientific method.</p>
<p>Accuracy(准确度), is the difference between the measured value and the true value of a tested material.<br>Precision(精确度), is the repeatability of successive measurements under the same conditions.</p>
<p>Accuracy is how close a measured value is to the actual(true) value.<br>Precision is how close the measured values are to each other.</p>
<p>Accuracy, 指在一定实验条件下多次测试的平均值和真实值相符合的程度, 以误差表示<br>Precision, 指多次重复测试同一量时各测定值之间彼此相符合的程度, 表征测定过程中随机误差的大小</p>
<p><img src="/precision和accuracy的区别/微信图片_20190412152554.png" width="300px"></p>
<p>Accuracy = 被正确划分的样本/所有预测的样本 =(TP + TN) / (TP + FP + TN + FN), 通常来说, 正确率越高, 分类器越好<br>Precision = 真正例/预测为正例的样本 = TP / (TP + FP), 实际值为正例的样本占预测值为正例的比例<br>recall = 真正例/实际值为正例 = TP / (TP + FN), 召回率是覆盖面的度量, 实际值为正例的样本占实际值的比例, 召回率也叫做查全率</p>
<script type="math/tex; mode=display">\displaystyle Precision = \frac{TP}{TP+FP} = \frac{正确被检索为正例的数量}{被检索为正例的数量}</script><p>用来描述被检索为正例的数量有多少是被正确检索的</p>
<script type="math/tex; mode=display">\displaystyle Recall = \frac{TP}{TP+FN} = \frac{正确被检索为正例的数量}{应该被检索为正例的数量}</script><p>用来描述所有希望被检索的正例对象有多少被检索到了</p>
<p><a href="https://blog.csdn.net/hit_chenpeng/article/details/54426564" target="_blank" rel="noopener">理解precision和recall的另外一种角度</a></p>
<h3 id="ROC-AUC和F-measure"><a href="#ROC-AUC和F-measure" class="headerlink" title="ROC, AUC和F-measure"></a>ROC, AUC和F-measure</h3><p>ROC(Receiver Operating Characteristic), 受试者工作特征曲线, 在于曲线上各点反映着相同的感受性, 都是对同一信号刺激的反应, 只不过是在几种不同判定标准下得到的结果而已.<br>ROC反映了敏感性和特异性连续变量的综合指标, 用构图法揭示敏感性和特异性之间的相互关系, 通过将连续变量设定出多个不同的临界值, 从而计算出一系列敏感性和特异性, 再以敏感性为纵坐标, (1-特异性)为横坐标</p>
<script type="math/tex; mode=display">TPR = TP/(TP+FN) = recall = sensitivity</script><script type="math/tex; mode=display">FPR = FP/(FP+TN)</script><script type="math/tex; mode=display">TNR = TN/(FP+TN) = specificity = 1 - FPR</script><p>纵轴是TPR(True Positive Rate), 横轴是1 - FPR(False Positive Rate)</p>
<p>AUC是ROC曲线下面积(Area Under roc Curve)的简称, 顾名思义, AUC的值就是处于ROC curve下方的那部分面积的大小. 通常, AUC的值介于0.5到1.0之间, AUC越大, 诊断准确性越高. 在ROC曲线上, 最靠近坐标图左上方的点为敏感性和特异性均较高的临界值. </p>
<p><a href="https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/" target="_blank" rel="noopener">Measuring Object Detection models - mAP - What is Mean Average Precision?</a></p>
<script type="math/tex; mode=display">\displaystyle F = \frac{Precision \ast Recall \ast 2}{Precision + Recall}</script><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p><img src="/precision和accuracy的区别/微信图片_20190414102547.png" width="300px"></p>
<h3 id="练习1"><a href="#练习1" class="headerlink" title="练习1"></a>练习1</h3><p>y_pred = [0, 1, 0, 0]<br>y_true = [0, 1, 0, 1]</p>
<p>真实值中a=2个1, b=2个0<br>预测值中c=1个1, d=3个0</p>
<p>TP = a - c = 1<br>TN = b = 2<br>FP = 0<br>FN = d - b = 1</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>True condition</th>
<th>True condition</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Total population</td>
<td>Condition positive</td>
<td>Condition negative</td>
</tr>
<tr>
<td>Predicted condition</td>
<td>Predicted condition positive</td>
<td>TP=1</td>
<td>FP=0</td>
</tr>
<tr>
<td>Predicted condition</td>
<td>Predicted condition negative</td>
<td>FN=1</td>
<td>TN=2</td>
</tr>
</tbody>
</table>
</div>
<p>precision = TP/(TP+FP) = 1/1 = 1<br>recall = TP/(TP+FN) = 1/(1+1) = 0.5<br>acc = (TP+TN)/(TP+TN+FP+FN) = (1+2)/(1+1+2)=0.75</p>
<p>上面是手算的结果</p>
<p>直接用程序如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import metrics</span><br><span class="line"></span><br><span class="line">y_pred = [0, 1, 0, 0]</span><br><span class="line">y_true = [0, 1, 0, 1]</span><br><span class="line"></span><br><span class="line">accuracy_score = metrics.accuracy_score(y_true, y_pred)</span><br><span class="line">recall_score = metrics.recall_score(y_true, y_pred)</span><br><span class="line">precision_score = metrics.precision_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">print(accuracy_score)</span><br><span class="line">print(precision_score)</span><br><span class="line">print(recall_score)</span><br></pre></td></tr></table></figure></p>
<h3 id="练习2"><a href="#练习2" class="headerlink" title="练习2"></a>练习2</h3><p>假如某个班级有男生80人,女生20人,共计100人.目标是找出所有女生.<br>现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了.<br>作为评估者的你需要来评估(evaluation)下他的工作</p>
<p>真实值中, 被正确的分为女生的为TP=20, 被正确的分为男生的为TN=50<br>预测值中, 被错误的分为女生的为FP=30, 被错误的分为男生的为FN=0</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>True condition</th>
<th>True condition</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>condition positive</td>
<td>condition negative</td>
</tr>
<tr>
<td>Predicted condition</td>
<td>Predicted condition positive</td>
<td>TP=20</td>
<td>FP=30</td>
</tr>
<tr>
<td>Predicted condition</td>
<td>Predicted condition negative</td>
<td>FN=0</td>
<td>TN=50</td>
</tr>
</tbody>
</table>
</div>
<p>precision = TP/(TP+FP) = 20/(20+30) = 0.4<br>recall = TP/(TP+FN) = 20/(20+0) = 1<br>acc = (TP+TN)/(all) = (20+50)/(100) = 0.7<br>F1_measure = (0.4<em>1</em>2)/(0.4+1) = 0.5714</p>
<p><a href="https://www.cnblogs.com/sddai/p/5696870.html" target="_blank" rel="noopener">例子</a></p>
<h3 id="练习3"><a href="#练习3" class="headerlink" title="练习3"></a>练习3</h3><p>假设一个班级有100个学生，其中男生70人，女生30人。不知道这些学生的性别，只知道他们的身高和体重。我们有一个程序(分类器)，这个程序可以通过分析每个学生的身高和体重，对这100个学生的性别分别进行预测。最后的预测结果为，60人为男生，40人为女生</p>
<p>有两种方法来区分</p>
<h4 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h4><p>设正例为男生, 那么负例就是女生<br>TP真正例, 实际值为男生, 预测值为男生<br>TN真负例, 实际值为女生, 预测值为女生<br>FP假正例, 实际值为女生, 预测值为男生<br>FN假负例, 实际值为男生, 预测值为女生</p>
<p>TP + FN = 70<br>TN + FP = 30<br>TP + FP = 60<br>TN + FN = 40</p>
<p>计算过程如下<br>TP, TN, FP, FN<br>1, 0, 0, 1, 70<br>0, 1, 0, 1, 40<br>0, 0, 1, -1, -10<br>0, 0, 0, 0, 0</p>
<p>FN - FP = 10</p>
<p>当FP=20, FN=30, TN=10, TP=40<br>acc = (TP+TN)/all = 50/100<br>precision = TP/(TP+FP) = 40/60<br>recall = TP/(TP+FN) = 40/70</p>
<p>当FP=10, FN=20, TN=20, TP=50<br>acc = (TP+TN)/all = 70/100<br>precision = TP/(TP+FP) = 50/60<br>recall = TP/(TP+FN) = 50/70</p>
<h4 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h4><p>设正例为女生, 那么负例就是男生<br>TP真正例, 实际值为女生, 预测值为女生<br>TN真负例, 实际值为男生, 预测值为男生<br>FP假正例, 实际值为男生, 预测值为女生<br>FN假负例, 实际值为女生, 预测值为男生</p>
<p>TN+FP=70<br>TP+FN=30<br>TN+FN=60<br>TP+FP=40</p>
<p>TP TN FP FN b<br>0  1  1  0  70<br>1  0  0  1  30<br>0  1  0  1  60<br>1  0  1  0  40</p>
<p>1  0  0  1  30<br>1  0  1  0  40<br>0  1  1  0  70<br>0  1  0  1  60</p>
<p>1  0  0  1  30<br>0  1  1  0  70<br>0  0  1 -1  10<br>0  0  0  0   0</p>
<p>TP TN FP FN b<br>1  0  0  1  30<br>0  1  0  1  60<br>0  0  1 -1  10<br>0  0  0  0   0</p>
<p>TP=30-FN<br>TN=60-FN<br>FP=10+FN</p>
<p>令FN=10, TP=20, TN=50, FP=20<br>acc = (TP+TN)/all = 70/100<br>precision = TP/(TP+FP) = 20/40<br>recall = TP/(TP+FN) = 20/30</p>
<p>令FN=20, TP=10, TN=40, FP=30<br>acc = (TP+TN)/all = 50/100<br>precision = TP/(TP+FP) = 10/40<br>recall = TP/(TP+FN) = 10/30</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/梯度下降/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/梯度下降/" itemprop="url">梯度下降</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-17T10:46:11+08:00">
                2019-03-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是常用的卷积神经网络模型参数的求解方法, 根据每次参数更新使用样本数量的多少, 可以分为以下三类:</p>
<ul>
<li>批量梯度下降(batch gradient descent, BGD)</li>
<li>小批量梯度下降(mini-batch gradient descent, MBGD)</li>
<li>随机梯度下降(stochastic gradient descent, SGD)</li>
</ul>
<p>求参数过程即最小化损失函数过程, 比如有一个含有D个训练数据的数据集, 损失函数为:</p>
<script type="math/tex; mode=display">L(W) = \displaystyle \frac{1}{\vert D \vert} \sum_{i}^{\vert D \vert} f_W(x^{(i)}) + \lambda \gamma(W)</script><p>$f_W(X^{(i)})$是单个样本$X^{(i)}$的损失, $\gamma(W)$是正则项, $\lambda$是权重</p>
<h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>量梯度下降(batch gradient descent, BGD), 是梯度下降法最原始的形式, 具体思路是在更新每一组参数的时候都使用所有的样本进行更新, 如果是几百万个样本集的话, 训练时间和内存将都不可取</p>
<p><img src="/梯度下降/WechatIMG71.png" width="300px"></p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>随机梯度下降(stochastic gradient descent, SGD), 是为了解决批量梯度下降法在训练过程中随着样本数量的增加而变得异常缓慢的问题提出的</p>
<p><img src="/梯度下降/WechatIMG72.png" width="300px"></p>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>批量梯度下降和随机梯度下降都有各自的优缺点, 不能取得性能和准确率之间的平衡, 而小批量梯度下降(mini-batch gradient descent, MBGD), 是在每次更新参数时时使用b个样本(b一般为较小的数, 如100)</p>
<p><img src="/梯度下降/WechatIMG73.png" width="300px"></p>
<h2 id="梯度下降法-L8"><a href="#梯度下降法-L8" class="headerlink" title="梯度下降法_L8"></a>梯度下降法_L8</h2><p>梯度下降法(Gradient Descent, GD)常用于求解无约束情况下凸函数(Convex Function)的极小值, 是一种迭代类型的算法, 因为凸函数只有一个极值点, 故求解出来的极小值就是函数的最小值点</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \displaystyle \sum_{i=1}^m ( h_\theta(x^{(i)}) - y^{(i)} )^2</script><script type="math/tex; mode=display">\theta^\ast = arg \min \limits_{\theta} J(\theta)</script><p>导数: 一个函数在某一点的导数描述类这个函数在这一点附近的变化率, 也可以认为是函数在某一点的导数就是该函数所代表的曲线在这一点的切线斜率. 导数值越大, 表示函数在该点处的变化越大.</p>
<p>梯度: 梯度是一个向量, 表示某一函数在该点处的方向导数沿着该方向取的最大值, 即函数在该点处沿着该方向变化最快, 变化率最大(即该梯度向量的模); 当函数为一维函数的时候, 梯度就是导数</p>
<p><img src="/梯度下降/WechatIMG145.png" width="500px"></p>
<script type="math/tex; mode=display">y' = \frac{\partial f(x)}{\partial x}</script><script type="math/tex; mode=display">\nabla f(x_1, x_2) = ( \frac{ \partial f(x_1, x_2) }{\partial x_1}, \frac{\partial f(x_1, x_2)}{\partial x_2})</script><p><img src="/梯度下降/WechatIMG146.png" width="300px"></p>
<p>梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向, 因为该方向为当前位置的最快下降方向, 所以梯度下降法也称为”最速下降法”. 梯度下降法中越接近目标值, 变量变化越小, 如下:</p>
<script type="math/tex; mode=display">\theta^{k+1} = \theta^k - \alpha \nabla f(\theta^k)</script><p>$\alpha$称为步长或者学习率(learning rate), 表示自变量$x$每次迭代变化的大小</p>
<p>收敛条件: 当目标函数的函数值变化非常小的时候或者达到最大迭代次数的时候, 就结束循环</p>
<p>由于梯度下降法中负梯度方向作为变量的变化方向, 所有有肯能导致最终解的值是局部最优解, 所以在使用梯度下降的时候, 一般需要进行一些调优策略</p>
<ul>
<li>学习率的选择: 学习率过大, 表示每次迭代更新的时候变化比较大, 有可能会跳过最优解; 学习率过小, 表示每次迭代更新的时候变化比较小, 就会导致迭代速度过慢, 很长时间都不能结束 </li>
<li>算法初始参数值的选择: 初始值不同, 最终获得的最小值也有可能不同, 因为梯度下降法求解的是局部最优解, 所以一般情况下, 选择多次不同初始值运行算法, 并最终返回损失函数最小情况下的结果值</li>
<li>标准化: 由于样本不同特征的取值范围不同, 可能导致在各个不同参数上迭代速度不同, 为了减少特征取值的影响, 可以将特征进行标准化操作</li>
</ul>
<h3 id="批量梯度下降法-Batch-Gradient-Descent-BGD"><a href="#批量梯度下降法-Batch-Gradient-Descent-BGD" class="headerlink" title="批量梯度下降法(Batch Gradient Descent, BGD)"></a>批量梯度下降法(Batch Gradient Descent, BGD)</h3><p>使用所有样本在当前点的梯度值对变量参数进行更新操作</p>
<script type="math/tex; mode=display">\theta^{k+1} = \theta^k - \alpha \displaystyle \sum_{i=1}^m \nabla f_{\theta^k} (x_i)</script><h3 id="随机梯度下降法-Stochastic-Gradient-Descent-SGD"><a href="#随机梯度下降法-Stochastic-Gradient-Descent-SGD" class="headerlink" title="随机梯度下降法(Stochastic Gradient Descent, SGD)"></a>随机梯度下降法(Stochastic Gradient Descent, SGD)</h3><p>在更新变量参数的时候, 选取一个样本的梯度值来更新参数</p>
<script type="math/tex; mode=display">\theta^{k+1} = \theta^k - \alpha \nabla f_{\theta^k} (x_i)</script><h3 id="小批量梯度下降法-Mini-batch-Gradient-Descent-MBGD"><a href="#小批量梯度下降法-Mini-batch-Gradient-Descent-MBGD" class="headerlink" title="小批量梯度下降法(Mini-batch Gradient Descent, MBGD)"></a>小批量梯度下降法(Mini-batch Gradient Descent, MBGD)</h3><p>集合BGD和SGD的特性, 从原始数据中, 每次选择n个样本来更新参数值, 一般n选择10</p>
<script type="math/tex; mode=display">\theta^{k+1} = \theta^k - \alpha \displaystyle \sum_{i=t}^{t+n-1} \nabla f_{\theta^k} (x_i)</script><h3 id="BGD-SGD-MBGD的区别"><a href="#BGD-SGD-MBGD的区别" class="headerlink" title="BGD/SGD/MBGD的区别"></a>BGD/SGD/MBGD的区别</h3><ul>
<li>当样本量为m的时候, 每次迭代BGD算法中对于参数值更新一次, SGD算法中对于参数值更新m次, MBGD算法中对于参数值更新$m/n$次, 相对来讲SGD算法的更新速度最快</li>
<li>SGD算法中对于每个样本都需要更新参数值, 当样本值不太正常的时候, 就有可能会导致本次的参数更新会产生相反的影响, 也就是说SGD算法的结果并不是完全收敛的, 而是在收敛结果处波动的</li>
<li>SGD算法是每个样本都更新一次参数值, 所以SGD算法特别适合样本数据量大的情况以及在线机器学习(Online ML)</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/L9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/L9/" itemprop="url">L9</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T16:00:57+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h2><p>设$\{X(t), t \in T\}$是一个随机过程, E是其状态空间, 若对于任意的$t_1 &lt; t_2 &lt; \cdots &lt; t_n &lt; t$, 任意的$x_1 &lt; t_2 &lt; \cdots &lt; t_n &lt; t$, 任意的$x_1, x_2, \cdots, x_n, x \in E$, 随机变量$X(t)$在已知变量$X(t_1) = x_1, \cdots, X(t_n) = x_n$下的条件分布函数只与$X(t_n)=x_n$有关, 而与$X(t_1)=x_1, \cdots, X(t_{n-1})=x_{n-1}$无关, 即条件分布函数满足下列等式, 此性质称为马尔可夫性质; 如果随机过程满足马尔可夫性质, 则该过程称为马尔可夫过程.</p>
<script type="math/tex; mode=display">p(X(t) \le x | X(t_1)=x_1, \cdots, X(t_n)=x_n) = p(X(t) \le x | X(t_n) = x_n)</script><script type="math/tex; mode=display">p(X_{n+1}=x| X_1=x_1, \cdots, X_n=x_n) = p(X_{n+1} | X_n = x_n)</script><h2 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h2><p>马尔可夫链是指具有马尔可夫性质的随机过程. 在过程中, 在给定当前信息的情况下, 过去的信息状态对于预测未来状态是无关的.</p>
<p>在马尔可夫链的每一步, 系统根据概率分布, 可以从一个状态变成另外一个状态, 也可以保持当前状态不变. 状态的改变叫做转移, 状态改变的相关概率叫做转移概率</p>
<p>马尔可夫链中的三元素是: 状态空间S, 转移概率矩阵P, 初始概率分布$\pi$</p>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>隐马尔可夫模型(Hidden Markov Model, HMM)是一种统计模型, 在语音识别, 行为识别, NLP, 故障诊断等领域具有高效的性能.</p>
<p>HMM是关于时序的概率模型, 描述一个含有未知参数的马尔可夫链所生成的不可观测的状态随机序列, 再由各个状态生成观测随机序列的过程. HMM是一个双重随机过程, 具有一定状态的隐马尔可夫链和随机的观测序列.</p>
<p>HMM随机生成的状态随机序列被称为状态序列; 每个状态生成一个观测, 由此产生的观测随机序列被称为观测序列.</p>
<p><img src="/L9/WechatIMG2.png" width="700px"></p>
<p>$z_1, z_2, \cdots, z_n$是不可观测的状态, $x_1, x_2, \cdots, x_n$是可观测到的序列; 不可观测的状态决定可观测序列的值(z的取值决定x的取值)</p>
<p>HMM由隐含状态S, 可观测状态O, 初始状态概率矩阵$\pi$, 隐含状态转移概率矩阵A, 可观测值转移概率矩阵B(也称为混淆矩阵, Confusion Matrix)</p>
<p>$\pi$和A决定了状态序列, B决定观测序列, 因此HMM可以使用三元符号表示, 称为HMM三元素</p>
<script type="math/tex; mode=display">\lambda = (A, B, \pi)</script><h3 id="HMM参数说明"><a href="#HMM参数说明" class="headerlink" title="HMM参数说明"></a>HMM参数说明</h3><p>HMM模型 <script type="math/tex">\lambda = (A, B, \pi)</script></p>
<p>S是所有可能的状态集合</p>
<script type="math/tex; mode=display">S = \{ S_1, S_2, \cdots, S_n\}</script><p>O是所有可能的观测集合</p>
<script type="math/tex; mode=display">O = \{ O_1, O_2, \cdots, O_n \}</script><p>I是长度为T的状态序列, Q对应的观测序列</p>
<script type="math/tex; mode=display">I = \{ i_1, i_2, \cdots, i_T \} \qquad Q = \{ q_1, q_2, \cdots, q_T \}</script><p>A是隐含状态转移概率矩阵</p>
<script type="math/tex; mode=display">A = [a_{ij}]_{n \ast n} = \begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{n1}}&{a_{n2}}&{\cdots}&{a_{nn}}\\
\end{bmatrix}</script><p>其中$a_{ij} = a_{i_t i_{t+1}}= p(i_{t+1}=s_j | i_t = s_i)$, 表示在时刻$t$处于状态序列中$s_i$状态的条件下时刻$t+1$转移到状态$s_j$的概率</p>
<p>B是可观测值转移概率矩阵</p>
<script type="math/tex; mode=display">B = [b_{ij}]_{n \ast m} = \begin{bmatrix}
{b_{11}}&{b_{12}}&{\cdots}&{b_{1m}}\\
{b_{21}}&{b_{22}}&{\cdots}&{b_{2m}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{b_{n1}}&{b_{n2}}&{\cdots}&{b_{nm}}\\
\end{bmatrix}</script><p>其中$b_{ij} = b_{i_t q_t} = p( q_t = o_j | i_t=s_i)$, 表示在时刻$t$处于状态序列中$s_i$状态的条件下, 生成可观测状态$o_j$的概率</p>
<p>$\pi$是初始状态概率向量</p>
<script type="math/tex; mode=display">\pi = (\pi_i)_{1 \ast n} = (\pi_1, \pi_2, \cdots, \pi_n)</script><p>其中$\pi_{i_1} = p(i_1 = s_i)$, 表示在时刻$t=1$处于状态序列中$s_i$的概率</p>
<h3 id="HMM的两个性质"><a href="#HMM的两个性质" class="headerlink" title="HMM的两个性质"></a>HMM的两个性质</h3><script type="math/tex; mode=display">p(i_{t} | i_{t-1}, q_{t-1}, i_{t-2}, q_{t-2}, \cdots, i_1, q_1) =  p(i_{t} | i_{t-1})</script><script type="math/tex; mode=display">p(q_{t} | i_t, i_{t-1}, q_{t-1}, i_{t-2}, q_{t-2}, \cdots, i_1, q_1) = p(q_{t} | i_t)</script><p>看这个意思就是当前状态的概率只和上一个状态的概率有关, 与其他都没有关系</p>
<h2 id="HMM案例"><a href="#HMM案例" class="headerlink" title="HMM案例"></a>HMM案例</h2><p>假设有三个盒子, 编号分别为1, 2, 3; 每个盒子都装有黑白两种颜色的小球, 小球的比例如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>白球</th>
<th>黑球</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>4</td>
<td>6</td>
</tr>
<tr>
<td>2</td>
<td>8</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>按照$\pi$的概率选择一个盒子, 从盒子中随机抽取一个小球, 记录颜色后, 放回盒子中</li>
<li>按照某种条件概率选择新的盒子, 重复该操作</li>
<li>最终得到的观测序列为: “白黑白白黑”</li>
</ul>
<p>对该题目的理解如下:</p>
<p>有放回的按照概率$\pi$来抽取, 假如第一次抽取的是1号盒子, 第二次抽取的是3号盒子, 接下来是2号, 2号, 3号, 举个例子, 所以</p>
<p>3号 -&gt; 2号 -&gt; 1号 -&gt; 1号 -&gt; 2号, 状态序列<br>白球 -&gt; 黑球 -&gt; 白球 -&gt; 白球 -&gt; 黑球, 观测序列</p>
<h2 id="HMM的的三个问题"><a href="#HMM的的三个问题" class="headerlink" title="HMM的的三个问题"></a>HMM的的三个问题</h2><ul>
<li>概率计算问题<br>给定模型$\lambda=(A, B, \pi)$和观测序列$Q = \{q_1, q_2, \cdots, q_T\}$, 利用前向-后向算法计算模型$\lambda$之下观测到序列Q出现的概率</li>
<li>学习问题<br>已知观测序列$Q = \{q_1, q_2, \cdots, q_T\}$, 使用Baum-Welch算法估计模型$\lambda=(A, B, \pi)$的参数, 使得在该模型下观测序列$p(Q|\lambda)$最大</li>
<li>预测问题<br>给定模型$\lambda=(A, B, \pi)$和观测序列$Q = \{q_1, q_2, \cdots, q_T\}$, 利用Viterbi算法求解给定观测序列条件概率$p(I|Q, \lambda)$的最大状态序列$I$</li>
</ul>
<h3 id="概率计算问题"><a href="#概率计算问题" class="headerlink" title="概率计算问题"></a>概率计算问题</h3><h4 id="直接计算法-暴力算法"><a href="#直接计算法-暴力算法" class="headerlink" title="直接计算法/暴力算法"></a>直接计算法/暴力算法</h4><p>给定模型$\lambda=(A, B, \pi)$和观测序列$Q = \{q_1, q_2, \cdots, q_T\}$, 计算在模型$\lambda$的情况下, 得到观测序列Q出现的概率$p(Q; \lambda)$</p>
<p>要想得到观测序列”白球 -&gt; 黑球 -&gt; 白球 -&gt; 白球 -&gt; 黑球”这样的结果, 又因为每个盒子都是有白球和黑球的, 然而我们并不知道状态序列及抽取的盒子的序列是什么样的, 所以如下:</p>
<p>按照概率公式, 列举所有可能的长度为T的状态序列$I = \{ i_1, i_2, \cdots, i_T\}$, 先求出各个状态序列与观测序列$Q=\{q_1, q_2, \cdots, q_T\}$的联合概率$P(Q,I; \lambda)$, 然后对所有可能的状态序列求和, 从而得到最后的概率$p(Q;\lambda)$</p>
<script type="math/tex; mode=display">I = \{ i_1, i_2, \cdots, i_T\} \qquad p(I;\lambda) = \pi_{i_1} a_{i_1 i_2} a_{i_2 i_3} \cdots a_{i_{T-1} i_T}</script><script type="math/tex; mode=display">Q = \{ q_1, q_2, \cdots, q_T\} \qquad p(Q | I;\lambda) = b_{i_1 q_1} b_{i_2 q_2} \cdots b_{i_T q_T}</script><script type="math/tex; mode=display">p(Q, I; \lambda) = p(Q | I;\lambda) p(I;\lambda) = \pi_{i_1} a_{i_1 i_2 } a_{i_2 i_3} \cdots a_{i_{T-1} i_T} \ast b_{i_1 q_1} b_{i_2 q_2} \cdots b_{i_T q_T}</script><script type="math/tex; mode=display">p(Q; \lambda) = \sum_I p(Q, I; \lambda) = \sum_I \pi_{i_1} a_{i_1 i_2 } a_{i_2 i_3} \cdots a_{i_{T-1} i_T} \ast b_{i_1 q_1} b_{i_2 q_2} \cdots b_{i_T q_T}</script><p>假设抽取盒子的状态序列 $I = 3号, 2号, 1号, 1号, 2号$, 那么该状态序列的长度$T = 5$</p>
<script type="math/tex; mode=display">\therefore p(I; \lambda) = \pi_3 a_{32} a_{21} a_{11} a_{12}</script><script type="math/tex; mode=display">\therefore p(Q | I; \lambda) = b_{3白} b_{2黑} b_{1白} b_{1白} b_{2黑}</script><script type="math/tex; mode=display">\therefore p(Q, I; \lambda) = p(Q | I;\lambda) p(I;\lambda) = \lambda) = \pi_3 a_{32} a_{21} a_{11} a_{12} \ast b_{3白} b_{2黑} b_{1白} b_{1白} b_{2黑}</script><p>上述只是求出了一种盒子的状态序列, 因为每个盒子里边既有白球又有黑球, 所以盒子的状态序列总共有$3^5$种可能.</p>
<p>然后对所有可能的状态序列求和</p>
<h4 id="前向概率-后向概率"><a href="#前向概率-后向概率" class="headerlink" title="前向概率/后向概率"></a>前向概率/后向概率</h4><p><img src="/L9/微信图片_20190815140634.png" width="500px"></p>
<p>$\alpha_t$是给定$q_t$, 时刻1到时刻t所有观测值$y_1, y_2, \cdots, y_t, q_t$出现的联合概率<br>$\beta_t$是给定$q_t$, 时刻t+1到时刻T, 所有观测值$y_{t+1}, y_{t+2}, \cdots, y_T$出现的联合概率</p>
<script type="math/tex; mode=display">\alpha_t(i) = p(y_1, y_2, \cdots, y_t, q_t = i | \lambda)</script><script type="math/tex; mode=display">\beta_t(i) = p(y_{t+1}, y_{t+2}, \cdots, y_T | q_t = i, \lambda)</script><p>前向概率-后向概率指的是在一个观测序列中, 时刻t对应状态$s_i$的概率值转换过来的信息</p>
<script type="math/tex; mode=display">\begin{align} p(q_1, q_2, \cdots, q_T, i_t=s_i) &= p(q_1, q_2, \cdots, q_{t}, q_{t+1}, q_{t+2}, \cdots, q_T, i_t=s_i) \\
&= p(q_1, q_2, \cdots, q_t, i_t=s_i) \ast p(q_{t+1}, q_{t+2}, \cdots, q_T | q_1, q_2, \cdots, q_t, i_t=s_i)\\
&= p(q_1, q_2, \cdots, q_t, i_t=s_i) \ast p(q_{t+1}, q_{t+2}, \cdots, q_T | i_t=s_i ) \\
&= p(q_1, q_2, \cdots, q_t, i_t=s_i; \lambda) \ast p(q_{t+1}, q_{t+2}, \cdots, q_T | i_t=s_i; \lambda) \\
\end{align}</script><h4 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h4><p>给定$\lambda$, 定义前向概率为, 在时刻t观测序列为$q_1, q_2, \cdots, q_t$, 状态序列的当前状态为$s_i$的概率, 记为:</p>
<script type="math/tex; mode=display">\alpha_t(i) = p(q_1, q_2, \cdots, q_t, i_t=s_i; \lambda)</script><p>初值$\alpha_1(i) = p(q_1, i_1=s_i; \lambda) = \pi_i b_{i q_1}$, 表示在时刻$t=1$, 观测序列为$q_1$和状态序列的当前状态为$s_i$的联合概率</p>
<script type="math/tex; mode=display">\begin{align} \alpha_1(i) &= p(q_1, i_1=s_i; \lambda)\\
&= p(i_1=s_1 | q_1; \lambda) \ast p(q_1; \lambda) \\
&= p(i_1=s_1; \lambda) \ast p(q_1 = o_j| i_1 = s_i; \lambda) \\
&= \pi_{i_1} b_{i_1 q_1}
\end{align}</script><p>递推: 对于$t=1, 2, \cdots, T-1$</p>
<script type="math/tex; mode=display">\alpha_{t+1}(i) = (\displaystyle \sum_{j=1}^n \alpha_t(j) a_{ji}) b_{i q_{t+1}}</script><p>最终</p>
<script type="math/tex; mode=display">p(Q; \lambda) = \displaystyle \sum_{i=1}^n \alpha_T(i)</script><script type="math/tex; mode=display">\begin{align} \alpha_t(i) &= p(q_1, q_2, \cdots, q_{t-1}, q_t, i_t=s_i) \\
&= p(q_1, q_2, \cdots, q_{t-1}, i_t = s_i) \ast p(q_t | q_1, q_2, \cdots, q_{t-1}, i_t = s_i) \\
&= p(q_1, q_2, \cdots, q_{t-1}, i_t = s_i) \ast p(q_t | i_t = s_i) \\
&= [\displaystyle \sum_{j=1}^n p(q_1, q_2, \cdots, q_{t-1}, i_{t-1} = s_j, i_t = s_i)] \ast p(q_t | i_t = s_i) \\
&= [\displaystyle \sum_{j=1}^n p(q_1, q_2, \cdots, q_{t-1}, i_{t-1} = s_j) \ast p( i_t = s_i | i_{t-1} = s_j)] \ast p(q_t | i_t = s_i) \\
&= (\displaystyle \sum_{j=1}^n \alpha_{t-1}(j) a_{ji}) \ast b_{i_t q_t}  \\
\end{align}</script><h4 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h4><p>给定$\lambda$, 定义后向概率为, 在时刻t状态序列的当前状态为$s_i$的前提下, 从时刻$t+1$到时刻T部分的观测序列为$q_{t+1}, q_{t+2}, \cdots, q_T$的概率, 记为:</p>
<script type="math/tex; mode=display">\beta_t(i) = p(q_{t+1}, q_{t+2}, \cdots, q_T | i_t=s_i; \lambda)</script><p>初值: $\beta_T(i) = 1$, 根据$\beta_t(i)$的公式, 令$t=T$, 那么整个状态序列和观测序列总共只有T个时刻, 是不可能有$T+1$时刻的, 所以$T+1$时刻之后发生的是必然事件, 所以$\beta_T(i) = 1$<br>递推: 对于$t = T-1, T-2, \cdots, 1$, 倒着推导</p>
<script type="math/tex; mode=display">\beta_t(i) = \displaystyle \sum_{j=1}^n (a_{ij} b_{j q_{t+1}} \beta_{t+1}(j))</script><p>最终</p>
<script type="math/tex; mode=display">p(Q; \lambda) = \displaystyle \sum_{i=1}^n \pi_i b_{i q_1} \beta_1(i)</script><p>根据上述$\beta_t$的定义, $\beta_t(i)$是给定$q_t$, 时刻$t+1$到时刻$T$, 所有观测值$y_{t+1}, y_{t+2}, \cdots, y_T$的联合概率, 如下图所示:<br><img src="/L9/微信图片_20190817095916.png" width="300px"></p>
<p>那么$\beta_{t+1}(i)$就是给定$q_t$(这里为什么不是$q_{t+1}$), 时刻$t+2$到时刻$T$, 所有观测值$y_{t+2}, \cdots, y_T$的联合概率, 如下图所示:<br><img src="/L9/微信图片_20190817100705.png" width="300px"></p>
<script type="math/tex; mode=display">\begin{align} \beta_t(i) &= p(q_{t+1}, q_{t+2}, \cdots, q_T | i_t=s_i) \\
&= \displaystyle \sum_{j=1}^n p(i_{t+1} = s_j, q_{t+1}, q_{t+2}, \cdots, q_T | i_t = s_i)  \\ 
&= \displaystyle \sum_{j=1}^n p(q_{t+1}, q_{t+2}, \cdots, q_T | i_{t+1} = s_j) \ast p(i_{t+1}=s_j | i_t=s_i) \\ 
&= \displaystyle \sum_{j=1}^n p(q_{t+2}, \cdots, q_T | i_{t+1} = s_j) \ast p(q_{t+1} | i_{t+1}=s_j) \ast p(i_{t+1}=s_j | i_t=s_i) \\ 
&= \displaystyle \sum_{j=1}^n (a_{ij} b_{j q_{t+1}} \beta_{t+1}(j))  \\
\end{align}</script><h4 id="单个状态的概率"><a href="#单个状态的概率" class="headerlink" title="单个状态的概率"></a>单个状态的概率</h4><p>求给定模型$\lambda$和观测序列Q的情况下, 在时刻t处于状态序列中$s_i$的概率, 计作:</p>
<script type="math/tex; mode=display">\gamma_t(i) = p(i_t=s_i|Q;\lambda)</script><p>单个状态概率的意义主要是用于判断在每个时刻最可能存在的状态序列中的状态, 从而可以得到整个状态序列作为最终的预测结果</p>
<script type="math/tex; mode=display">p(i_t=s_i, Q;\lambda) = \alpha_t(i)\beta_t(i)</script><script type="math/tex; mode=display">\begin{align} \gamma_t(i) = p(i_t=s_i|Q;\lambda) &= \frac{p(i_t=s_i, Q; \lambda)}{p(Q;\lambda)} \\
&= \frac{\alpha_t(i)\beta_t(i)}{p(Q;\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\displaystyle \sum_{j=1}^n\alpha_t(j)\beta_t(j)} \\
\end{align}</script><h4 id="两个状态的联合概率"><a href="#两个状态的联合概率" class="headerlink" title="两个状态的联合概率"></a>两个状态的联合概率</h4><p>求给定模型$\lambda$和观测序列Q的情况下, 在时刻t处于状态$s_i$并且在时刻$t+1$处于状态$s_j$概率, 计作:</p>
<script type="math/tex; mode=display">\begin{align} \xi_t(i, j) &= p(i_t=s_i, i_{t+1}=s_j|Q;\lambda) \\
&= \frac{p(i_t=s_i, i_{t+1}=s_j, Q; \lambda)}{p(Q; \lambda)} \\
&= \frac{ p(i_t=s_i, i_{t+1}=s_j, Q; \lambda)  }{ \displaystyle \sum_{i=1}^n \sum_{j=1}^n p(i_t=s_i, i_{t+1}=s_j, Q;\lambda ) }  \\
\end{align}</script><script type="math/tex; mode=display">p(i_t=s_i, i_{t+1}=s_j, Q; \lambda) = \alpha_t(i) a_{ij} b_{j q_{t+1}} \beta_{t+1}(j)</script><h3 id="学习问题"><a href="#学习问题" class="headerlink" title="学习问题"></a>学习问题</h3><p>若训练数据包含观测序列和状态序列, 则HMM的学习问题非常简单, 是监督学习算法<br>若训练数据值包含观测序列, 则HMM的学习问题需要使用EM算法求解, 是非监督学习算法</p>
<h4 id="学习问题-监督学习"><a href="#学习问题-监督学习" class="headerlink" title="学习问题-监督学习"></a>学习问题-监督学习</h4><p>直接利用大数定理的结论”频率的极限是概率”, 直接给出HMM的参数估计</p>
<script type="math/tex; mode=display">\hat \pi_i = \frac{ \vert s_i \vert}{ \displaystyle \sum_{i=1}^n \vert s_i \vert }</script><script type="math/tex; mode=display">a_{ij} = \frac{\vert s_{ij} \vert}{ \displaystyle \sum_{j=1}^n \vert s_{ij} \vert }</script><script type="math/tex; mode=display">\hat b_{ij} = \frac{\vert q_{ij} \vert}{ \displaystyle \sum_{j=1}^n \vert q_{ij} \vert   }</script><h4 id="学习问题-非监督学习"><a href="#学习问题-非监督学习" class="headerlink" title="学习问题-非监督学习"></a>学习问题-非监督学习</h4><h3 id="预测问题"><a href="#预测问题" class="headerlink" title="预测问题"></a>预测问题</h3><h4 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h4><p>直接在每个时刻t时候最有可能的状态作为最终的预测状态, 使用下列公式计算概率值</p>
<script type="math/tex; mode=display">\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{p(Q; \lambda)} = \frac{ \alpha_t(i)\beta_t(i) }{ \displaystyle \sum_{j=1}^n \alpha_t(j)\beta_t(j) }</script><h4 id="viterbi算法"><a href="#viterbi算法" class="headerlink" title="viterbi算法"></a>viterbi算法</h4><p>viterbi算法实际是用动态规划的思路求解HMM预测问题, 求概率最大的”路径”, 每条”路径”对应一个状态序列</p>
<script type="math/tex; mode=display">\delta_t(i) = \max \limits_{i_1, i_2, \cdots, i_{t-1}} p(i_t=i, i_1, i_2, \cdots, i_{t-1}, q_t, q_{t-1}, \cdots, q_1; \lambda)</script><script type="math/tex; mode=display">\delta_1(i) = \pi_i b_{i q_1}</script><script type="math/tex; mode=display">\delta_{t+1}(i) = \max \limits_{ 1 \le j \le n} (\delta_t(j)a_{ji}) b_{i q_{t+1}}</script><script type="math/tex; mode=display">p^{\ast} = \max \limits_{ 1 \le j \le n} \delta_T(i)</script><h4 id="viterbi算法案例"><a href="#viterbi算法案例" class="headerlink" title="viterbi算法案例"></a>viterbi算法案例</h4><p>给定参数$\pi, A, B$, 得到观测序列为”白黑白白黑”, 求出最优的隐藏状态序列</p>
<script type="math/tex; mode=display">\pi = \begin{bmatrix}
{0.2}\\
{0.5}\\
{0.3}\\
\end{bmatrix}</script><script type="math/tex; mode=display">A = \begin{bmatrix}
{0.5}&{0.4}&{0.1}\\
{0.2}&{0.2}&{0.6}\\
{0.2}&{0.5}&{0.3}\\
\end{bmatrix}</script><script type="math/tex; mode=display">B = \begin{bmatrix}
{0.4}&{0.6}\\
{0.8}&{0.2}\\
{0.5}&{0.5}\\
\end{bmatrix}</script><script type="math/tex; mode=display">\delta_1(i) = \pi_i b_{i q_1} = \pi_i b_{i白 }</script><script type="math/tex; mode=display">\delta_1(1) = 0.2 * 0.4 = 0.08 \qquad \delta_1(2) = 0.5 * 0.8 = 0.4 \qquad \delta_1(3) = 0.3 * 0.5 = 0.15</script><script type="math/tex; mode=display">\delta_2(i) = \max \limits_{1 \le j \le n} ( \delta_1(j) a_{ji}) b_{i黑}</script><script type="math/tex; mode=display">\delta_2(1) = max(\delta_1(1)*a_{11}, \delta_1(2)*a_{21}, \delta_1(3)*a_{31}) * b_{1黑} = max(0.08*0.5, 0.4*0.2, 0.15*0.2) * 0.6 = max(0.04, 0.08, 0.03) * 0.6 = 0.08*0.6 = 0.048</script><script type="math/tex; mode=display">\delta_2(2) = max(\delta_1(1)*a_{12}, \delta_1(2)*a_{22}, \delta_1(3)*a_{32}) * b_{2黑} = max(0.08*0.4, 0.4*0.2, 0.15*0.5) * 0.2 = max(0.032, 0.08, 0.075) * 0.2 = 0.08*0.2 = 0.016</script><script type="math/tex; mode=display">\delta_2(3) = max(\delta_1(1)*a_{13}, \delta_1(2)*a_{23}, \delta_1(3)*a_{33}) * b_{3黑} = max(0.08*0.1, 0.4*0.6, 0.15*0.3) * 0.5 = max(0.008, 0.24, 0.045) * 0.5 = 0.24*0.5 = 0.12</script><script type="math/tex; mode=display">\delta_3(i) = \max \limits_{1 \le j \le n} ( \delta_2(j) a_{ji}) b_{i白}</script><script type="math/tex; mode=display">\delta_3(1) = max(\delta_2(1)*a_{11}, \delta_2(2)*a_{21}, \delta_2(3)*a_{31}) * b_{1白} = max(0.048*0.5, 0.016*0.2, 0.12*0.2) * 0.4 = max(0.024, 0.0032, 0.024)*0.4 = 0.024*0.4 = 0.0096</script><script type="math/tex; mode=display">\delta_3(2) = max(\delta_2(1)*a_{12}, \delta_2(2)*a_{22}, \delta_2(3)*a_{32}) * b_{2白} = max(0.048*0.4, 0.016*0.2, 0.12*0.5) * 0.8 = max(0.0192, 0.0032, 0.06)*0.8 = 0.06*0.8 = 0.048</script><script type="math/tex; mode=display">\delta_3(3) = max(\delta_2(1)*a_{13}, \delta_2(2)*a_{23}, \delta_2(3)*a_{33}) * b_{3白} = max(0.048*0.1, 0.016*0.6, 0.12*0.3) * 0.5 = max(0.0048, 0.0096, 0.036)*0.5 = 0.036*0.5 = 0.018</script><script type="math/tex; mode=display">\delta_4(i) = \max \limits_{1 \le j \le n} ( \delta_3(j) a_{ji}) b_{i白}</script><script type="math/tex; mode=display">\delta_4(1) = max(\delta_3(1)*a_{11}, \delta_3(2)*a_{21}, \delta_3(3)*a_{31}) * b_{1白} = max(0.0096*0.5, 0.048*0.2, 0.018*0.2)*0.4 = max(0.0048, 0.0096, 0.0036)*0.4 = 0.0096*0.4 = 0.00384</script><script type="math/tex; mode=display">\delta_4(2) = max(\delta_3(1)*a_{12}, \delta_3(2)*a_{22}, \delta_3(3)*a_{32}) * b_{2白} = max(0.0096*0.4, 0.048*0.2, 0.018*0.5)*0.8 = max(0.00384, 0.0096, 0.009)*0.8 = 0.0096*0.8 = 0.00768</script><script type="math/tex; mode=display">\delta_4(3) = max(\delta_3(1)*a_{13}, \delta_3(2)*a_{23}, \delta_3(3)*a_{33}) * b_{3白} = max(0.0096*0.1, 0.048*0.6, 0.018*0.3)*0.5 = max(0.00096, 0.0288, 0.0054)*0.5 = 0.0288*0.5 = 0.0144</script><script type="math/tex; mode=display">\delta_5(i) = \max \limits_{1 \le j \le n} ( \delta_4(j) a_{ji}) b_{i黑}</script><script type="math/tex; mode=display">\delta_5(1) = max(\delta_4(1)*a_{11}, \delta_4(2)*a_{21}, \delta_4(3)*a_{31}) * b_{1黑} = max(0.00384*0.5, 0.00768*0.2, 0.0144*0.2 )*0.6 = max(0.00192, 0.001536, 0.00288)*0.6 = 0.00288*0.6 = 0.001728</script><script type="math/tex; mode=display">\delta_5(2) = max(\delta_4(1)*a_{12}, \delta_4(2)*a_{22}, \delta_4(3)*a_{32}) * b_{2黑} = max(0.00384*0.4, 0.00768*0.2, 0.0144*0.5 )*0.2 = max(0.001536, 0.001536, 0.0072)*0.2 = 0.0072*0.2 = 0.00144</script><script type="math/tex; mode=display">\delta_5(3) = max(\delta_4(1)*a_{13}, \delta_4(2)*a_{23}, \delta_4(3)*a_{33}) * b_{3黑} = max(0.00384*0.1, 0.00768*0.6, 0.0144*0.3 )*0.5 = max(0.000384, 0.004608, 0.00432)*0.5 = 0.004608*0.5 = 0.002304</script><p>同样的方法继续计算$\delta_3(i), \delta_4(i), \delta_5(i)$, 得到如下:</p>
<script type="math/tex; mode=display">\delta_3(1)=0.0096, \delta_3(2)=0.048, \delta_3(3)=0.018</script><script type="math/tex; mode=display">\delta_4(1)=0.00384, \delta_4(2)=0.00768, \delta_4(3)=0.0144</script><script type="math/tex; mode=display">\delta_5(1)=0.001728, \delta_5(2)=0.00144, \delta_5(3)=0.002304</script><p>从$\delta_5$得知, $\delta_5(3)$最大, 往前推导, 第5个状态是由$\delta_4$的哪个产生的?<br>由$max(0.00384 \ast 0.1, 0.00768 \ast 0.6, 0.0144 \ast 0.3)$<br>得到是由$\delta_4(2)$来的, 依次往回倒推</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5 -&gt; 4 -&gt; 3 -&gt; 2 -&gt; 1</span><br><span class="line">3 -&gt; 2 -&gt; 2 -&gt; 3 -&gt; 2</span><br></pre></td></tr></table></figure>
<p>可以看出盒子的序列为(2, 3, 2, 2, 3)</p>
<p><a href="https://www.jianshu.com/p/c80ca0aa4213" target="_blank" rel="noopener">参考</a><br><a href="https://www.cnblogs.com/pinking/p/8531405.html" target="_blank" rel="noopener">盒子顺序的正确解释</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/L8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/L8/" itemprop="url">L8</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-05T20:56:25+08:00">
                2019-02-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>在线性回归中, 可以通过多项式扩展将低维度扩展成为高维度数据, 从而可以使用线性回归模型来解决问题. 即对于二维空间中不是线性可分的数据, 将其映射到高维空间中后, 变成了线性可分的数据<br><img src="/L8/WechatIMG39.png" width="250px"></p>
<p>两维线性模型: </p>
<script type="math/tex; mode=display">h_\theta(x_1, x_2) = \theta_0 + \theta_1 x_1 + \theta_2 x_2</script><script type="math/tex; mode=display">(x_1, x_2) \quad \underrightarrow{多项式扩展} \quad (x_1, x_2, x_1^2, x_2^2, x_1x_2)</script><p>五维线性模型:</p>
<script type="math/tex; mode=display">h_\theta(x_1, x_2) = \theta_0 + \theta_1 x_1 + \theta_2 x_2  + \theta_3 x_3 + \theta_4 x_4 + \theta_5 x_5</script><script type="math/tex; mode=display">\underrightarrow{等价于} \quad h_\theta(x_1, x_2) = \theta_0 + \theta_1 z_1 + \theta_2 z_2  + \theta_3 z_3 + \theta_4 z_4 + \theta_5 z_5</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>假设函数$\phi$是一个从低维特征空间到高维特征空间到一个映射, 那么如果存在函数$K(x,z)$, 对于任意的低维度特征向量x和z, 都有$K(x, z) = \phi(x) \cdot \phi(z)$, 该函数称为<strong>核函数</strong></p>
<p>核函数在解决线性不可分问题的时候, 采用的方式是: 使用低维特征空间的计算来避免在高维特征空间中向量内积的恐怖计算量, 也就是说此时SVM模型可以应用在高维特征空间中数据可线性分割的优点, 同时也避免来引入这个高维特征空间恐怖的内积计算量</p>
<p>设两个向量$x_1 = (\mu_1, \mu_2)^T$和$x_2 = (\eta_1, \eta_2)^T$, 映射过后的内积为:</p>
<script type="math/tex; mode=display">\phi(x_1) \cdot \phi(x_2) = \mu_1 \eta_1 + \mu_2 \eta_2 + \mu_1^2 \eta_1^2 + \mu_2^2 \eta_2^2 + \mu_1 \mu_2 \eta_1 \eta_2</script><p>同时发现另外一个公式:</p>
<script type="math/tex; mode=display">(x_1 \cdot x_2 + 1)^2 = 2\mu_1 \eta_1 + 2\mu_2 \eta_2 + \mu_1^2 \eta_1^2 + \mu_2^2 \eta_2^2 + 2\mu_1 \mu_2 \eta_1 \eta_2 + 1</script><p>上述两个公式非常相似, 只要乘上一个相关系数, 就可以让两个式子的值相等, 这样就可以将五维空间的一个内积转换为两维的内积运算</p>
<ul>
<li>线性核函数(Linear Kernel)<script type="math/tex; mode=display">K(x, z) = x \cdot z</script></li>
<li>多项式核函数(polynomial Kernal), 其中$\gamma, r, d$属于超参, 需要调参定义<script type="math/tex; mode=display">K(x, z) = (\gamma x \cdot r)^d</script></li>
<li>高斯核函数(Gaussian Kernal), 其中$\gamma$属于超参, 要求大于0, 需要调参定义<script type="math/tex; mode=display">K(x,z) = e^{-\gamma \Vert x-z \Vert_{2}^{2}}</script></li>
<li>Sigmoid核函数(Sigmoid Kernal), 其中$\gamma, r$属于超参, 需要调参定义<script type="math/tex; mode=display">K(x,z) = tanh(\gamma x \cdot z + r)</script></li>
</ul>
<p><img src="/L8/WechatIMG40.png" width="300px"></p>
<h3 id="核函数总结"><a href="#核函数总结" class="headerlink" title="核函数总结"></a>核函数总结</h3><ul>
<li>核函数可以自定义, 核函数必须是正定核函数, 即Gram矩阵是半正定矩阵</li>
<li>核函数的价值在于它虽然也是将特征进行从低维度到高维度的转换, 但核函数事先在低维上进行计算, 而将实质的分类效果表现在了高维上, 也就避免了直接在高维空间上的复杂计算</li>
<li>通过核函数, 可以将非线性可分的数据转换为线性可分数据</li>
</ul>
<p>\begin{bmatrix}<br>{K(x_1, x_1)}&amp;{K(x_1, x_2)}&amp;{\cdots}&amp;{K(x_1, x_m)}\\<br>{K(x_2, x_1)}&amp;{K(x_2, x_2)}&amp;{\cdots}&amp;{K(x_2, x_m)}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{K(x_m, x_1)}&amp;{K(x_m, x_2)}&amp;{\cdots}&amp;{K(x_m, x_m)}\\<br>\end{bmatrix}</p>
<h3 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h3><p>令$z=x$, 进行多维变换后, 应该是同一个向量, 从而得到以下公式:</p>
<script type="math/tex; mode=display">\begin{align} K(x, z) &= e^{-\gamma \Vert x-z \Vert_2^2} \\
&= e^{-\gamma \Vert x \Vert_2^2} \cdot e^{-\gamma \Vert z \Vert_2^2} \cdot e^{2\gamma x \cdot z} \\
& \overset{\text{泰勒展开}}{=} e^{-\gamma \Vert x \Vert_2^2} \cdot e^{-\gamma \Vert z \Vert_2^2} (\displaystyle \sum_{i=0}^{+\infty} \frac{(2\gamma x \cdot z)^i}{i!}) \\
&= \displaystyle \sum_{i=0}^{+\infty} e^{-\gamma \Vert x \Vert_2^2} \cdot e^{-\gamma \Vert z \Vert_2^2} \frac{(2\gamma x \cdot z)^i}{i!}
& \Rightarrow \phi(x) = e^{-\gamma \Vert x \Vert_2^2} \cdot (1, \sqrt{\frac{(2 \gamma)^1}{1!}} \Vert x\Vert_2^1, \sqrt{\frac{(2 \gamma)^2}{2!}} \Vert x\Vert_2^2, \cdots) \\
& = \displaystyle \sum_{i=0}^{+\infty} e^{-\gamma \Vert x \Vert_2^2} \cdot e^{-\gamma \Vert z \Vert_2^2} \sqrt{\frac{(2\gamma)^i}{i!}} \sqrt{\frac{(2\gamma)^i}{i!}} \Vert x \Vert_2^i \Vert z \Vert_2^i \\
&= \phi(x) \cdot \phi(z)\\
\end{align}</script><h2 id="非线性可分SVM"><a href="#非线性可分SVM" class="headerlink" title="非线性可分SVM"></a>非线性可分SVM</h2><p>不管是线性可分SVM还是加入惩罚系数的软间隔线性可分SVM其实都是要求数据本身都是线性可分的, 对于完全不可以线性可分的数据, 这两种算法模型就没法解决这个问题了 </p>
<h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>未完待续</p>
<h2 id="贝叶斯算法"><a href="#贝叶斯算法" class="headerlink" title="贝叶斯算法"></a>贝叶斯算法</h2><h3 id="贝叶斯定理相关公式"><a href="#贝叶斯定理相关公式" class="headerlink" title="贝叶斯定理相关公式"></a>贝叶斯定理相关公式</h3><h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><h4 id="朴素贝叶斯算法推导"><a href="#朴素贝叶斯算法推导" class="headerlink" title="朴素贝叶斯算法推导"></a>朴素贝叶斯算法推导</h4><h4 id="朴素贝叶斯算法流程"><a href="#朴素贝叶斯算法流程" class="headerlink" title="朴素贝叶斯算法流程"></a>朴素贝叶斯算法流程</h4><h3 id="高斯朴素贝叶斯"><a href="#高斯朴素贝叶斯" class="headerlink" title="高斯朴素贝叶斯"></a>高斯朴素贝叶斯</h3><h3 id="伯努利朴素贝叶斯"><a href="#伯努利朴素贝叶斯" class="headerlink" title="伯努利朴素贝叶斯"></a>伯努利朴素贝叶斯</h3><h3 id="多项式朴素贝叶斯"><a href="#多项式朴素贝叶斯" class="headerlink" title="多项式朴素贝叶斯"></a>多项式朴素贝叶斯</h3><h3 id="案例一-鸢尾花数据分类"><a href="#案例一-鸢尾花数据分类" class="headerlink" title="案例一: 鸢尾花数据分类"></a>案例一: 鸢尾花数据分类</h3><h3 id="案例二-文本数据分类"><a href="#案例二-文本数据分类" class="headerlink" title="案例二: 文本数据分类"></a>案例二: 文本数据分类</h3><h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><h3 id="最简单的贝叶斯网络"><a href="#最简单的贝叶斯网络" class="headerlink" title="最简单的贝叶斯网络"></a>最简单的贝叶斯网络</h3><h3 id="全连接贝叶斯网络"><a href="#全连接贝叶斯网络" class="headerlink" title="全连接贝叶斯网络"></a>全连接贝叶斯网络</h3><h3 id="正常贝叶斯网络"><a href="#正常贝叶斯网络" class="headerlink" title="正常贝叶斯网络"></a>正常贝叶斯网络</h3><h4 id="实际贝叶斯网络-判断是否下雨"><a href="#实际贝叶斯网络-判断是否下雨" class="headerlink" title="实际贝叶斯网络: 判断是否下雨"></a>实际贝叶斯网络: 判断是否下雨</h4><h4 id="贝叶斯网络判定条件独立"><a href="#贝叶斯网络判定条件独立" class="headerlink" title="贝叶斯网络判定条件独立"></a>贝叶斯网络判定条件独立</h4><p>如果x是连续的, 一般选择高斯朴素贝叶斯; 如果x是离散的, 一般选择多项式朴素贝叶斯; 如果x既有连续值又有离散值, 选择高斯朴素贝叶斯</p>
<h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><h4 id="最大似然估计回顾"><a href="#最大似然估计回顾" class="headerlink" title="最大似然估计回顾"></a>最大似然估计回顾</h4><p>MLE就是利用已知的样本结果, 反推最有可能(最大概率)导致这样结果的参数值的计算过程. 就是给定了一定的数据, 假定知道数据是从某种分布中随机抽取出来的, 但是不知道这个分布具体的参数值, 即”模型已定, 参数未知”, MLE就是用来估计模型的参数的. MLE的目标是找出一组参数(模型中的参数), 使得模型产出观察数据的概率最大.</p>
<script type="math/tex; mode=display">\displaystyle arg\max \limits_{\theta}p(X;\theta)</script><h4 id="贝叶斯算法估计"><a href="#贝叶斯算法估计" class="headerlink" title="贝叶斯算法估计"></a>贝叶斯算法估计</h4><ul>
<li>贝叶斯算法估计是一种从先验概率和样本分布情况来计算后验概率的一种方式</li>
<li>贝叶斯算法中的常见概念<ul>
<li>p(A)是事件A的先验概率或者边缘概率</li>
<li>p(A|B)是已知B发生后A发生的条件概率, 也称为A的后验概率</li>
<li>p(B|A)是已知A发生后B发生的条件概率, 也称为B的后验概率</li>
<li>p(B)是事件B的先验概率或者边缘概率<script type="math/tex; mode=display">p(AB) = p(A)p(B|A) = p(B)p(A|B) \Rightarrow p(A|B) = \frac{p(A)p(B|A)}{p(B)}</script><script type="math/tex; mode=display">p(A_i|B) = \frac{p(A_i)p(B|A_i)}{p(B)} = \frac{p(A_i)p(B|A_i)}{\displaystyle \sum_j p(B|A_j)p(A_j)}</script><script type="math/tex; mode=display">f(\theta|x) = \frac{f(x|\theta)g(\theta)}{\displaystyle \int_{\theta^{'} \in \Theta}f(x|{\theta}^{'})g(\theta^{'})}</script></li>
</ul>
</li>
</ul>
<p>假设有5个盒子, 假定每个盒子中都有黑白两种球, 并且黑白球的比例如下; 已知从这5个盒子中的任意一个盒子中有放回的抽取两个球, 且均为白球, 问这两个球是从哪个盒子中抽取出来的?</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>盒子编号</th>
<th>白球(p)</th>
<th>黑球(q)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>0.3</td>
<td>0.7</td>
</tr>
<tr>
<td>3</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td>4</td>
<td>0.7</td>
<td>0.3</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>p(A)</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
</tr>
</tbody>
</table>
</div>
<p>使用MLE(最大似然估计), 结论是从第5个盒子抽取的球:</p>
<script type="math/tex; mode=display">L(X;p) = p^2 \quad \underrightarrow{arg\max L(X;p)} \quad p=1</script><p>使用贝叶斯算法估计, 结论是从第5个盒子抽取的球: 假设抽出白球为事件B, 从第i个盒子中抽取为事件$A_i$</p>
<p>$p(B|A_i)$, 抽到第i个盒子的概率 <strong>乘以</strong> 在第i个盒子中第一次抽出的球为白球的概率 <strong>乘以</strong> 在第i个盒子中第二次抽出的球为白球的概率</p>
<script type="math/tex; mode=display">p(B|A_1) = 0.2 * 0 * 0 = 0</script><script type="math/tex; mode=display">p(B|A_2) = 0.2 * 0.3 * 0.3 = 0.018</script><script type="math/tex; mode=display">p(B|A_3) = 0.2 * 0.5 * 0.5 = 0.05</script><script type="math/tex; mode=display">p(B|A_4) = 0.2 * 0.7 * 0.7 = 0.098</script><script type="math/tex; mode=display">p(B|A_5) = 0.2 * 1 * 1 = 0.2</script><p>$p(B)$, 从任意一个盒子中有放回的抽取两个球, 且均为白球的概率</p>
<script type="math/tex; mode=display">p(B) = \sum_{i=1}^{5} p(B|A_i) = 0 + 0.018 + 0.05 + 0.098 + 0.2 = 0.366</script><p>$p(A_1|B)$, 两抽为白的是第一个盒子的后验概率<br>$p(A_1)$, 抽到第一个盒子的先验概率<br>$p(B|A_1)$, 抽到第一个盒子并且两抽为白的条件概率<br>$p(B)$, 任意的盒子, 两抽为白的边缘概率</p>
<script type="math/tex; mode=display">p(A_1|B) = \frac{p(A_1)p(B|A_1)}{p(B)} = \frac{0.2 * 0}{0.366} = 0</script><script type="math/tex; mode=display">p(A_2|B) = \frac{p(A_2)p(B|A_2)}{p(B)} = \frac{0.2 * 0.018}{0.366} = 0.2 * 0.049</script><script type="math/tex; mode=display">p(A_3|B) = \frac{p(A_3)p(B|A_3)}{p(B)} = \frac{0.2 * 0.05}{0.366} = 0.2 * 0.137</script><script type="math/tex; mode=display">p(A_4|B) = \frac{p(A_4)p(B|A_3)}{p(B)} = \frac{0.2 * 0.098}{0.366} = 0.2 * 0.268</script><script type="math/tex; mode=display">p(A_5|B) = \frac{p(A_5)p(B|A_3)}{p(B)} = \frac{0.2 * 0.2}{0.366} = 0.2 * 0.546</script><p>现在不是从5个盒子中任选一个盒子进行抽取, 而是按照一定的概率选择对应的盒子, 概率如下. 结论是从第4个盒子抽取的</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>p(A)</td>
<td>0.1</td>
<td>0.2</td>
<td>0.2</td>
<td>0.4</td>
<td>0.1</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">p(A_1|B) = \frac{p(A_1)p(B|A_1)}{p(B)} = \frac{0.1 * 0}{0.366} = 0</script><script type="math/tex; mode=display">p(A_2|B) = \frac{p(A_2)p(B|A_2)}{p(B)} = \frac{0.2 * 0.018}{0.366} = 0.2 * 0.049</script><script type="math/tex; mode=display">p(A_3|B) = \frac{p(A_3)p(B|A_3)}{p(B)} = \frac{0.2 * 0.05}{0.366} = 0.2 * 0.137</script><script type="math/tex; mode=display">p(A_4|B) = \frac{p(A_4)p(B|A_3)}{p(B)} = \frac{0.4 * 0.098}{0.366} = 0.2 * 0.536</script><script type="math/tex; mode=display">p(A_5|B) = \frac{p(A_5)p(B|A_3)}{p(B)} = \frac{0.1 * 0.2}{0.366} = 0.2 * 0.366</script><h4 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h4><p>MAp(Maximum a posteriori estimation), 和MLE一样, 都是通过样本估计参数$\theta$的值; 在MLE中, 是使得似然函数$p(X|\theta)$最大的时候参数$\theta$的值, MLE中假设先验概率是一个等值的; 而在MAp中, 则是求$\theta$使$p(X|\theta)p(\theta)$的值最大, 这就要求$\theta$值不仅仅是让似然函数最大, 同时要求$\theta$本身出现的先验概率也得比较大</p>
<p>可以认为MAp是贝叶斯算法的一种应用</p>
<script type="math/tex; mode=display">p(\theta^{'}|X) = \frac{p(\theta^{'})p(X|\theta^{'})}{p(X)} \quad \rightarrow \quad arg\max \limits_{\theta^{'}} p(\theta^{'}|X) \quad \rightarrow \quad arg\max \limits_{\theta^{'}} p(\theta^{'})p(X|\theta^{'})</script><h4 id="K-means算法回顾"><a href="#K-means算法回顾" class="headerlink" title="K-means算法回顾"></a>K-means算法回顾</h4><p>K-means算法, 也叫做k-均值聚类算法, 是一种非常广泛使用的聚类算法之一<br>假定输入样本为$S=x_1, x_2, \cdots, x_m$, 则算法步骤为:</p>
<ul>
<li>选择初始的k个簇中心点$\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将样本$x_i$标记为距离簇中心最近的簇: $label_i = arg \min \limits_{1 \le j \le k} \Vert x_i - \mu_j \Vert$</li>
<li>迭代处理所有样本数据, 计算出各个样本点所属的对应簇</li>
<li>更新簇中心点坐标$\mu_i = \displaystyle \frac{1}{\vert C_i \vert} \sum_{j \in C_i} x_j$</li>
<li>重复上述三个操作,  直到算法收敛<br>算法收敛条件: 迭代次数/簇中心变化率/MSE/MAE<script type="math/tex; mode=display">J(k, \mu) = \frac{1}{m}\sum_{i=1}^{m}\Vert x^{(i)} - \mu_{k^{(i)}} \Vert^2</script><script type="math/tex; mode=display">k, \mu = arg \min \limits_{k, \mu} J(k, \mu)</script></li>
</ul>
<h4 id="EM算法引入"><a href="#EM算法引入" class="headerlink" title="EM算法引入"></a>EM算法引入</h4><p>公司有男同事=[A，B，C]，同时有很多漂亮的女职员=[小甲，小章，小乙]。（请勿对号入座）你迫切的怀疑这些男同事跟这些女职员有“问题”。为了科学的验证你的猜想，你进行了细致的观察。于是：<br>观察数据:</p>
<ul>
<li>A，小甲、小乙一起出门了；</li>
<li>B，小甲、小章一起出门了；</li>
<li>B，小章、小乙一起出门了；</li>
<li>C，小乙一起出门了；</li>
</ul>
<h5 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h5><p>你觉得三个同事一样帅，一样有钱，三个美女一样漂亮，每个人都可能跟每个人有关系。所以，每个男同事跟每个女职员“有问题”的概率都是1/3;</p>
<h5 id="EM算法中的E步骤"><a href="#EM算法中的E步骤" class="headerlink" title="EM算法中的E步骤"></a>EM算法中的E步骤</h5><ul>
<li>A和小甲出去过的次数为$\displaystyle \frac{1}{2} \cdot \frac{1}{3} = \frac{1}{6}$, 和小乙出去的次数也是$\displaystyle \frac{1}{6}$</li>
<li>B和小甲, 小章出去的次数分别为$\displaystyle \frac{1}{6}$</li>
<li>B和小章, 小乙出去的次数分别为$\displaystyle \frac{1}{6}$</li>
<li>C和小乙出去的次数为$\displaystyle \frac{1}{3}$</li>
</ul>
<p>归纳总结:</p>
<ul>
<li>A和小甲出去了$\displaystyle \frac{1}{6}$次, 和小乙出去了$\displaystyle \frac{1}{6}$次</li>
<li>B和小甲出去了$\displaystyle \frac{1}{6}$次, 和小乙出去了$\displaystyle \frac{1}{6}$次, 和小章出去了$\displaystyle \frac{1}{3}$次</li>
<li>C和小乙出去了$\displaystyle \frac{1}{3}$次</li>
</ul>
<h5 id="EM算法中的M步骤"><a href="#EM算法中的M步骤" class="headerlink" title="EM算法中的M步骤"></a>EM算法中的M步骤</h5><ul>
<li>A和小甲, 小乙有问题的概率为$\displaystyle \frac{1/6}{1/6 + 1/6} = \frac{1}{2}$</li>
<li>B和小甲, 小乙有问题的概率为$\displaystyle \frac{1/6}{1/6 + 1/6 + 1/6 + 1/6} = \frac{1}{4}$</li>
<li>B和小章有问题的概率为$\displaystyle \frac{1/3}{1/6 + 1/6 + 1/3} = \frac{1}{2}$</li>
<li>C和小乙有问题的概率为$\displaystyle \frac{1/3}{1/3} = 1$</li>
</ul>
<h5 id="第二次迭代EM算法中的E步骤"><a href="#第二次迭代EM算法中的E步骤" class="headerlink" title="第二次迭代EM算法中的E步骤"></a>第二次迭代EM算法中的E步骤</h5><p>根据已知条件</p>
<ul>
<li>A，小甲、小乙一起出门了；</li>
<li>B，小甲、小章一起出门了；</li>
<li>B，小章、小乙一起出门了；</li>
<li>C，小乙一起出门了；</li>
</ul>
<p>结合上个M步骤更新的概率</p>
<ul>
<li>A和小甲出去了$\displaystyle \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$次, 和小乙出去了$\displaystyle \frac{1}{4}$次</li>
<li>B和小甲出去了$\displaystyle \frac{1}{2} \cdot \frac{1}{4} = \frac{1}{8}$次, 和小章出去了$\displaystyle \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$次, </li>
<li>B和小章出去了$\displaystyle \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$次, 和小乙出去了$\displaystyle \frac{1}{2} \cdot \frac{1}{4} = \frac{1}{8}$次</li>
<li>C和小乙出去了1次</li>
</ul>
<p>归纳总结:</p>
<ul>
<li>A和小甲出去了$\displaystyle \frac{1}{4}$次, 和小乙出去了$\displaystyle \frac{1}{4}$次</li>
<li>B和小甲出去了$\displaystyle \frac{1}{8}$次, 和小乙出去了$\displaystyle \frac{1}{8}$次, 和小章出去了$\displaystyle \frac{1}{2}$次</li>
<li>C和小乙出去了1次</li>
</ul>
<h5 id="第二次迭代EM算法中的M步骤"><a href="#第二次迭代EM算法中的M步骤" class="headerlink" title="第二次迭代EM算法中的M步骤"></a>第二次迭代EM算法中的M步骤</h5><ul>
<li>A和小甲, 小乙有问题的概率为$\displaystyle \frac{1/4}{1/4 + 1/4} = \frac{1}{2}$</li>
<li>B和小甲, 小乙有问题的概率为$\displaystyle \frac{1/8}{1/8 + 1/8 + 1/2} = \frac{1}{6}$</li>
<li>B和小章有问题的概率为$\displaystyle \frac{1/2}{1/8 + 1/8 + 1/2} = \frac{2}{3}$</li>
<li>C和小乙有问题的概率为1</li>
</ul>
<p>你继续计算，反思，总之，最后，你得到了真相。</p>
<p>通过上面的计算我们可以得知，EM算法实际上是一个不停迭代计算的过程，根据我们事先估计的先验概率A，得出一个结果B，再根据结果B，再计算得到结果A，然后反复直到这个过程收敛。<br>可以想象饭店的后方大厨，炒了两盘一样的菜，现在，菜炒好后从锅中倒入盘，不可能一下子就分配均匀，所以先往两盘中倒入，然后发现B盘菜少了，就从A中匀出一些，A少了，从B匀…..</p>
<h4 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h4><h5 id="EM算法原理"><a href="#EM算法原理" class="headerlink" title="EM算法原理"></a>EM算法原理</h5><p>给定m个训练样本${x(1), x(2), \cdots, x(m)}$, 样本间独立, 找出样本的模型参数$\theta$, 极大化模型参数的对数似然函数, 如下:</p>
<script type="math/tex; mode=display">\theta = arg \max \limits_\theta \displaystyle \sum_{i=1}^{m} log(p(x^{(i)}; \theta))</script><p>假定样本中存在隐含数据$z={z(1), z(2), \cdots, z(k)}$, 此时极大化模型分布的对数似然函数如下:</p>
<script type="math/tex; mode=display">\begin{align}\theta &= arg \max \limits_\theta \displaystyle \sum_{i=1}^{m} log(p(x^{(i)}; \theta))\\
&= arg \max \limits_\theta \displaystyle \sum_{i=1}^{m} log( \sum_{z^{(i)}} p(z^{(i)}) p(x^{(i)}|z^{(i)}; \theta) )\\
&= arg \max \limits_\theta \displaystyle \sum_{i=1}^{m} log( \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) )\\
\end{align}</script><p>Q(z)是一个随机变量的分布, 每一个取值对应的概率密度之和等于1.<br>$E(Y) = E(g(x)) = \sum_i g(x_i)p_i$, $\displaystyle \sum_z Q(z; \theta) = 1$</p>
<p>令z的分布为$Q(z; \theta)$, 并且$Q(z; \theta) \ge 0$, 那么如上的极大似然估计的最大化的函数$l(\theta)$</p>
<script type="math/tex; mode=display">\begin{align} l(\theta) &= \displaystyle \sum_{i=1}^{m} log( \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) )\\
&= \displaystyle \sum_{i=1}^{m} log( \sum_{z^{(i)}} Q(z^{(i)}; \theta) \cdot \displaystyle \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q(z^{(i)}; \theta)} ) \\
&= \displaystyle \sum_{i=1}^{m} log( \quad \displaystyle E_Q( \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q(z^{(i)}; \theta)} ) \quad) \\
&\ge \displaystyle \sum_{i=1}^{m} E_Q ( \quad log( \displaystyle  \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q(z^{(i)}; \theta)} ) \quad) \\
&\ge \displaystyle \sum_{i=1}^{m} \sum_{z^{(i)}} Q(z^{(i)}; \theta) (log( \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q(z^{(i)}; \theta)} ) ) \\
\end{align}</script><p><img src="/L8/WechatIMG42.png" width="300px"></p>
<p>一般情况下, z是隐含变量, 不知道应该取多少合适, 但是可以通过最后得到的公式对z进行求解. 如果最后两步相等的话, 就用公式的最后一步去替代$l(\theta)$, 进而得到新公式的最大值. 那么最后两步在什么情况下才能相等?</p>
<p>根据不等式的性质, 当下面式子中的对数中的随机变量为常数的时候, $l(\theta)$和右边的式子取等号</p>
<script type="math/tex; mode=display">\displaystyle \frac{p(x, z; \theta)}{Q(z; \theta)} = c, \quad \sum_zQ(z; \theta) = 1</script><script type="math/tex; mode=display">\begin{align}
Q(z; \theta) &= \frac{p(x, z; \theta)}{c} = \frac{p(x, z; \theta)}{c \cdot \displaystyle \sum_{z^{(i)}} Q(z^{(i)}; \theta)}\\
&=\frac{p(x, z; \theta)}{\displaystyle \sum_{z^{(i)}} c \cdot Q(z^{(i)}; \theta)} \\
&=\frac{p(x, z; \theta)}{\displaystyle \sum_{z^{(i)}} p(x, z^{(i)}; \theta)} \\
&=\frac{p(x, z; \theta)}{ p(x; \theta)} = p(z|x; \theta) \\
\end{align}</script><p>所以最后, 当$Q(z; \theta) = p(z|x; \theta)$的时候$l(\theta)$函数取得等号</p>
<script type="math/tex; mode=display">\begin{align} \theta &= arg \max \limits_{\theta} l(\theta) = arg \max \limits_{\theta} \sum_{i=1}^{m} \sum_{z} Q(z; \theta^{old}) log( \frac{p(x, z; \theta)}{Q(z; \theta^{old})} )\\
&= arg \max \limits_{\theta} \sum_{i=1}^{m} \sum_{z} p(z|x; \theta^{old}) log( \frac{p(x, z; \theta)}{ p(z|x; \theta^{old}) } ) \\
&= arg \max \limits_{\theta} \sum_{i=1}^{m} \sum_{z} p(z|x; \theta^{old}) log( p(x, z; \theta) ) \\
\end{align}</script><h5 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h5><p>样本数据$X={x_1, x_2, \cdots, x_m}$, 联合分布$p(x, z; \theta)$, 条件分布$p(z|x; \theta)$, 最大迭代次数J</p>
<ul>
<li>随机初始化模型参数$\theta$的初始值$\theta_0$</li>
<li>开始EM算法的迭代处理:<ul>
<li>E步: 计算联合分布的条件概率期望, $Q^j = p(z|x; \theta^j) \quad l(\theta) = \displaystyle \sum_{i=1}^{m} \sum_z Q^j log(p(x, z; \theta^j))$</li>
<li>M步: 极大化L函数, 得到$\theta^{j+1}$, $\theta^{j+1} = arg \max \limits_{\theta} l(\theta)$</li>
<li>如果$\theta^{j+1}$已经收敛, 则算法结束, 输出最终的模型参数$\theta$, 否则继续迭代处理</li>
</ul>
</li>
</ul>
<h5 id="EM算法直观案例"><a href="#EM算法直观案例" class="headerlink" title="EM算法直观案例"></a>EM算法直观案例</h5><p>假设现有两个装有不定数量黑球和白球的盒子, 随机从盒子中抽取出一个白球的概率分布为$p_1$和$p_2$; 为了估计这两个概率, 每次选择一个盒子, 有放回的连续随机抽取5个球, 记录如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>盒子编号</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>统计</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>白</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>3白2黑</td>
</tr>
<tr>
<td>2</td>
<td>黑</td>
<td>黑</td>
<td>白</td>
<td>白</td>
<td>黑</td>
<td>2白3黑</td>
</tr>
<tr>
<td>1</td>
<td>白</td>
<td>黑</td>
<td>黑</td>
<td>黑</td>
<td>黑</td>
<td>1白4黑</td>
</tr>
<tr>
<td>2</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>3白2黑</td>
</tr>
<tr>
<td>1</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>2白3黑</td>
</tr>
</tbody>
</table>
</div>
<p>使用MLE最大似然估计:</p>
<script type="math/tex; mode=display">l(p_1) = log(p_1^6(1-p_1)^9) = 6logp_1 + 9log(1-p_1)</script><script type="math/tex; mode=display">\displaystyle \frac{\partial l(p_1)}{\partial p_1} = \frac{6}{p_1} - \frac{9}{1-p_1} \quad \underrightarrow{ 令\frac{\partial l(p_1)}{\partial p_1} = 0 } \quad p_1 = 0.4</script><p>同理</p>
<script type="math/tex; mode=display">l(p_2) = log(p_2^5(1-p_2)^5) = 5logp_2 + 5log(1-p_2)</script><script type="math/tex; mode=display">\displaystyle \frac{\partial l(p_2)}{\partial p_2} = \frac{5}{p_2} - \frac{5}{1-p_2} \quad \underrightarrow{ 令\frac{\partial l(p_2)}{\partial p_2} = 0 } \quad p_2 = 0.5</script><p>如果现在不知道具体的盒子编号, 但是同样还是为了求解$p_1$和$p_2$的值, 这个时候就相当于多了一个隐藏变量z, z表示每次抽取的时候选择的盒子编号, 比如$z_1$表示第一次抽取的时候选择的是盒子1还是盒子2</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>盒子编号</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>统计</th>
</tr>
</thead>
<tbody>
<tr>
<td>z1</td>
<td>白</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>3白2黑</td>
</tr>
<tr>
<td>z2</td>
<td>黑</td>
<td>黑</td>
<td>白</td>
<td>白</td>
<td>黑</td>
<td>2白3黑</td>
</tr>
<tr>
<td>z3</td>
<td>白</td>
<td>黑</td>
<td>黑</td>
<td>黑</td>
<td>黑</td>
<td>1白4黑</td>
</tr>
<tr>
<td>z4</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>3白2黑</td>
</tr>
<tr>
<td>z5</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>2白3黑</td>
</tr>
</tbody>
</table>
</div>
<p>随机初始化一个概率值: $p_1=0.1$和$p_2=0.9$; 然后使用最大似然估计计算每轮操作中从两个盒子中抽取的最大概率. 然后使用计算出来的z值, 重新使用极大似然估计法估计概率值</p>
<script type="math/tex; mode=display">L(z_1=1|x; p_1) = 0.1^3 * 0.9^2 = 0.00081</script><script type="math/tex; mode=display">L(z_1=2|x; p_2) = 0.9^3 * 0.1^2 = 0.00729</script><div class="table-container">
<table>
<thead>
<tr>
<th>轮数</th>
<th>盒子1</th>
<th>盒子2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.00081</td>
<td>0.00729</td>
</tr>
<tr>
<td>2</td>
<td>0.00729</td>
<td>0.00081</td>
</tr>
<tr>
<td>3</td>
<td>0.06561</td>
<td>0.00009</td>
</tr>
<tr>
<td>4</td>
<td>0.00081</td>
<td>0.00729</td>
</tr>
<tr>
<td>5</td>
<td>0.00729</td>
<td>0.00081</td>
</tr>
</tbody>
</table>
</div>
<p>在这里, 切记$p_1$表示从盒子1中抽取到白球的概率, $p_2$表示从盒子2中抽取到白球的概率, 那么上述表格就表示第1轮从盒子2中抽取的概率较大, 第2轮从盒子1中抽取的概率较大, 总结一下就是第1, 4轮从盒子1中抽取的概率较大, 第2, 3, 5轮从盒子2中抽取的概率较大</p>
<p>那么更新一下原始表格如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>盒子编号</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>统计</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>白</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>3白2黑</td>
</tr>
<tr>
<td>1</td>
<td>黑</td>
<td>黑</td>
<td>白</td>
<td>白</td>
<td>黑</td>
<td>2白3黑</td>
</tr>
<tr>
<td>1</td>
<td>白</td>
<td>黑</td>
<td>黑</td>
<td>黑</td>
<td>黑</td>
<td>1白4黑</td>
</tr>
<tr>
<td>2</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>3白2黑</td>
</tr>
<tr>
<td>1</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>白</td>
<td>黑</td>
<td>2白3黑</td>
</tr>
</tbody>
</table>
</div>
<p>根据此表格, 得出$p_2=\frac{6}{10}=0.6, \quad p_1=\frac{5}{15}=\frac{1}{3}$</p>
<script type="math/tex; mode=display">L(z_1=1|x; p_1) = (\frac{1}{3})^3 * 0.6^2 = 0.01565</script><script type="math/tex; mode=display">L(z_1=2|x; p_2) = 0.6^3 * (\frac{1}{3})^2 = 0.03456</script><div class="table-container">
<table>
<thead>
<tr>
<th>轮数</th>
<th>盒子1</th>
<th>盒子2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.01565</td>
<td>0.03456</td>
</tr>
<tr>
<td>2</td>
<td>0.0313</td>
<td>0.02304</td>
</tr>
<tr>
<td>3</td>
<td>0.0626</td>
<td>0.01536</td>
</tr>
<tr>
<td>4</td>
<td>0.01565</td>
<td>0.03456</td>
</tr>
<tr>
<td>5</td>
<td>0.0313</td>
<td>0.02304</td>
</tr>
</tbody>
</table>
</div>
<p>使用最大似然概率法则估计z和p的值, 但是在这个过程中, 只使用一个最有可能的值.  如果考虑所有的z值, 然后对每一组z值都估计一个概率p, 那么这个时候估计出来的概 率可能会更好, 可以用期望的方式来简化这个操作</p>
<p>归一化操作</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>轮数</th>
<th>盒子1</th>
<th>盒子2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.00081/(0.00081+0.00729) = 0.1</td>
<td>0.00729/(0.00081+0.00729) = 0.9</td>
</tr>
<tr>
<td>2</td>
<td>0.00729</td>
<td>0.00081</td>
</tr>
<tr>
<td>3</td>
<td>0.06561</td>
<td>0.00009</td>
</tr>
<tr>
<td>4</td>
<td>0.00081</td>
<td>0.00729</td>
</tr>
<tr>
<td>5</td>
<td>0.00729</td>
<td>0.00081</td>
</tr>
</tbody>
</table>
</div>
<h5 id="EM算法收敛证明"><a href="#EM算法收敛证明" class="headerlink" title="EM算法收敛证明"></a>EM算法收敛证明</h5><p>EM算法的收敛性只要能够证明对数似然函数的值在迭代过程中是增加的就可以</p>
<script type="math/tex; mode=display">\displaystyle \sum_{i=1}^{m}log(p(x^i; \theta^{j+1})) \ge \sum_{i=1}^{m}log(p(x^i; \theta^{j}))</script><p>构造函数如下:</p>
<script type="math/tex; mode=display">L(\theta, \theta^j) = \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(p(x^i, z; \theta))</script><script type="math/tex; mode=display">H(\theta, \theta^j) = \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(p(z|x^i; \theta))</script><script type="math/tex; mode=display">\because \sum_z p(z|x^i; \theta^j) = 1</script><script type="math/tex; mode=display">\begin{align} \therefore L(\theta, \theta^j) - H(\theta, \theta^j) &= \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(\frac{p(x^i, z; \theta)}{p(z|x^i; \theta}) \\
&= \sum_{i=1}^{m} log(\frac{p(x^i, z; \theta)}{p(z|x^i; \theta})\\
&= \sum_{i=1}^{m} log(p(x^i; \theta))\\
\end{align}</script><script type="math/tex; mode=display">\begin{align} \therefore [L(\theta^{j+1}, \theta^j) - H(\theta^{j+1}, \theta^j)] - [L(\theta^{j}, \theta^j) - H(\theta^{j}, \theta^j)] &= [L(\theta^{j+1}, \theta^j) - L(\theta^{j}, \theta^j)] - [H(\theta^{j+1}, \theta^j) - H(\theta^{j}, \theta^j)] \\
&= \sum_{i=1}^{m} log(p(x^i; \theta^{j+1})) - \sum_{i=1}^{m} log(p(x^i; \theta^j))\\
\end{align}</script><p>而</p>
<script type="math/tex; mode=display">\begin{align} L(\theta^{j+1}, \theta^j) - L(\theta^{j}, \theta^j) &= \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(p(x^i, z; \theta^{j+1})) - \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(p(x^i, z; \theta^j)) \\
&= \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log( \frac{p(x^i, z; \theta^{j+1})}{p(x^i, z; \theta^j)} )\\
& \ge 0 \\
\end{align}</script><script type="math/tex; mode=display">\begin{align} H(\theta^{j+1}, \theta^j) - H(\theta^{j}, \theta^j) &= \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(p(z|x^i; \theta^{j+1})) - \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log(p(z|x^i; \theta^j)) \\
&= \sum_{i=1}^{m} \sum_z p(z|x^i; \theta^j) log( \frac{p(z|x^i; \theta^{j+1})}{p(z|x^i; \theta^j)}) \\
&\because log函数是开口向下的凸函数 \\
&\therefore \le \sum_{i=1}^{m} log( \sum_z p(z|x^i; \theta^j) \frac{p(z|x^i; \theta^{j+1})}{p(z|x^i; \theta^j)}) = 0 \\ 
\end{align}</script><script type="math/tex; mode=display">\sum_{i=1}^{m} log(p(x^i; \theta^{j+1})) - \sum_{i=1}^{m} log(p(x^i; \theta^j)) \ge 0</script><p>然而如下问题<br>随机选择1000名用户, 测量用户的身高; 若样本中存在男性和女性, 身高分别服从高斯分布$N(\mu_1, \sigma_1)$和$N(\mu_2, \sigma_2)$的分布, 试估计参数: $\mu_1, \sigma_1, \mu_2, \sigma_2$; </p>
<p>解析:</p>
<ul>
<li>如果明确的知道样本的情况(即男性和女性数据是分开的), 那么我们使用极大似然估计来估计这个参数值</li>
<li>如果样本是混合而成的, 不能明确的区分开, 那么就没法直接使用极大似然估计来进行参数的估计.  可以使用EM算法来估计男女这两个参数值, 即男女这两个性别就变成了隐含变量.  实际上, EM算法在某些层面上是在帮助我们做聚类的操作. 即帮助我们找到隐含变量的取值. </li>
</ul>
<h3 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h3><p>GMM(Gaussian Mixture Model), 高斯混合模型, 是指该算法由多个高斯模型线性叠加混合而成, 每个高斯模型称为component. GMM算法描述的是数据本身存在的一种分布<br>GMM常用于聚类应用中, component的个数就可以认为是类别的数量<br>假定GMM由k个Gaussian分布线性叠加而成, 那么概率密度函数如下:</p>
<p><img src="/L8/WechatIMG44.png" width="300px"></p>
<script type="math/tex; mode=display">p(x) = \sum_{k=1}^{K}p(k)p(x|k) = \sum_{k=1}^{K} \pi_k p(x; \mu_k, \sum_k)</script><h4 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h4><script type="math/tex; mode=display">l(\pi, \mu, \sigma) = \displaystyle \sum_{i=1}^{N} log( \sum_{k=1}^{K}\pi_k p(x^i; \mu_k, \sum_k) )</script><h4 id="E-step"><a href="#E-step" class="headerlink" title="E step"></a>E step</h4><script type="math/tex; mode=display">w_{j}^{(i)} = Q_i(z^{(i)} = j) = p(z^{(i)} = j|x^{(i)}; \pi, \mu, \sum)</script><h4 id="M-step"><a href="#M-step" class="headerlink" title="M step"></a>M step</h4><script type="math/tex; mode=display">\begin{align} l(\pi, \mu, \sum) &= \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i(z^{(i)}) log( \frac{p(x^{(i)}, z^{(i)}; \pi, \mu, \sum)}{Q_i(z^{(i)})} ) \\
&= \sum_{i=1}^{m} \sum_{j=1}^{k} Q_i(z^{(i)=j}) log( \frac{p(x^{(i)}|z^{(i)}=j; \mu, \sum) \cdot p(z^{(i)}=j; \pi)}{Q_i(z^{(i)}=j)} ) \\

&= \sum_{i=1}^{m} \sum_{j=1}^{k} w_j^{(i)} log( \frac{ \frac{1}{ (2\pi)^{\frac{n}{2}} \vert \sum_j \vert^{ \frac{1}{2} } } e^{-\frac{1}{2} (x^{(i)}-\mu_j)^T \sum_j^{-1}(x^{(i)}-\mu_j) \cdot \pi_j}  }{w_j^{(i)}} ) \\
\end{align}</script><h4 id="对均值求偏导"><a href="#对均值求偏导" class="headerlink" title="对均值求偏导"></a>对均值求偏导</h4><script type="math/tex; mode=display">l(\pi, \mu, \sum) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_j^{(i)}(-\frac{1}{2}(x^{(i)}-\mu_j)^T \sum_j^{-1}(x^{(i)}-\mu_j)) + c</script><p><img src="/L8/WechatIMG45.png" width="300px"><br><img src="/L8/WechatIMG46.png" width="300px"></p>
<h4 id="对方差求偏导"><a href="#对方差求偏导" class="headerlink" title="对方差求偏导"></a>对方差求偏导</h4><p><img src="/L8/WechatIMG47.png" width="300px"></p>
<h4 id="对概率使用拉格朗日乘子法求解"><a href="#对概率使用拉格朗日乘子法求解" class="headerlink" title="对概率使用拉格朗日乘子法求解"></a>对概率使用拉格朗日乘子法求解</h4><p><img src="/L8/WechatIMG48.png" width="300px"></p>
<h2 id="多分类及多标签分类"><a href="#多分类及多标签分类" class="headerlink" title="多分类及多标签分类"></a>多分类及多标签分类</h2><h3 id="单标签二分类"><a href="#单标签二分类" class="headerlink" title="单标签二分类"></a>单标签二分类</h3><h3 id="单标签多分类"><a href="#单标签多分类" class="headerlink" title="单标签多分类"></a>单标签多分类</h3><h3 id="多标签分类算法"><a href="#多标签分类算法" class="headerlink" title="多标签分类算法"></a>多标签分类算法</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/L7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/L7/" itemprop="url">L7</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-20T20:59:11+08:00">
                2019-01-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>决策树和集成学习</p>
<ul>
<li>分类决策树和回归决策树</li>
<li><p>决策树的构建</p>
<ul>
<li>目的: 让同一个类别/y的取值接近的样本在同一个叶子节点中, 让一个叶子节点中的样本足够的”纯”</li>
<li>“纯”的度量方式(值越大表示数据越不”纯”)<ul>
<li>信息熵(分类)</li>
<li>gini系数(分类)</li>
<li>错误率(分类)</li>
<li>MSE(回归)</li>
<li>MAE(回归)</li>
</ul>
</li>
<li>进行数据划分的特征属性的选择<ul>
<li>基于划分前”纯”度的指标和划分后”纯”度值之间的差距, 作为特征属性的选择, 叫做信息增益, 选择增益越大的特征属性作为最终的划分数据<br>基于划分前”纯”度的指标和划分后”纯”度值之间的差距, 然后相对于划分属性的”纯”度值的比率, 作为特征属性选择的依据, 叫做信息增益率, 选择增益率最大的特征属性作为最终的划分属性</li>
</ul>
</li>
<li>数据的划分方式<ul>
<li>对于离散数据, 如果是构建多叉树, 那么此时一个特征的取值就是一个分支</li>
<li>对于离散数据, 如果是构建二叉树, 那么将离散数据转换为”属于该值”和”不属于该值”两个类别, 然后再每个类别一个分支</li>
<li>对于连续数据, 在连续数据中找出一个split_point点, 然后让大于等于split_point的数据属于一个分支, 让其他数据属于另外一个分支</li>
</ul>
</li>
</ul>
</li>
<li><p>决策树的预测</p>
</li>
<li><p>欠拟合和过拟合</p>
<ul>
<li>欠拟合解决方案: 可以通过增加树的层次来解决这个问题, 但是如果层次足够高的话又会导致过拟合</li>
<li>过拟合解决方案: 剪枝(前剪枝和后剪枝), 或者多棵树的集成学习(随机森林), 在选择划分的特征属性的时候不进行全局最优选择, 而是进行局部最优选择</li>
</ul>
</li>
<li><p>常见的算法</p>
<ul>
<li>ID3</li>
<li>C4.5</li>
<li>CART</li>
</ul>
</li>
<li><p>集成学习</p>
<ul>
<li>思路, 将多个模型进行融合, 使用融合后的结果作为真实的预测值</li>
<li>bagging, 对数据进行重采样, 然后使用重采样的数据进行模型训练, 然后将训练的多个模型预测结果进行融合; 如果是分类就多数投票或者加权的多数投票, 如果是回归就均值或者加权均值<ul>
<li>代表算法是随机森林</li>
</ul>
</li>
<li><p>boosting, 对训练数据进行更改(样本权重的更改, 或者样本数据值的更改), 然后使用更改后的数据来训练模型, 训练之后对模型进行加权的操作</p>
<ul>
<li>代表算法有两种, adaboost基于样本的权重进行模型构建, 权重越高的样本在模型训练的时候起到的决策性作用越大; 样本的更新规则, 如果一个样本被前面的模型预测错误, 那么当前这个样本的权重就增大; 模型的权重给定规则, 如果一个模型的预测越准确, 模型的权重越高 </li>
<li>adaboost本质是每次迭代构建模型的时候, 对于之前分类错误的样本着重考虑 </li>
<li>GBDT, 梯度提升树, 在每次迭代的时候使用上一次的训练数据和上一次训练之后模型的预测值之间的残差作为当前模型的输入</li>
<li>GBDT本质, 每次迭代模型的时候, 都是在之前模型预测的结果的基础上进行预测, 每次产生的模型都让误差/偏度变得更小</li>
<li>GBDT中所有模型权重相同</li>
<li>衰减因子, 每次考量模型的时候, 不是完全的相信模型效果, 所以可以对模型进行一个缩放操作; 每次更新的时候不是基于全部的预测值(y_true - y_predict), 而是采用变化一小点的策略(y_true - alpha * y_predict)</li>
</ul>
</li>
<li><p>stacking</p>
<ul>
<li>使用原始数据和算法模型(不要求一样)训练多个模型</li>
<li>使用第一步训练的多个模型在训练集上的预测值组成一个特征举证X, 使用原始数据中的目标属性作为Y, 然后再训练一个模型, 就相当于模型的算法集成, 而不是通过数据上的算法集成</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><h3 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h3><ul>
<li>Jaccard相似度, Pearson相似度</li>
<li>K-means聚类</li>
<li>聚类算法效果评估(准确率, 召回率等)</li>
<li>层次聚类算法</li>
<li>密度聚类算法</li>
<li>谱聚类算法</li>
</ul>
<h3 id="什么是聚类"><a href="#什么是聚类" class="headerlink" title="什么是聚类"></a>什么是聚类</h3><p>聚类就是对大量未知标注对数据集, 按照数据内部存在的数据特征将数据划分为多个不同的类别, 使类别内的数据比较相似, 类别之间的数据相似度比较小; 属于无监督学习</p>
<p>聚类算法的重点是计算样本项之间的相似度, 有时候也称为样本间的距离</p>
<p>和分类算法的区别</p>
<ul>
<li>分类算法属于有监督学习, 基于有标注的历史数据进行算法模型构建</li>
<li>聚类算法属于无监督学习, 数据集中的数据是没有标注的</li>
</ul>
<h3 id="相似度-距离公式"><a href="#相似度-距离公式" class="headerlink" title="相似度/距离公式"></a>相似度/距离公式</h3><h4 id="闵可夫斯基距离-Minkovski"><a href="#闵可夫斯基距离-Minkovski" class="headerlink" title="闵可夫斯基距离(Minkovski)"></a>闵可夫斯基距离(Minkovski)</h4><script type="math/tex; mode=display">dist(X, Y) = \displaystyle \sqrt[p]{ \sum_{i=1}^n \vert x_i - y_i \vert^p}</script><ul>
<li>当p为1的时候就是曼哈顿距离(Manhattan)<script type="math/tex; mode=display">M_{dist} = \displaystyle \sum_{i=1}^n \vert x_i - y_i \vert</script></li>
<li>当p为2的时候就是欧式距离(Euclidean)<script type="math/tex; mode=display">E_{dist} = \displaystyle \sqrt{ \displaystyle \sum_{i=1}^n (x_i - y_i)^2}</script></li>
<li>当p为无穷大的时候就是切比雪夫距离(Chebyshev)<script type="math/tex; mode=display">C_{dist} = \max \limits_{i} ( \vert x_i - y_i \vert )</script></li>
</ul>
<h4 id="标准化欧式距离-Standardized-Euclidean-Distance"><a href="#标准化欧式距离-Standardized-Euclidean-Distance" class="headerlink" title="标准化欧式距离(Standardized Euclidean Distance)"></a>标准化欧式距离(Standardized Euclidean Distance)</h4><script type="math/tex; mode=display">X^{\ast} = \frac{ X - \bar X}{s}</script><script type="math/tex; mode=display">s = \sqrt{ \frac{ \displaystyle \sum_{i=1}^n (x_i - y_i)^2}{n} }</script><script type="math/tex; mode=display">S\_E\_D = \sqrt{ \displaystyle \sum_{i=1}^n (\frac{x_i - y_i}{s_i})^2 }</script><h4 id="夹角余弦相似度-Cosine"><a href="#夹角余弦相似度-Cosine" class="headerlink" title="夹角余弦相似度(Cosine)"></a>夹角余弦相似度(Cosine)</h4><script type="math/tex; mode=display">a = (x_{11}, x_{12}, \cdots, x_{1n})</script><script type="math/tex; mode=display">b = (x_{21}, x_{22}, \cdots, x_{2n})</script><script type="math/tex; mode=display">cos(\theta) = \displaystyle \frac{\displaystyle \sum_{k=1}^n x_{1k} x_{2k}}{\sqrt{\displaystyle \sum_{k=1}^n x_{1k}^2} \ast \sqrt{\displaystyle \sum_{k=1}^n x_{2k}^2}} = \frac{a^T \cdot b}{\vert a \vert \vert b \vert}</script><h4 id="KL距离-相对熵"><a href="#KL距离-相对熵" class="headerlink" title="KL距离(相对熵)"></a>KL距离(相对熵)</h4><script type="math/tex; mode=display">D(P \Vert\Vert Q) = \sum_x P(x) log(\frac{P(x)}{Q(x)})</script><h4 id="杰卡德相似系数-Jaccard"><a href="#杰卡德相似系数-Jaccard" class="headerlink" title="杰卡德相似系数(Jaccard)"></a>杰卡德相似系数(Jaccard)</h4><script type="math/tex; mode=display">J(A, B) = \frac{\vert A \bigcap B \vert}{\vert A \bigcup B \vert}, \quad dist(A, B) = 1 - J(A, B) = \frac{\vert A \bigcup B \vert - \vert A \bigcap B \vert}{\vert A \bigcup B \vert}</script><h4 id="Pearson相关系数"><a href="#Pearson相关系数" class="headerlink" title="Pearson相关系数"></a>Pearson相关系数</h4><script type="math/tex; mode=display">\rho_{XY} = \frac{Cov(X, Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{ E[(X-E(X))(Y-E(Y)) ]} {\sqrt{D(X)} \sqrt{D(Y)}}=  \frac{ \displaystyle \sum_{i=1}^n (X_i - \mu_X)(Y_i - \mu_Y) }{\sqrt{\displaystyle \sum_{i=1}^n (X_i - \mu_X)^2} \ast \sqrt{\displaystyle \sum_{i=1}^n (Y_i - \mu_Y)^2}}</script><script type="math/tex; mode=display">dist(X, Y) = 1 - \rho_{XY}</script><h3 id="聚类的思想"><a href="#聚类的思想" class="headerlink" title="聚类的思想"></a>聚类的思想</h3><p>给定一个有M个对象的数据集, 构建一个具有k个簇的模型, 其中$k \le M$, 满足以下条件:</p>
<ul>
<li>每个簇至少包含一个对象</li>
<li>每个对象属于且仅属于一个簇</li>
<li>将满足上述条件的k个簇称为一个合理的聚类划分</li>
</ul>
<p>基本思想: 对于给定的类别数据k, 首先给定初始划分, 通过迭代改变样本和簇的隶属关系, 使得每次处理后的得到的划分比上一次的好(总的数据集之间的距离和变小了)</p>
<h3 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h3><p>K-means算法, 也称为K平均或者K均值, 是使用广泛的最基础的聚类算法</p>
<p>假设输入样本为$T=X_1, X_2, \cdots, X_m$, 则算法步骤(使用欧几里得距离公式)为:</p>
<ul>
<li>选择初始化的k个类别中心$a_1, a_2, \cdots, a_k$</li>
<li>对于每个样本$X_i$, 将其标记为距离类别中心$a_j$最近的类别j</li>
<li>更新每个类别的中心点$a_j$为隶属于该类别的所有样本的均值</li>
<li>重复上述两步操作, 直到达到某个终止条件</li>
</ul>
<p>终止条件</p>
<ul>
<li>迭代次数, 最小平方误差MSE, 簇中心点变化率</li>
</ul>
<script type="math/tex; mode=display">label_i = arg \min \limits_{1 \le j \le k} { \sqrt{ \displaystyle \sum_{i=1}^n (x_i - a_j )^2 }  }</script><script type="math/tex; mode=display">a_j = \frac{1}{N(c_j)} \sum_{i \in c_j}x_i</script><h4 id="K-means算法过程"><a href="#K-means算法过程" class="headerlink" title="K-means算法过程"></a>K-means算法过程</h4><p><img src="/L7/WechatIMG143.png"></p>
<p>记K个簇中心分别为$a_1, a_2, \cdots, a_K$, 每个簇的样本数量为$N_1, N_2, \cdots, N_K$<br>使用平方误差作为目标函数(使用欧几里得距离), 公式为:</p>
<script type="math/tex; mode=display">J(a_1, a_2, \cdots, a_K) = \frac{1}{2} \displaystyle \sum_{j=1}^K \sum_{i=1}^{N_j} ( \overrightarrow {x_i} - \overrightarrow {a_j})^2</script><p>要获取最优解, 即使得目标函数尽可能的小, 对函数J求偏导, 可以得到簇中心点a更新对公式:</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial a_j} = \displaystyle \sum_{i=1}^{N_j} (x_i - a_j) \underrightarrow{令} 0 \Rightarrow a_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x_i</script><p>K-means算法在迭代过程中使用所有点的均值作为新的质心(中心点), 如果簇中存在异常点, 将导致均值偏差比较严重<br>比如一个簇中有2, 4, 6, 8, 100五个数据, 那么新的质心为24, 显然这个质心离绝大多数点都比较远, 在当前情况下, 使用中位数6可能比使用均值的想法更好, 使用中位数的聚类方法叫做K-Mediods聚类(K中值聚类)</p>
<h4 id="K-means算法初值敏感"><a href="#K-means算法初值敏感" class="headerlink" title="K-means算法初值敏感"></a>K-means算法初值敏感</h4><p>K-means算法是初值敏感的, 选择不同的初始值可能导致不同的簇划分规则, 为了避免初值敏感最终导致的结果异常, 可以采用初始化多套初始节点构造不同的分类规则, 然后选择最优的构造规则</p>
<p><img src="/L7/WechatIMG144.png" width="500px"></p>
<h4 id="K-means算法初优缺点"><a href="#K-means算法初优缺点" class="headerlink" title="K-means算法初优缺点"></a>K-means算法初优缺点</h4><p>缺点:</p>
<ul>
<li>K值是用户给定的, 在进行数据处理前, K值是未知的, 不同的K值得到的结果也不一样</li>
<li>对初始簇中心点是敏感的</li>
<li>不适合发现非凸形状的簇或者大小差别较大的簇</li>
<li>特殊值(离群值)对模型的影响比较大</li>
</ul>
<p>优点:</p>
<ul>
<li>理解容易, 聚类效果不错</li>
<li>处理大数据集的时候, 该算法可以保证较好的伸缩性和高效率</li>
<li>当簇近似高斯分布的时候, 效果非常不错</li>
</ul>
<h4 id="K-means算法案例"><a href="#K-means算法案例" class="headerlink" title="K-means算法案例"></a>K-means算法案例</h4><h3 id="二分K-means算法"><a href="#二分K-means算法" class="headerlink" title="二分K-means算法"></a>二分K-means算法</h3><p>解决K-means算法对初始簇中心比较敏感的问题, 二分K-means算法是一种弱化初始质心的一种算法, 具体思路步骤如下:</p>
<ul>
<li>将所有样本数据作为一个簇放到一个队列中</li>
<li>从队列中选择一个簇进行K-means算法划分, 划分为两个簇, 并将子簇添加到队列中 </li>
<li>循环迭代第二步操作, 知道终止条件达到(聚簇数量, 最小平方误差, 迭代次数)</li>
<li>队列中的簇就是最终的分类簇集合</li>
</ul>
<p>从队列中选择划分聚簇的规则一般有两种方式:</p>
<ul>
<li>对所有簇计算误差和SSE(SSE也可以认为是距离函数的一种变种), 选择SSE最大的聚簇进行划分操作(优先这种策略)</li>
<li>选择样本数据量最多的簇进行划分操作</li>
</ul>
<script type="math/tex; mode=display">SSE = \displaystyle \sum_{i=1}^n w_i ( \overrightarrow{x_i} - )^2</script><h3 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means++算法"></a>K-means++算法</h3><h3 id="K-means-Vert-算法"><a href="#K-means-Vert-算法" class="headerlink" title="K-means$\Vert$算法"></a>K-means$\Vert$算法</h3><h3 id="Canopy算法"><a href="#Canopy算法" class="headerlink" title="Canopy算法"></a>Canopy算法</h3><h3 id="Mini-Batch-K-means算法"><a href="#Mini-Batch-K-means算法" class="headerlink" title="Mini Batch K-means算法"></a>Mini Batch K-means算法</h3><h3 id="K-means和Mini-Batch-K-means算法比较案例"><a href="#K-means和Mini-Batch-K-means算法比较案例" class="headerlink" title="K-means和Mini Batch K-means算法比较案例"></a>K-means和Mini Batch K-means算法比较案例</h3><h3 id="聚类算法的衡量指标"><a href="#聚类算法的衡量指标" class="headerlink" title="聚类算法的衡量指标"></a>聚类算法的衡量指标</h3><ul>
<li>混淆矩阵</li>
<li>均一性</li>
<li>完整性</li>
<li>V-measure</li>
<li>调整兰德系数(ARI)</li>
<li>调整互信息(AMI)</li>
<li>轮廓系数(Silhouette)</li>
</ul>
<h4 id="均一性"><a href="#均一性" class="headerlink" title="均一性"></a>均一性</h4><ul>
<li>均一性: 一个簇中只包含一个类别的样本, 则满足均一性; 其实也可以认为是正确率(每个聚簇中正确分类的样本数占该聚簇总样本书的比例和)<script type="math/tex; mode=display">p = \frac{1}{k} \displaystyle \sum_{i=1}^{k} \frac{N(C_i == K_i)}{N(K_i)}</script>N表示数量, $C_i$表示实际的类别, $K_i$表示在簇中间的类别, 总共k个簇</li>
</ul>
<h4 id="完整性"><a href="#完整性" class="headerlink" title="完整性"></a>完整性</h4><ul>
<li>完整性: 同类别样本被归类到相同簇中, 则满足完整性; 每个聚簇中正确分类的样本数占该类型的总样本比例的和, 和之前讲的”召回率”有点类似<br><script type="math/tex">r = \frac{1}{k} \displaystyle \sum_{i=1}^{k} \frac{N(C_i == K_i)}{N(C_i)}</script> - V-measure: 均一性和完整性的加权平均</li>
</ul>
<h4 id="V-measure"><a href="#V-measure" class="headerlink" title="V-measure"></a>V-measure</h4><script type="math/tex; mode=display">v_\beta = \frac{(1 + \beta^2) \cdot pr}{\beta^2 \cdot p + r}</script><h4 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h4><ul>
<li>簇内不相似度</li>
<li>簇间不相似度</li>
<li>轮廓系数</li>
</ul>
<h2 id="层次聚类方法"><a href="#层次聚类方法" class="headerlink" title="层次聚类方法"></a>层次聚类方法</h2><h3 id="凝聚的层次聚类AGNES算法"><a href="#凝聚的层次聚类AGNES算法" class="headerlink" title="凝聚的层次聚类AGNES算法"></a>凝聚的层次聚类AGNES算法</h3><h3 id="分裂的层次聚类DIANA算法"><a href="#分裂的层次聚类DIANA算法" class="headerlink" title="分裂的层次聚类DIANA算法"></a>分裂的层次聚类DIANA算法</h3><h3 id="AGNES和DIANA算法优缺点"><a href="#AGNES和DIANA算法优缺点" class="headerlink" title="AGNES和DIANA算法优缺点"></a>AGNES和DIANA算法优缺点</h3><h3 id="AGNES算法中簇间距离"><a href="#AGNES算法中簇间距离" class="headerlink" title="AGNES算法中簇间距离"></a>AGNES算法中簇间距离</h3><h3 id="层次聚类优化算法"><a href="#层次聚类优化算法" class="headerlink" title="层次聚类优化算法"></a>层次聚类优化算法</h3><h4 id="BIRCH算法"><a href="#BIRCH算法" class="headerlink" title="BIRCH算法"></a>BIRCH算法</h4><h4 id="CURE算法"><a href="#CURE算法" class="headerlink" title="CURE算法"></a>CURE算法</h4><h3 id="BRICH算法案例"><a href="#BRICH算法案例" class="headerlink" title="BRICH算法案例"></a>BRICH算法案例</h3><h2 id="密度聚类算法"><a href="#密度聚类算法" class="headerlink" title="密度聚类算法"></a>密度聚类算法</h2><p>密度聚类方法的指导思想是: 只要样本点的密度大于某个阈值, 则将样本添加到最近的簇中</p>
<p>这类算法可以客服基于距离的算法只能发现凸聚类的缺点, 可以发现任意形状的聚类, 而且对噪声数据不敏感</p>
<p>计算复杂度高, 计算量大<br>常用的算法有:</p>
<ul>
<li>DBSCAN</li>
<li>密度最大值算法</li>
</ul>
<h3 id="DBSCAN算法"><a href="#DBSCAN算法" class="headerlink" title="DBSCAN算法"></a>DBSCAN算法</h3><h3 id="密度最大值聚类算法-MDCA"><a href="#密度最大值聚类算法-MDCA" class="headerlink" title="密度最大值聚类算法(MDCA)"></a>密度最大值聚类算法(MDCA)</h3><h3 id="密度聚类算法案例"><a href="#密度聚类算法案例" class="headerlink" title="密度聚类算法案例"></a>密度聚类算法案例</h3><h2 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h2><h3 id="拉普拉斯矩阵变换"><a href="#拉普拉斯矩阵变换" class="headerlink" title="拉普拉斯矩阵变换"></a>拉普拉斯矩阵变换</h3><h3 id="谱聚类应用场景及面临的问题"><a href="#谱聚类应用场景及面临的问题" class="headerlink" title="谱聚类应用场景及面临的问题"></a>谱聚类应用场景及面临的问题</h3><h3 id="谱聚类应用案例"><a href="#谱聚类应用案例" class="headerlink" title="谱聚类应用案例"></a>谱聚类应用案例</h3><h2 id="聚类综合案例"><a href="#聚类综合案例" class="headerlink" title="聚类综合案例"></a>聚类综合案例</h2><h3 id="不同聚类算法在不同数据分布情况下的聚类效果"><a href="#不同聚类算法在不同数据分布情况下的聚类效果" class="headerlink" title="不同聚类算法在不同数据分布情况下的聚类效果"></a>不同聚类算法在不同数据分布情况下的聚类效果</h3><h3 id="图片压缩"><a href="#图片压缩" class="headerlink" title="图片压缩"></a>图片压缩</h3><h2 id="有约束的最优化问题"><a href="#有约束的最优化问题" class="headerlink" title="有约束的最优化问题"></a>有约束的最优化问题</h2><ul>
<li>最优化问题一般是指对于某一个函数而言, 求解在其指定作用域上的全局最小值问题, 一般分为三种情况, 这三种方法求出来的解都有可能有局部最小值, 只有当函数是凸函数的时候, 才可以得到全局最小值<ul>
<li>无约束问题, 一般求解方式为梯度下降法, 牛顿法, 坐标轴下降法, $ \displaystyle \min \limits_{x}f(x)$</li>
<li>等式约束条件, 求解方式为拉格朗日乘子法, $\displaystyle \min \limits_{x}f(x); s.t: h_k(x)=0, k=1,2,\cdots, p$</li>
<li>不等式约束条件, 求解方式为KKT条件<script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x), s.t: h_k(x)=0, k=1,2,\cdots, p, g_j(x) \le 0, j=1, 2, \cdots, q</script></li>
</ul>
</li>
</ul>
<h3 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h3><script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x), s.t: h_i(x)=0, i=1,2,\cdots, p</script><script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x) + \displaystyle \sum_{i=1}^{p} \alpha_i h_i(x); \quad \alpha_i \ne 0</script><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>在优化问题中, 目标函数f(x)存在多种形式, 如果目标函数和约束条件都为变量x的线性函数, 则称问题为线性规划; 如果目标函数为二次函数, 则称优化问题为二次规划; 如果目标函数或者约束条件为非线性函数, 则称最优化问题为非线性规划. 每个线性规划问题都有一个对偶问题</p>
<ul>
<li>对偶问题的对偶是愿问题</li>
<li>无论原始问题是否是凸的, 对偶问题都是凸优化问题</li>
<li>对偶问题可以给出原始问题的一个下界</li>
<li>当满足一定条件的时候, 原始问题和对偶问题的解是完美等价的</li>
</ul>
<h3 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h3><p>KKT条件是泛拉格朗日乘子法的一种形式, 主要应用在当我们的优化函数存在不等值约束的情况下的一种最优化解决方式; KKT条件即满足不等值约束的条件</p>
<script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x); \quad s.t.: h_k(x)=0, k=1,2,\cdots,p; \quad g_j(x) \le 0, j=1, 2, \cdots, q</script><script type="math/tex; mode=display">L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{p}\alpha_ih_i(x) + \sum_{i=1}^{q}\beta_ig_i(x);\quad\alpha_i \ne 0, \beta_i \ge 0</script><script type="math/tex; mode=display">\min \limits_{x}L(x, \alpha, \beta)</script><p>考虑简化的形式</p>
<script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x); \quad s.t.: g_j(x) \le 0, j=1, 2, \cdots, q</script><script type="math/tex; mode=display">L(x, \beta) = f(x) + \sum_{i=1}^{q}\beta_ig_i(x);\quad \beta_i \ge 0</script><p><img src="/L7/微信图片_20190622104414.png" width="300px"></p>
<p>从上式可以看到, $\beta_i g_i(x) \le 0$, 那么它的最大值顶多是0, 那么$f(x) = L(x, \beta) - \displaystyle \sum_{i=1}^q{\beta_ig_i(x)}$, 由此可以得到$L(x, \beta)$的最大值顶多就是$f(x)$, 所以最后如果要求解$f(x)$的最小值, 即可以求解$\displaystyle \max \limits_{\beta}L(x, \beta)$的最小值</p>
<h3 id="KKT条件总结"><a href="#KKT条件总结" class="headerlink" title="KKT条件总结"></a>KKT条件总结</h3><ul>
<li>拉格朗日取得可行解的充要条件, $\nabla_xL(x, \alpha, \beta) = 0$</li>
<li>将不等式约束转换后的一个约束, 称为松弛互补条件, $\beta_ig_i(x) = 0, \quad i=1,2,\cdots,q$</li>
<li>初始约束条件, $h_i(x)=0, \quad i=1,2,\cdots,p$</li>
<li>初始约束条件, $g_i(x)\le0,\quad i=1,2,\cdots,q$</li>
<li>不等式约束需要满足的条件, $\beta_i\ge0,\quad i=1,2,\cdots,q$</li>
</ul>
<p>参考<a href="https://www.cnblogs.com/liaohuiqiang/p/7805954.html" target="_blank" rel="noopener">拉格朗日乘子法和KKT条件</a></p>
<p><br><br>

	<div class="row">
    <embed src="./KKT.pdf" width="100%" height="550" type="application/pdf">
	</div>



<br></p>
<h2 id="感知器模型"><a href="#感知器模型" class="headerlink" title="感知器模型"></a>感知器模型</h2><p>感知器模型的前提是数据线性可分</p>
<p>对于m个样本, 每个样本n维特征以及一个二元类别输出y, 如下:</p>
<script type="math/tex; mode=display">\lbrace (x_1^{(1)}, x_2^{(1)}, \cdots, x_n^{(1)}), (x_1^{(2)}, x_2^{(2)}, \cdots, x_n^{(2)}), \cdots, (x_1^{(m)}, x_2^{(m)}, \cdots, x_n^{(m)}) \rbrace</script><p>目标是找到一个超平面, 即</p>
<script type="math/tex; mode=display">\theta_0 + \theta_1x_1 + \cdots + \theta_nx_n = 0 \Rightarrow \theta x = 0</script><p>让一个类别的数据满足$\theta x &gt; 0$, 另外一个类别满足$\theta x &lt; 0$<br>感知器模型, $y=sign(\theta x )\begin{cases}<br>+1, \theta x &gt; 0\\<br>-1, \theta x &lt; 0\\<br>\end{cases}$</p>
<p>定义正确分类为: $y\theta x&gt;0$, 定义错误分类为: $y\theta x&lt;0$, 所以定义的损失函数为: 期望使分类错误的所有样本(m条样本)到超平面的距离之和最小</p>
<script type="math/tex; mode=display">L = \displaystyle \sum_{i=1}^{m} \frac{-y^{(i)}\theta x^{(i)}}{\Vert \theta \Vert_2}</script><p>分子分母简化后</p>
<script type="math/tex; mode=display">L = \displaystyle - \sum_{i=1}^{m} y^{(i)}\theta x^{(i)}</script><p>直接使用梯度下降法对损失函数进行求解, 由于m是分类错误对样本集合, 不是固定的, 所以我们不能使用批量梯度下降法(BGD)求解, 只能使用随机梯度下降(SGD)和小批量梯度下降法(MBGD), 一般使用SGD求解</p>
<script type="math/tex; mode=display">\frac{\partial L(\theta)}{\partial \theta} = - \sum_{i=1}^{m} y^{(i)}\theta x^{(i)}</script><script type="math/tex; mode=display">\theta^{k+1} = \theta^k + \alpha y^{(i)}x^{(i)}</script><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><ul>
<li>梯度下降法, 拉格朗日乘子法, KKT条件回顾</li>
<li>感知器模型</li>
<li>SVM线性可分</li>
<li>SVM线性不可分</li>
<li>核函数</li>
<li>SMO</li>
</ul>
<h3 id="线性可分SVM"><a href="#线性可分SVM" class="headerlink" title="线性可分SVM"></a>线性可分SVM</h3><p>在感知器模型中, 是可以找到多个可以分类的超平面将数据分开. 并且希望所有的点都离超平面尽可能的远, 但是实际上离超平面足够远的点基本上都是被正确分类的, 所以这个是没有意义的. 反而是离超平面很近的点, 这些点比较容易分错. 所以只要让离超平面比较近的点尽可能的远离这个超平面, 这样模型分类效果就会不错, 此即为SVM的思想.</p>
<p><img src="/L7/微信图片_20190624114549.png" width="300px"></p>
<ul>
<li>支持向量到超平面的距离为:<script type="math/tex; mode=display">\because w^T x + b = \pm 1</script><script type="math/tex; mode=display">\because y \in \{+1, -1\}</script><script type="math/tex; mode=display">\therefore \displaystyle \frac{y(w^Tx+b)}{\Vert w \Vert_2} = \frac{1}{\Vert w \Vert_2}</script></li>
</ul>
<p><strong>在SVM中支持向量到超平面的函数距离一般设置为1</strong></p>
<p>SVM模型是让所有的分类点在各自类别的支持向量的两边(见公式(2)), 同时要求支持向量尽可能的远离这个超平面(见公式(1)), 用数学公式表示如下:</p>
<script type="math/tex; mode=display">\max \limits_{w, b} \displaystyle \frac{1}{\Vert w \Vert_2} \tag{1}</script><script type="math/tex; mode=display">s.t: y^{(i)}(w^Tx^{(i)}+b) \ge 1, \quad i=1, 2, \cdots, m \tag{2}</script><script type="math/tex; mode=display">\because w^T = (w_1, w_2, \cdots, w_n)</script><script type="math/tex; mode=display">\therefore \Vert w \Vert_2 = \sqrt{w_1^2 + w_2^2 + \cdots + w_n^2}</script><p>优化问题等价于$ \min \limits_{w, b} \Vert w \Vert_2; \quad s.t: y^{(i)}(w^Tx^{(i)}+b) \ge 1, \quad i=1, 2, \cdots, m$<br>那么SVM的目标函数/损失函数为:</p>
<script type="math/tex; mode=display">J(w) = \displaystyle \frac{1}{2} \Vert w \Vert_2^2; \quad s.t: y^{(i)}(w^Tx^{(i)}+b) \ge 1, \quad i=1, 2, \cdots, m</script><p>优化目标为$w^{\ast}, b^{\ast} = \min \limits_{w, b} J(w)$<br>这时可以将此此时的目标函数和约束条件使用KKT条件转换为拉格朗日函数, 从而转化为无约束优化函数:</p>
<script type="math/tex; mode=display">J(w) = \displaystyle \frac{1}{2} \Vert w \Vert_2^2; \quad s.t: 1 - y^{(i)}(w^Tx^{(i)}+b) \le 0, \quad i=1, 2, \cdots, m</script><script type="math/tex; mode=display">L(w, b, \beta) = \displaystyle \frac{1}{2} \Vert w \Vert_2^2 + \sum_{i=1}^m \beta_i[1 - y^{(i)}(w^Tx^{(i)} + b)], \quad \beta_i \ge 0</script><p>引入拉格朗日乘子之后, 优化目标变成了:</p>
<script type="math/tex; mode=display">\min \limits_{w, b} \max \limits_{\beta \ge 0} L(w, b, \beta)</script><p>根据拉格朗日对欧化特性, 将优化目标转换为等价的对偶问题</p>
<script type="math/tex; mode=display">\max \limits_{\beta \ge 0} \min \limits_{w, b} L(w, b, \beta)</script><p>对于该优化函数, 要先求优化函数对于w和b的极小值, 然后再求解对于拉格朗日乘子$\beta$的极大值</p>
<p>优化函数L对于w和b的极小值, 可以通过对函数L分别对w和b求偏导数得到</p>
<script type="math/tex; mode=display">\displaystyle \frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)}</script><script type="math/tex; mode=display">\displaystyle \frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^{m} \beta_i y^{(i)} = 0</script><p>将求解出的w和b带入优化函数L中, 那么优化后的函数如下:</p>
<script type="math/tex; mode=display">\begin{align} \mathcal{l}(\beta) &= \displaystyle \frac{1}{2} \Vert w \Vert_2^2 + \sum_{i=1}^{m} \beta_i [1 - y^{(i)}(w^Tx^{(i)} + b)]\\
&= \frac{1}{2} w^T w - \sum_{i=1}^{m} \beta_i y^{(i)} w^T x^{(i)} - \sum_{i=1}^{m} \beta_i y^{(i)} b + \sum_{i=1}^{m} \beta_i\\
&= \frac{1}{2} w^T \sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)} - \sum_{i=1}^{m} \beta_i y^{(i)} w^T x^{(i)} - \sum_{i=1}^{m} \beta_i y^{(i)} b + \sum_{i=1}^{m} \beta_i\\
&= - \frac{1}{2} w^T \sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)} - b \sum_{i=1}^{m} \beta_i y^{(i)} + \sum_{i=1}^{m} \beta_i \\
&= - \frac{1}{2} w^T \sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \beta_i \\
&= - \frac{1}{2} (\sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)})^T (\sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)}) + \sum_{i=1}^{m} \beta_i \\
&= - \frac{1}{2} (\sum_{i=1}^{m} \beta_i y^{(i)} {(x^{(i)})}^T) (\sum_{i=1}^{m} \beta_i y^{(i)} x^{(i)}) + \sum_{i=1}^{m} \beta_i \\
&= - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \beta_i \beta_j y^{(i)} y^{(j)} {x^{(i)}}^T x^{(j)} + \sum_{i=1}^{m} \beta_i \\
\end{align}</script><script type="math/tex; mode=display">\therefore l(\beta) = - \frac{1}{2} \sum_{i=1, j=1}^{m} \beta_i \beta_j y^{(i)} y^{(j)} {x^{(i)}}^T x^{(j)} + \sum_{i=1}^{m} \beta_i; \quad s.t: \sum_{i=1}^{m} \beta_i y^{(i)} = 0, \quad \beta_i \ge 0, \quad i=1, 2, \cdots, m</script><p>此时得到的优化函数只和$\beta$有关, 这时直接最大化优化函数得到$\beta$值, 从而最终得到w和b的值</p>
<script type="math/tex; mode=display">\max \limits_{\beta \ge 0} l(\beta); \quad s.t: \sum_{i=1}^{m} \beta_i y^{(i)} = 0</script><p>该优化问题等价于:</p>
<script type="math/tex; mode=display">\min \limits_{\beta \ge 0} - l(\beta); \quad s.t: \sum_{i=1}^{m} \beta_i y^{(i)} = 0</script><p>即:</p>
<script type="math/tex; mode=display">\min \limits_{\beta_i \ge 0} l(\beta) = \min \limits_{\beta_i \ge 0} \frac{1}{2} ( \sum_{i=1, j=1}^{m} \beta_i \beta_j y^{(i)} y^{(j)} {x^{(i)}}^T x^{(j)} - \sum_{i=1}^{m} \beta_i); \quad s.t: \sum_{i=1}^{m} \beta_i y^{(i)} = 0</script><p><strong>$\beta$值的求解采用SMO算法</strong></p>
<p>假设求解得到的$\beta$值为$\beta^{\ast}$, 则根据w, b, $\beta$的关系, 分别计算如下:</p>
<script type="math/tex; mode=display">w^{\ast} = \displaystyle \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} x^{(i)}</script><p>b的计算采用所有支持向量的计算均值</p>
<script type="math/tex; mode=display">\because y^s (w^T x^s +b) = y^s (\displaystyle \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} {x^{(i)}}^T x^s + b ) = 1</script><script type="math/tex; mode=display">\therefore b^{\ast} = \frac{1}{y^s} - \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} {x^{(i)}}^T x^s</script><script type="math/tex; mode=display">又\because y^s \in \{1, -1\}</script><script type="math/tex; mode=display">\therefore b^{\ast} = {y^s} - \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} {x^{(i)}}^T x^s</script><p>在这里$(x^s, y^s)$是支持向量, 根据KKT条件中的对欧互补条件(松弛约束条件), 支持向量必须满足以下公式:</p>
<script type="math/tex; mode=display">\beta_i (y^{(i)}(w^T x^{(i)} + b) - 1 ) = 0</script><script type="math/tex; mode=display">\{ (x^{(i), y^{(i)}}) \quad | \quad \beta_i > 0, \quad i=1,2,\cdots,m \}</script><h3 id="线性可分SVM算法流程"><a href="#线性可分SVM算法流程" class="headerlink" title="线性可分SVM算法流程"></a>线性可分SVM算法流程</h3><p>输入线性可分的m个样本数据$\{ (x^1, y^1), (x^2, y^2), \cdots, (x^m, y^m)\}$, 其中x为n维的特征向量, y为二元输出, 取值为$+1$或者$-1$, SVM输出为参数$w$, $b$以及分类决策函数.</p>
<ul>
<li>构造约束优化维问题, $\displaystyle \min \limits_{\beta_i \ge 0} \frac{1}{2} ( \sum_{i=1, j=1}^{m} \beta_i \beta_j y^{(i)} y^{(j)} {x^{(i)}}^T x^{(j)} - \sum_{i=1}^{m} \beta_i); \quad s.t: \sum_{i=1}^{m} \beta_i y^{(i)} = 0 $</li>
<li>使用SMO算法求出上述优化中对应的最优解$\beta^{\ast}$</li>
<li>找出所有的支持向量集合$S = \{ (x^{(i)}, y^{(i)}) \quad | \quad \beta_i &gt; 0, \quad i=1,2,\cdots,m \}$</li>
<li>更新参数$w^{\ast}$和$b^{\ast}$的值, $w^{\ast} = \displaystyle \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} x^{(i)}, \quad b^{\ast} = \frac{1}{s} \sum_{s=1}^{S} ({y^s} - \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} {x^{(i)}}^T x^s) $</li>
<li>构建最终的分类决策函数, $f(x)=sign(w^{\ast} \cdot x + b^{\ast})$</li>
</ul>
<script type="math/tex; mode=display">\beta_1 \beta_1 3*3*3*3 + \beta_1 \beta_2 3*3*3*4 - \beta_1 \beta_33*1*3*1 + \beta_2 \beta_1 3*3*4*3 + \beta_2 \beta_2 3*3*4*4 - \beta_2 \beta_3 3*1*4*1 - \beta_3 \beta_1 1*3*1*3 - \beta_3 \beta_2 1*3*4*1 - \beta_3 \beta_3 1*1*1*1</script><script type="math/tex; mode=display">81\beta_1 \beta_1 + 216\beta_1 \beta_2 - 18\beta_1 \beta_3 + 144 \beta_2 \beta_2 - 24 \beta_2 \beta_3 + \beta_3 \beta_3</script><p>这里的例子没有理解</p>
<p><a href="https://www.jb51.net/article/131591.htm" target="_blank" rel="noopener">svm参考例子1</a></p>
<h3 id="线性可分SVM总结"><a href="#线性可分SVM总结" class="headerlink" title="线性可分SVM总结"></a>线性可分SVM总结</h3><ul>
<li>要求数据必须是线性可分的</li>
<li>纯线性可分的SVM模型对于异常数据的预测可能会不太准</li>
<li>对于线性可分的数据, SVM分类器的效果非常不错</li>
</ul>
<h3 id="SVM的软间隔模型"><a href="#SVM的软间隔模型" class="headerlink" title="SVM的软间隔模型"></a>SVM的软间隔模型</h3><p>线性可分SVM中要求数据必须是线性可分的, 才可以找到分类的超平面, 但是有的时候线性数据集中存在少量的异常点, 由于这些异常点导致了数据集不能够线性可分; 正常数据是线性可分的, 但是由于存在异常点数据, 导致数据集不能够线性可分<br><img src="/L7/WechatIMG37.png" width="250px"></p>
<p>如果线性数据中存在异常点导致没法直接使用SVM线性分割模型的时候, 那我们可以通过引入软间隔的概念来解决这个问题</p>
<ul>
<li>硬间隔: 认为线性可分SVM中的距离度量就是硬间隔, 在线性划分SVM中, 要求函数距离一定是大于1的, 最大化硬间隔条件为:<script type="math/tex; mode=display">\min \limits_{w,b} \frac{1}{2} \Vert w \Vert_{2}^{2}; \quad s.t: \quad y^{(i)}(w^Tx^{(i)} + b) \le 1, i=1,2,\cdots,m</script></li>
<li>软间隔: SVM对于训练集中的每个样本都引入一个松弛因子($\xi$), 使得函数距离加上松弛因子后的值是大于或者等于1; 这表示相对于硬间隔, 对样本到超平面距离的要求放松了<script type="math/tex; mode=display">y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i, i=1,2,\cdots,m, \quad \xi_i \ge 0</script></li>
<li><p>松弛因子($\xi$)越大, 表示样本点离超平面越近, 如果松弛因子大于1, 那么允许该样本点分错, 所以加入松弛因子是有成本的, 过大的松弛因子可能会导致模型分类错误, 所以最终的目标函数就转换成为:</p>
<script type="math/tex; mode=display">\min \limits_{w,b} \frac{1}{2} \Vert w \Vert_{2}^{2} + C \displaystyle \sum_{i=1}^{m} \xi_i</script><script type="math/tex; mode=display">y^{(i)}(w^Tx^{(i)} + b) \ge 1 - \xi_i, \quad \xi_i \ge 0, \quad i=1,2,\cdots,m</script><p>其中, 函数中的$C &gt; 0$是惩罚系数, 是一个超参, 类似于L1/L2 norm中的参数; C越大表示对误分类的惩罚越大, C越小表示对误分类的惩罚越小; C值的给定需要调参</p>
</li>
<li><p>同线性可分SVM, 构造软间隔最大化的约束问题对应的拉格朗日函数如下:</p>
<script type="math/tex; mode=display">L(w,b,\xi,\beta,\mu) = \frac{1}{2} \Vert w \Vert_{2}^{2} + C \displaystyle \sum_{i=1}^{m}\xi_i + \displaystyle \sum_{i=1}^{m} \beta_i [1-\xi_i-y^{(i)}(w^Tx^{(i)}+b)]-\displaystyle \sum_{i=1}^{m}\mu_i \xi_i, \quad \beta_i \ge 0, \mu_i \ge 0</script><p>从而可以将优化目标函数转换为:</p>
<script type="math/tex; mode=display">\min \limits_{w,b,\xi} \max \limits_{\beta, \mu} L(w, b, \xi, \beta, \mu)</script></li>
<li>优化条件同样满足KTT条件, 所有使用拉格朗日对偶将优化问题转换为等价的对偶问题<script type="math/tex; mode=display">\max \limits_{\beta, \mu} \min \limits_{w,b,\xi} L(w, b, \xi, \beta, \mu)</script></li>
</ul>
<p>先求优化函数对于$w,b,\xi$的极小值, 这个可以通过分别优化函数L求$w,b,\xi$的偏导数, 从而可以得到$w,b,\xi$关于$\beta$和$\mu$之间的关系</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w} = 0 \Rightarrow w - \sum_{i=1}^{m}\beta_iy^{(i)}x^{(i)} = 0 \Rightarrow w = \sum_{i=0}^{m}\beta_iy^{(i)}x^{(i)}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b} = 0 \Rightarrow - \sum_{i=1}^{m}\beta_iy^{(i)} = 0 \Rightarrow \sum_{i=1}^{m}\beta_iy^{(i)} =0</script><script type="math/tex; mode=display">\frac{\partial L}{\partial \xi_i} = 0 \Rightarrow C - \beta_i - \mu_i = 0</script><p>将$w,b,\xi$的值带入L函数中, 就可以消去优化函数中的$w,b,\xi$, 定义优化中后的函数如下:</p>
<script type="math/tex; mode=display">l(\beta) = \sum_{i=1}^{m}\beta_i - \frac{1}{2}\sum_{i=1,j=1}^{m}\beta_i\beta_jy^{(i)}y^{(j)}x^{(i)^{T}}x^{(j)}; \quad s.t: \sum_{i=1}^{m}\beta_iy^{(i)}=0, C-\beta_i - \mu_i = 0, \beta_i \ge 0, i=1,2,\cdots,m, \mu_i \ge 0, i=1,2,\cdots,m</script><p>最终优化后的目标函数/损失函数和线性可分SVM模型基本一样, 除了约束条件不同而已, 也就是说也可以使用SMO算法来求解</p>
<script type="math/tex; mode=display">\min\frac{1}{2}\sum_{i=1,j=1}^{m}\beta_i\beta_jy^{(i)}y^{(j)}x^{(i)^{T}}x^{(j)} - \sum_{i=1}^{m}\beta_i</script><script type="math/tex; mode=display">s.t: \sum_{i=1}^{m}\beta_iy^{(i)}=0, 0 \le \beta_i \le C, i=1,2,\cdots,m</script><ul>
<li>在硬间隔最大化的时候, 支持向量比较简单, 就是离超平面的函数距离为1的样本点就是支持向量</li>
<li>在软间隔中, 根据KKT条件中的对偶互补条件: $\beta(y(wx+b)-1+\xi)=0$, 从而有:<ul>
<li>当$0&lt;\beta_i \le C$的时候, 并且$\xi_i=0$的样本点均是支持向量(即所有的$0&lt;\beta_i &lt; C$). 即满足$|wx+b|=1$的所有样本均是支持向量</li>
<li>软间隔和硬间隔中的支持向量的规则是一样的<br><img src="/L7/WechatIMG38.png" width="300px"></li>
</ul>
</li>
</ul>
<h3 id="SVM软间隔模型算法流程"><a href="#SVM软间隔模型算法流程" class="headerlink" title="SVM软间隔模型算法流程"></a>SVM软间隔模型算法流程</h3><p>输入线性可分的m个样本数据${(x^1, y^1), (x^2, y^2), \cdots, (x^m, y^m)}$, 其中x为n维的特征向量, y为二元输出, 取值为+1或者-1； SVM模型输出为参数$w,b$以及分类决策函数</p>
<ul>
<li>选择一个惩罚系数C&gt;0, 构造约束优化问题, $\displaystyle \min \limits_{\beta_i \ge 0} \frac{1}{2} ( \sum_{i=1, j=1}^{m} \beta_i \beta_j y^{(i)} y^{(j)} {x^{(i)}}^T x^{(j)} - \sum_{i=1}^{m} \beta_i); \quad s.t: \sum_{i=1}^{m} \beta_i y^{(i)} = 0, 0 \le \beta_i \le C, i=1,2,\cdots,m$</li>
<li>使用SMO算法求出上述优化中对应的最优解$\beta^*$</li>
<li>找出所有的支持向量集合$S = \{ (x^{(i)}, y^{(i)}) \quad | \quad 0 &lt; \beta_i \le C,  \xi_i = 0, i = 1, 2, \cdots, m $</li>
<li>更新参数$w^{\ast}$和$b^{\ast}$的值, $w^{\ast} = \displaystyle \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} x^{(i)}, \quad b^{\ast} = \frac{1}{s} \sum_{s=1}^{S} ({y^s} - \sum_{i=1}^{m} \beta_i^{\ast} y^{(i)} {x^{(i)}}^T x^s) $</li>
<li>构建最终的分类器$f(x)=sign(w^{\ast} x + b^{\ast})$</li>
</ul>
<h3 id="SVM软间隔模型总结"><a href="#SVM软间隔模型总结" class="headerlink" title="SVM软间隔模型总结"></a>SVM软间隔模型总结</h3><ul>
<li>可以解决线性数据中携带异常点的分类模型构建的问题</li>
<li>通过引入惩罚系数(松弛因子), 可以增加模型的泛化能力, 即鲁棒性</li>
<li>如果给定的惩罚系数越小, 表示在模型构建的时候, 就允许存在越多的分类错误的样本, 表示此时模型的准确率会比较低; 如果惩罚系数越大, 表示模型在构建的时候, 就越不允许存在分类错误的样本, 表示此时模型的准确率会比较高</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/信息熵和决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/信息熵和决策树/" itemprop="url">信息熵和决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-31T16:39:25+08:00">
                2018-12-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>H(X)就叫做随机变量X的信息熵</p>
<script type="math/tex; mode=display">H(X) = - \displaystyle \sum_{i=1}^{m}{p_ilog_{2}{(p_i)}}</script><ul>
<li>信息量: 指的是一个样本/事件所蕴含的信息, 如果一个事件的概率越大, 就认为该事件所蕴含的信息越少. 极端情况下, “太阳从东方升起”, 因为是确定事件, 所以不携带任何信息量</li>
<li>信息熵, 一个系统越有序, 信息熵就越低, 一个系统越混乱, 信息熵就越高, 所以信息熵被认为是一个系统有序程度的度量</li>
<li><strong>信息熵就是用来描述系统信息量的不确定度</strong></li>
<li>High Entropy(高信息熵): 表示随机变量X是均匀分布的, 各种去追情况是等概率出现</li>
<li>Low Entropy(低信息熵): 表示随机变量X各种取值不是等概率出现, 可能出现有的事件概率很大, 有的事件概率很小</li>
</ul>
<p><img src="/信息熵和决策树/WechatIMG32.png" alt="信息熵"></p>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><ul>
<li>给定条件X的情况下, 随机变量Y的信息熵叫做条件熵</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>专业(X)</th>
<th>性别(Y)</th>
</tr>
</thead>
<tbody>
<tr>
<td>数学</td>
<td>M</td>
</tr>
<tr>
<td>IT</td>
<td>M</td>
</tr>
<tr>
<td>英语</td>
<td>F</td>
</tr>
<tr>
<td>数学</td>
<td>F</td>
</tr>
<tr>
<td>数学</td>
<td>M</td>
</tr>
<tr>
<td>IT</td>
<td>M</td>
</tr>
<tr>
<td>英语</td>
<td>F</td>
</tr>
<tr>
<td>数学</td>
<td>F</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">P(X=数学) = 4/8 = 0.5</script><script type="math/tex; mode=display">P(X=IT) = 2/8 = 0.25</script><script type="math/tex; mode=display">P(X=英语) = 2/8 =0.25</script><script type="math/tex; mode=display">P(Y=M) = 4/8 = 0.5</script><script type="math/tex; mode=display">P(X=数学, Y=F) = 2/8 = 0.25</script><script type="math/tex; mode=display">P(X=数学, Y=M) = 2/8 = 0.25</script><script type="math/tex; mode=display">P(X=IT, Y=M) = 2/8 = 0.25</script><script type="math/tex; mode=display">P(X=英语, Y=F) = 2/8 = 0.25</script><script type="math/tex; mode=display">P(Y=M|X=英语) = 0/8 = 0</script><script type="math/tex; mode=display">\begin{align}H(X) &= -[P(X=数学)log_2P(X=数学) + P(X=IT)log_2P(X=IT) + P(X=英语)log_2P(X=英语)]\\
&=-[0.5log_{2}0.5 + 0.25 log_{2}0.25 + 0.25 log_{2}0.25]\\
&=-[0.5 \cdot (-1) + 0.25 \cdot (-2) + 0.25 \cdot (-2)] = 1.5\\
\end{align}</script><script type="math/tex; mode=display">\begin{align}H(Y) &= -[P(Y=F)log_2P(Y=F) + P(Y=M)log_2P(Y=M)]\\
&=-[0.5log_{2}0.5 + 0.5log_{2}0.5] = 1\\
\end{align}</script><ul>
<li>当专业(X)为数学的时候, Y的信息熵的值为H(Y|X=数学)</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>专业(X)</th>
<th>性别(Y)</th>
</tr>
</thead>
<tbody>
<tr>
<td>数学</td>
<td>M</td>
</tr>
<tr>
<td>数学</td>
<td>F</td>
</tr>
<tr>
<td>数学</td>
<td>M</td>
</tr>
<tr>
<td>数学</td>
<td>F</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">H(Y|X=数学) = H(Y=M|X=数学) + H(Y=F|X=数学) = -[0.5 log_{2}0.5 + 0.5 log_{2}0.5] = 1</script><p>同理</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>专业(X)</th>
<th>性别(Y)</th>
</tr>
</thead>
<tbody>
<tr>
<td>IT</td>
<td>M</td>
</tr>
<tr>
<td>IT</td>
<td>M</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">H(Y|X=IT) = H(Y=M|X=IT) = -[1 log_{2}1] = 0</script><div class="table-container">
<table>
<thead>
<tr>
<th>专业(X)</th>
<th>性别(Y)</th>
</tr>
</thead>
<tbody>
<tr>
<td>英语</td>
<td>F</td>
</tr>
<tr>
<td>英语</td>
<td>F</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">H(Y|X=英语) = H(Y=F|X=英语) = -[1 log_{2}1 ] = 0</script><div class="table-container">
<table>
<thead>
<tr>
<th>$v_j$</th>
<th>$P(X=v_j)$</th>
<th>$H(Y \mid X=v_j)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>数学</td>
<td>0.5</td>
<td>1</td>
</tr>
<tr>
<td>IT</td>
<td>0.25</td>
<td>0</td>
</tr>
<tr>
<td>英语</td>
<td>0.25</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">H(Y|X) = \displaystyle \sum_{j=1}{P(X=v_j)H(Y|X=v_j)} = 0.5*1 + 0.25*0 + 0.25*0 = 0.5</script><script type="math/tex; mode=display">\begin{align}H(X,Y) &= -[P(X=数学,Y=F)log(P(X=数学,Y=F)) + P(X=数学,Y=M) log( P(X=数学,Y=M)) + P(X=IT,Y=M) log( P(X=IT,Y=M) ) + P(X=英语,Y=F) log( P(X=英语,Y=F) )]\\
&= -[ 0.25log_{2}0.25 + 0.25log_{2}0.25 + 0.25log_{2}0.25 + 0.25log_{2}0.25 ] \\
&= -[-0.5 - 0.5 - 0.5 - 0.5] = 2\\
\end{align}</script><script type="math/tex; mode=display">\begin{align}H(X) &= -[P(X=数学)log( P(X=数学) ) + P(X=IT)log( P(X=IT) ) + P(X=英语) log( P(X=英语) )]\\
&=-[0.5log_{2}0.5 + 0.25log_{2}0.25 + 0.25log_{2}0.25] = -[-0.5 - 0.5 - 0.5] = 1.5\\
\end{align}</script><script type="math/tex; mode=display">H(Y|X) = H(X,Y)−H(X) = 2 - 1.5 = 0.5</script><ul>
<li>事件(X, Y)发生所包含的熵, 减去事件X单独发生的熵, 即为在事件X发生的前提下, Y发生”新”带来的熵<script type="math/tex; mode=display">\begin{align} H(Y|X) &= \sum_{j=1}{P(X=v_j)H(Y|X=v_j) = \sum_xP(X)H(Y|X)}\\
&= \sum_xp(x)(- \sum_yp(y|x)log(p(y|x)) = -\sum_x \sum_y p(x)p(y|x)log(p(y|x))\\
&= - \sum_x \sum_y p(x, y)log(\frac {p(x, y)} {p(x)})\\
&= - \sum_x \sum_y p(x, y)log(p(x,y)) - [- \sum_x (\sum_y p(x,y))log(p(x))]\\
&= H(X,Y) - [- \sum_x p(x)log(p(x))] = H(X, Y) -H(X)
\end{align}</script></li>
</ul>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><ul>
<li>决策树(Decision Tree)是在已知各种情况发生概率的基础上, 通过构建决策树来进行分析的一种方式, 是一种直观应用概率分析的一种图解法; 决策树是一种预测模型, 代表的是对象属性与对象值之间的映射关系; 决策树是一种树形结构, 其中每个内部节点表示一个属性的测试, 每个分支表示一个测试输出, 每个叶节点代表一种类别; 决策树是一种非常常用的有监督的分类算法</li>
<li>决策树的决策过程就是从根节点开始, 测试待分类项中对应的特征属性, 并按照其值选择输出分支, 直到叶子节点, 将叶子节点的存放类别作为决策结果</li>
<li>决策树分为两大类: 分类树和回归树, 前者用于分类标签值, 后者用于预测连续值, 常用的算法有ID3, C4.5, CART等</li>
</ul>
<h3 id="决策树构建过程"><a href="#决策树构建过程" class="headerlink" title="决策树构建过程"></a>决策树构建过程</h3><ul>
<li>决策树算法的重点就是决策树的构造; 决策树的构造就是进行属性选择度量, 确定各个特征属性之间的拓扑结构(树结构); 构建决策树的关键步骤就是分裂属性, 分裂属性是指某个节点按照某一类特征属性的不同划分构建不同的分支, 其目标就是让各个分裂子集尽可能的<strong>纯</strong>(让一个分裂子类中待分类的项尽可能的属于同一个类别)</li>
<li>构建步骤如下:<ul>
<li>将所有的特征看成一个一个的节点;</li>
<li>遍历每个特征的每一种分割方式, 找到最好的分割点; 将数据划分为不同的子节点, $N_1, N_2, \cdots N_m$, 计算划分后所有子节点的”纯度”信息;</li>
<li>对第二步产生的分割, 选择出最优的特征以及最优的划分方式; 得出最终的子节点$N_1, N_2, \cdots N_m$</li>
<li>对子节点$N_1, N_2, \cdots N_m$分别继续执行2-3步, 直到每个最终的子节点都足够”纯”</li>
</ul>
</li>
</ul>
<h3 id="决策树特征属性类型"><a href="#决策树特征属性类型" class="headerlink" title="决策树特征属性类型"></a>决策树特征属性类型</h3><p>根据特征属性的类型不同, 在构建决策树的时候, 采用不同的方式, 具体如下:</p>
<ul>
<li>属性是离散值, 而且不要求生成的是二叉决策树, 此时一个属性就是一个分支</li>
<li>属性是离散值, 而且要求生成的是二叉决策树, 此时使用属性划分的子集进行测试, 按照”属于此子集”和”不属于此子集”分成两个分支</li>
<li>属性是连续值, 可以确定一个值作为分裂点split_point, 按照<code>&gt; split_point</code>和$\le$<code>split_point</code>生成两个分支</li>
</ul>
<h3 id="决策树分割属性选择"><a href="#决策树分割属性选择" class="headerlink" title="决策树分割属性选择"></a>决策树分割属性选择</h3><ul>
<li>决策树算法是一种”贪心”算法策略, 只考虑在当前数据特征下的最好分割方式, 不能进行回溯操作</li>
<li>对于整体的数据集而言, 按照所有的特征属性进行划分操作, 对所有划分操作的结果集的”纯度”进行比较, 选择”纯度”越高的特征属性作为当前需要分割的数据集进行分割操作, 持续迭代, 直到得到最终结果. 决策树是通过”纯度”来进行分割特征属性点的</li>
</ul>
<h3 id="决策树量化纯度"><a href="#决策树量化纯度" class="headerlink" title="决策树量化纯度"></a>决策树量化纯度</h3><ul>
<li>决策树的构建是通过样本概率和纯度进行构建操作的, 那么进行判断数据集是否”纯”可以通过三个公式进行判断, 分别是Gini系数, 熵(Entropy), 错误率, 这三个公式值越大, 表示数据越”不纯”; 越小表示越”纯”, 这三个公式效果差不多, 一般使用熵公式<script type="math/tex; mode=display">P(1)=7/10=0.7; 可以偿还概率</script><script type="math/tex; mode=display">P(2)=3/10=0.3; 无法偿还概率</script></li>
</ul>
<script type="math/tex; mode=display">Gini = 1 - \sum_{i=1}^{n}{P(i)^2}, H(Entropy) = -\sum_{i=1}^{n}{P(i)log_2(P(i))}, Error = 1 - \max_{i=1}^{n}{P(i)}</script><ul>
<li>当计算出各个特征属性的量化纯度值后使用<strong>信息增益度</strong>来选择出当前数据集的分割特征属性; 如果信息增益度的值越大, 表示在该特征属性上会损失的纯度越大, 那么该属性就越应该在决策树的上层, 计算公式为:<script type="math/tex; mode=display">Gain = \Delta = H(D) - H(D|A)</script><ul>
<li>Gain为特征A对训练集D的信息增益, 它为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差</li>
</ul>
</li>
</ul>
<h3 id="决策树算法的停止条件"><a href="#决策树算法的停止条件" class="headerlink" title="决策树算法的停止条件"></a>决策树算法的停止条件</h3><p>一般情况下有两个停止条件:</p>
<ul>
<li>当每个子节点只有一种类型的时候停止构建</li>
<li>当前节点中记录数小于某个阈值, 同时迭代次数达到给定值时, 停止构建过程, 此时使用$max(p(i))$作为节点的对应类型<br>方式一可能会使树的节点过多, 导致过拟合等问题; 比较常用的方式是使用方式二作为停止条件</li>
</ul>
<h3 id="决策树算法效果评估"><a href="#决策树算法效果评估" class="headerlink" title="决策树算法效果评估"></a>决策树算法效果评估</h3><ul>
<li>决策树的效果评估和一般的分类算法一样, 采用混淆举证来进行计算准确率, 召回率和精确率等指标</li>
<li>也可以采用叶子节点的纯度值总和来评估算法的效果, 值越小, 效果越好</li>
</ul>
<h2 id="决策树生成算法"><a href="#决策树生成算法" class="headerlink" title="决策树生成算法"></a>决策树生成算法</h2><p>建立决策树主要有三种算法</p>
<ul>
<li>ID3</li>
<li>C4.5</li>
<li>CART(Classification And Regression Tree)</li>
</ul>
<h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>ID3算法是决策树比较经典的一个构造算法, 内部使用信息熵以及信息增益来进行构造; 每次迭代选择信息增益最大的特征属性作为分割属性</p>
<script type="math/tex; mode=display">H(D) = - \displaystyle \sum_{i=1}^{n}{P(i)log_2(P(i))}</script><script type="math/tex; mode=display">Gain = \Delta = H(D) - H(D|A)</script><p>优点: 决策树构建速度快, 实现简单<br>缺点:</p>
<ul>
<li>计算依赖于特征数目比较多的特征, 而属性值最多的属性不一定最优</li>
<li>ID3算法不是递增算法</li>
<li>ID3算法是单变量决策树, 对于特征属性之间的关系不会考虑</li>
<li>抗噪性差</li>
<li>只适合小规模数据集, 需要将数据放到内存中</li>
</ul>
<h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><p>使用信息增益率来取代ID3算法中的信息增益, 在树的构造过程中会进行剪枝操作进行优化; 能够自动完成对连续属性的离散化处理; C4.5算法在选中分割属性的时候选择信息增益率最大的属性, 涉及到的公式为:</p>
<script type="math/tex; mode=display">H(D) = - \displaystyle \sum_{i=1}^{n}{P(i)log_2(P(i))}</script><script type="math/tex; mode=display">Gain = \Delta = H(D) - H(D|A)</script><script type="math/tex; mode=display">Gain_ratio(A) = \frac{Gain(A)}{H(A)}</script><p>优点:</p>
<ul>
<li>产生的规则容易理解</li>
<li>准确率较高</li>
<li>实现简单</li>
</ul>
<p>缺点:</p>
<ul>
<li>对数据集需要进行多次顺序扫描和排序, 效率较低</li>
<li>只适合小规模数据集, 需要将数据放到内存中</li>
</ul>
<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>使用基尼系数作为数据纯度的量化指标来构建决策树, 使用GINI增益作为分割属性选择的标准, 选择GINI增益最大的作为当前数据集的分割属性; 可用于分类和回归两类问题. CART构建是二叉树</p>
<script type="math/tex; mode=display">Gini = 1 - \sum_{i=1}^{n}{P(i)^2}</script><script type="math/tex; mode=display">Gain = \Delta = Gini(D) - Gini(D|A)</script><h3 id="ID3-C4-5-CART分类算法总结"><a href="#ID3-C4-5-CART分类算法总结" class="headerlink" title="ID3, C4.5, CART分类算法总结"></a>ID3, C4.5, CART分类算法总结</h3><ul>
<li>ID3和C4.5算法均只适合在小规模数据集上使用</li>
<li>ID3和C4.5算法都是单变量决策树</li>
<li>当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差</li>
<li>决策树分类一般情况只适合小数据量的情况(数据可以放内存)</li>
<li>CART算法是三种算法中最常用的一种决策树构建算法</li>
<li>三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益, C4.5使用信息增益率、CART使用基尼系数</li>
<li>CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
<th>特征属性多次使用</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益率</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类, 回归</td>
<td>二叉树</td>
<td>基尼系数, 均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
</div>
<h3 id="决策树优化策略"><a href="#决策树优化策略" class="headerlink" title="决策树优化策略"></a>决策树优化策略</h3><ul>
<li>剪枝优化, 决策树过拟合一般情况是由于节点太多导致的, 剪枝优化对决策树的正确率影响是比较大的, 也是最常用的一种优化方式</li>
<li>Random Forest, 利用训练数据随机产生多个决策树, 形成一个森林, 然后使用这个森林对数据进行预测, 选取最多结果作为预测结果</li>
</ul>
<h4 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h4><ul>
<li>前置剪枝: 在构建决策树过程中, 提前停止, 得到的决策树一般比较小, 实践证明这种策略无法得到比较好的结果</li>
<li>后置剪枝: 在决策树构建好之后, 然后再开始裁剪, 一般使用两种方法, 后置剪枝的主要问题是计算效率问题, 存在一定的浪费情况<ul>
<li>用单一叶子节点代替整个子树, 叶子节点的分类采用子树中最主要的分类</li>
<li>将一个子树完全代替另外一颗子树</li>
</ul>
</li>
<li>后剪枝总体思路(交叉验证):<ul>
<li>由完全树$T_0$开始, 剪枝部分节点得到$T_1$, 再次剪枝得到$T_2$……直到仅剩树根的树$T_k$</li>
<li>在验证数据集上对这$k+1$个树进行评价, 选择最优树$T_a$(即损失函数最小的树)</li>
</ul>
</li>
<li>对于给定的决策树$T_0$<ul>
<li>计算所有内部非叶子节点的剪枝系数</li>
<li>查找最小剪枝系数的节点, 将其子节点进行删除操作, 进行剪枝得到决策树$T_k$; 如果存在多个最小剪枝系数节点, 选择包含数据项最多的节点进行了剪枝操作</li>
<li>重复上述操作, 直到产生的剪枝决策树$T_k$只有1个节点</li>
<li>得到决策树$T_0T_1 \cdots T_k$</li>
<li>使用验证样本集选择最优子树$T_a$</li>
</ul>
</li>
<li>使用验证集选择最优子树的标准, 可以使用原始损失函数来考虑:<script type="math/tex; mode=display">loss = \displaystyle \sum_{t=1}^{leaf}{ \frac{\vert D_t \vert}{\vert D \vert}H(t)}</script></li>
<li>叶节点越多, 决策树越复杂, 损失越大; 修正添加剪枝系数, 修改后的损失函数为:<script type="math/tex; mode=display">loss_{\alpha} = loss + \alpha * leaf</script></li>
<li>考虑根节点为r的子树, 剪枝前后的损失函数分别为loss(R)和loss(r), 当这两者相等的时候, 可以求的剪枝系数<script type="math/tex; mode=display">loss_{\alpha}(r) = loss(r) + \alpha</script><script type="math/tex; mode=display">loss_{\alpha}(R) = loss(R) + \alpha * R_{leaf}</script><script type="math/tex; mode=display">\alpha = \displaystyle \frac{loss(r) - loss(R)}{R_{leaf} - 1}</script></li>
</ul>
<p>GridSearchCV: 网格交叉验证, 主要用于模型开发阶段找出模型的最优参数的一种方式, 内部会利用交叉验证</p>
<p>现在对于A和B的每个参数组合都进行一次k折交叉验证, 将k折交叉验证得到的k个模型的score(model.score(x,y))均值作为当前这组参数在训练集上的模型整体效果, GridSearchCV最终认为模型整体效果最优的对应参数是最优参数</p>
<h3 id="分类树和回归树的区别"><a href="#分类树和回归树的区别" class="headerlink" title="分类树和回归树的区别"></a>分类树和回归树的区别</h3><ul>
<li>分类树采用信息增益、信息增益率、基尼系数来评价树的效果，都是基于概率值进行判断的;而分类树的叶子节点的预测值一般为叶子节点中概率最大的类别作为当前叶子的预测值。</li>
<li>在回归树种，叶子节点的预测值一般为叶子节点中所有值的均值来作为当前叶子 节点的预测值。所以在回归树中一般采用MSE作为树的评价指标，即均方差。<script type="math/tex; mode=display">MSE = \frac{1}{n} \sum_{i=1}^{n}{(y_i - \hat{y_i})^2}</script></li>
<li>一般情况下只使用CART算法构建回归树</li>
</ul>
<p>使用特征树实现特征选择<br>因为决策树构建过程中, 每次选择的划分特征的目的/方向是让数据具有更加明显的区分能力; 也就是我们每次选择的特征其实是具有明显的区分能力的, 可以认为这些被选择的特征其实对于y的取值具有更大的影响, 所有我们可以使用决策树来实现特征选择的操作, 即选择clf.feature_importances_返回列表中, 权重比较大的特征 </p>
<p>决策树</p>
<ul>
<li><p>区别</p>
<ul>
<li>分类树中使用信息熵, gini系数, 错误率作为”纯度”的度量指标, 回归树中使用MSE/MAE作为树的”纯度”的度量指标</li>
<li>分类树使用叶子节点中包含最多的那个类别作为当前叶子的预测值, 回归树中使用叶子节点中包含的所有样本的目标属性的均值作为叶子的预测值</li>
</ul>
</li>
<li><p>决策树的构建</p>
<ul>
<li>让每次分裂数据集的时候, 让分裂之后的数据集更加的”纯”</li>
</ul>
</li>
<li><p>决策树分裂属性选择方式</p>
<ul>
<li>基于最优划分的规则进行选择, 迭代计算特征属性上所有划分方式的”纯度”, 选择划分后更加”纯”的一种方式(信息增益, 信息增益率), 该方法只能说明在当前数据集上是最优的, 可能会存在一定的过拟合情况</li>
<li>基于随机的划分规则: 每次划分的时候, 都是先选择一定数目的特征, 然后在这部分特征中选择出一个最优的划分特征. 因为每次选择的划分特征都是局部最优的, 相对来讲可以增加模型的鲁棒性</li>
</ul>
</li>
<li><p>决策树的欠拟合和过拟合</p>
<ul>
<li>通过增加树的深度来缓解决策树的欠拟合问题</li>
<li>通过限制树的复杂程度来缓解这个过拟合的问题</li>
</ul>
</li>
<li><p>网格交叉验证(GridSearchCV)</p>
</li>
<li>决策树的效果评估</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/matplotlib/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/matplotlib/" itemprop="url">matplotlib</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-04T16:48:30+08:00">
                2018-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="显示"><a href="#显示" class="headerlink" title="显示"></a>显示</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib tk  # 在GUI中显示</span><br><span class="line">%matplotlib inline  # 行内显示, 默认是行内显示</span><br></pre></td></tr></table></figure>
<h3 id="颜色，标记，-线形"><a href="#颜色，标记，-线形" class="headerlink" title="颜色，标记， 线形"></a>颜色，标记， 线形</h3><ul>
<li>LineStyle 线形</li>
<li>LineWidth 线宽</li>
<li>Color 颜色</li>
<li>Marker 标记点的形状</li>
<li>Label 图例的标签</li>
</ul>
<h3 id="刻度-标题-标签和图例"><a href="#刻度-标题-标签和图例" class="headerlink" title="刻度, 标题, 标签和图例"></a>刻度, 标题, 标签和图例</h3><ul>
<li>legend, 为了展示每个数据对应的图像名称和数据结构, 生成默认图例</li>
<li>xlabel/ylabel, 设置x/y轴标签</li>
<li>title, 设置标题</li>
<li>xlim/ylim, 控制图标的范围</li>
<li>xticks/yticks, 控制图标的刻度</li>
<li>gca, 获取当前坐标轴信息</li>
<li>spines, 设置边框</li>
<li>set_color, 设置边框颜色</li>
</ul>
<h3 id="中文显示问题"><a href="#中文显示问题" class="headerlink" title="中文显示问题"></a>中文显示问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">x1 = [1,2,3]</span><br><span class="line">y1 = [5,7,4]</span><br><span class="line">x2 = [1,2,3]</span><br><span class="line">y2 = [10,14,12]</span><br><span class="line"></span><br><span class="line">plt.plot(x1, y1, &apos;ro--&apos;, label=&quot;1st line&quot;)</span><br><span class="line">plt.plot(x2, y2, &apos;b-&apos;, label=&quot;2nd line&quot;)</span><br><span class="line">plt.xlabel(&apos;月份&apos;)</span><br><span class="line">plt.ylabel(&apos;年份&apos;)</span><br><span class="line">plt.xlim(1,3)</span><br><span class="line">plt.ylim(0,15)</span><br><span class="line">plt.xticks(np.linspace(0, 6, 5))</span><br><span class="line">plt.yticks(np.arange(1, 15, 3), [&apos;2011年&apos;, &apos;2012年&apos;, &apos;2013年&apos;, &apos;2014年&apos;, &apos;2015年&apos;])</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</span><br><span class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="subplot子图"><a href="#subplot子图" class="headerlink" title="subplot子图"></a>subplot子图</h3><p>figure对象下边创建一个或多个subplot对象(即axes)用于绘制图像</p>
<p><code>subplot(numRows, numCols, plotNum)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">x1 = [1,2,3]</span><br><span class="line">y1 = [5,7,4]</span><br><span class="line">x2 = [1,2,3]</span><br><span class="line">y2 = [10,14,12]</span><br><span class="line"></span><br><span class="line">plt.subplot(221)</span><br><span class="line">plt.plot(x1, y1, &apos;r-&apos;)</span><br><span class="line">plt.subplot(224)</span><br><span class="line">plt.plot(x2, y2, &apos;b--&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>面向对象的形式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()  # figure实例, 可以添加axes实例</span><br><span class="line">ax = fig.add_subplot(111)  # 返回axes实例, 参数1是子图的总行数, 参数2是子图的总列数, 参数3是子图的位置</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(8, 6))</span><br><span class="line">ax1 = fig.add_subplot(221)</span><br><span class="line">ax2 = fig.add_subplot(222)</span><br><span class="line">ax3 = fig.add_subplot(223)</span><br><span class="line"></span><br><span class="line">ax1.plot(np.random.randn(50).cumsum(), &apos;g-&apos;)</span><br><span class="line">ax2.plot(np.random.randn(50).cumsum(), &apos;b--&apos;)</span><br><span class="line">ax3.plot(np.random.randn(50).cumsum(), &apos;k--&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="subplots"><a href="#subplots" class="headerlink" title="subplots"></a>subplots</h3><p>返回一个图像和多个子图<br>参数: nrows=x, ncols=x, sharex=True, sharey=False, gridspec_kw={‘height_ratios’:[2, 2, 1, 1]}</p>
<p><code>fig, ax = plt.subplots(2, 2)</code>, 参数表示子图的行数和列数, 总共2 * 2个子图, 函数返回一个fig图像和一个子图ax的array列表.</p>
<p>e.g.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(nrows=4, ncols=1, sharex=True, sharey=False)</span><br><span class="line"></span><br><span class="line">print(dir(fig))</span><br><span class="line">fig.suptitle(&apos;test&apos;, fontsize=20)</span><br><span class="line"></span><br><span class="line">axes[0].plot(range(10), &apos;ro-&apos;)</span><br><span class="line">axes[1].plot(range(10), &apos;bo-&apos;)</span><br><span class="line">axes[2].plot(range(10), &apos;go-&apos;)</span><br><span class="line">axes[3].plot(range(10), &apos;mo-&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>e.g.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(2, 2)</span><br><span class="line"></span><br><span class="line">for i in range(2):</span><br><span class="line">    for j in range(2):</span><br><span class="line">        axes[i, j].hist(np.random.randn(100), 10, color=&apos;g&apos;, alpha=0.75)</span><br><span class="line">fig.subplots_adjust(wspace=0, hspace=0)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h3 id="图像保存文件"><a href="#图像保存文件" class="headerlink" title="图像保存文件"></a>图像保存文件</h3><p><code>plt.savefit(文件名称)</code></p>
<h3 id="matplotlib柱状图"><a href="#matplotlib柱状图" class="headerlink" title="matplotlib柱状图"></a>matplotlib柱状图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matplotlib.pyplot.bar(*args, **kwargs)</span><br><span class="line">bar(x, height, width, bottom, *args, align=&apos;center&apos;, **kwargs)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.bar([1,3,5,7,9,11], [5,2,7,8,2,6], label=&quot;xxoo&quot;, color=&apos;y&apos;)</span><br><span class="line">plt.bar([2,4,6,8,10,12], [8,6,2,5,6,3], label=&quot;ooxx&quot;, color=&apos;g&apos;)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(&apos;bar number&apos;)</span><br><span class="line">plt.ylabel(&apos;bar height&apos;)</span><br><span class="line"></span><br><span class="line">plt.title(&apos;hello&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="matplotlib直方图"><a href="#matplotlib直方图" class="headerlink" title="matplotlib直方图"></a>matplotlib直方图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]</span><br><span class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br><span class="line"></span><br><span class="line">mu, sigma = 100, 15</span><br><span class="line">x = mu + sigma * np.random.randn(10000)</span><br><span class="line">plt.hist(x, 100, normed=1, facecolor=&apos;g&apos;, alpha=0.75)</span><br><span class="line">plt.title(&apos;直方图&apos;)</span><br><span class="line">plt.text(60, 0.025, r&apos;$\mu=100, \ \sigma=15$&apos;)</span><br><span class="line">plt.axis([40, 160, 0, 0.03])</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="matplotlib散点图"><a href="#matplotlib散点图" class="headerlink" title="matplotlib散点图"></a>matplotlib散点图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = [1, 2, 3, 4, 5, 6, 7, 8]</span><br><span class="line">y = [5, 2, 4, 2, 1, 4, 5, 2]</span><br><span class="line">T = np.random.rand(8) * 125</span><br><span class="line">plt.scatter(x, y, label=&apos;散点分布&apos;, c=T, s=25, marker=&apos;o&apos;, alpha=0.5)</span><br><span class="line">plt.xlabel(&apos;x&apos;)</span><br><span class="line">plt.ylabel(&apos;y&apos;)</span><br><span class="line">plt.title(&apos;散点图&apos;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/qq_34337272/article/details/79555544" target="_blank" rel="noopener">参考</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/pandas/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/pandas/" itemprop="url">pandas</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-03T07:23:09+08:00">
                2018-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Pandas的数据结构: Pandas主要有Series(一维数组), DataFrame(二维数组), Panel(三维数组), Panel4D(四维数组), PanelND(更多维数组)等数据结构. 其中Series和DataFrame应用的最为广泛。</p>
<p>Series是一维带标签的数组, 它可以包含任何数据类型. 包括整数, 字符串, 浮点数, Python对象等. Series可以通过标签来定位.<br>DataFrame是二维的带标签的数据结构. 我们可以通过标签来定位数据. 这是NumPy所没有的.</p>
<h2 id="查看pandas版本"><a href="#查看pandas版本" class="headerlink" title="查看pandas版本"></a>查看pandas版本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(pd.__version__)</span><br><span class="line">0.24.2</span><br><span class="line">&gt;&gt;&gt; pd.show_versions()</span><br></pre></td></tr></table></figure>
<h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><h3 id="创建Series"><a href="#创建Series" class="headerlink" title="创建Series"></a>创建Series</h3><p>Series可以看做由一列数据组成的数据集<br>Series语法如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = pd.Series(data, index=index)</span><br></pre></td></tr></table></figure></p>
<p>常用的创建Series的方法有如下三种:</p>
<h4 id="从列表创建Series"><a href="#从列表创建Series" class="headerlink" title="从列表创建Series"></a>从列表创建Series</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; arr = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]</span><br><span class="line">&gt;&gt;&gt; s1 = pd.Series(arr)  # 如果不指定index, 则默认从0开始</span><br><span class="line">&gt;&gt;&gt; s1</span><br><span class="line">0    a</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    d</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h4 id="从Ndarray创建Series"><a href="#从Ndarray创建Series" class="headerlink" title="从Ndarray创建Series"></a>从Ndarray创建Series</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; n = np.random.randn(5)  # 几行几列</span><br><span class="line">&gt;&gt;&gt; index = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]</span><br><span class="line">&gt;&gt;&gt; s2 = pd.Series(n, index)</span><br><span class="line">&gt;&gt;&gt; s2</span><br><span class="line">a    0.077125</span><br><span class="line">b   -0.554191</span><br><span class="line">c    0.167562</span><br><span class="line">d   -1.214207</span><br><span class="line">e   -1.038312</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h4 id="从字典创建Series"><a href="#从字典创建Series" class="headerlink" title="从字典创建Series"></a>从字典创建Series</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; dict = &#123;&apos;a&apos;:1, &apos;b&apos;:2, &apos;c&apos;:3, &apos;d&apos;:4&#125;</span><br><span class="line">&gt;&gt;&gt; s3 = pd.Series(d)</span><br><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<h3 id="Seires基本操作"><a href="#Seires基本操作" class="headerlink" title="Seires基本操作"></a>Seires基本操作</h3><h4 id="修改Series索引"><a href="#修改Series索引" class="headerlink" title="修改Series索引"></a>修改Series索引</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s1</span><br><span class="line">0    a</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    d</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; s1.index = [&apos;a1&apos;,&apos;a2&apos;,&apos;a3&apos;,&apos;a4&apos;]</span><br><span class="line">&gt;&gt;&gt; s1</span><br><span class="line">a1    a</span><br><span class="line">a2    b</span><br><span class="line">a3    c</span><br><span class="line">a4    d</span><br><span class="line">dtype: object</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; s1.index[0] = &apos;01&apos; </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    File &quot;/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py&quot;, line 3938, in __setitem__</span><br><span class="line">        raise TypeError(&quot;Index does not support mutable operations&quot;)</span><br><span class="line">        TypeError: Index does not support mutable operations</span><br><span class="line"># index不支持可变操作</span><br></pre></td></tr></table></figure>
<h4 id="Series纵向拼接"><a href="#Series纵向拼接" class="headerlink" title="Series纵向拼接"></a>Series纵向拼接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s1</span><br><span class="line">a    a</span><br><span class="line">b    b</span><br><span class="line">c    c</span><br><span class="line">d    d</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s4 = s3.append(s1)</span><br><span class="line">&gt;&gt;&gt; s4</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">a    a</span><br><span class="line">b    b</span><br><span class="line">c    c</span><br><span class="line">d    d</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h4 id="Series按照指定索引删除元素"><a href="#Series按照指定索引删除元素" class="headerlink" title="Series按照指定索引删除元素"></a>Series按照指定索引删除元素</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s4.drop(&apos;a&apos;)</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">b    b</span><br><span class="line">c    c</span><br><span class="line">d    d</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h4 id="Series修改指定索引的元素"><a href="#Series修改指定索引的元素" class="headerlink" title="Series修改指定索引的元素"></a>Series修改指定索引的元素</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s4[&apos;a&apos;] = [11, &apos;a1&apos;]</span><br><span class="line">&gt;&gt;&gt; s4</span><br><span class="line">a    11</span><br><span class="line">b     2</span><br><span class="line">c     3</span><br><span class="line">d     4</span><br><span class="line">a    a1</span><br><span class="line">b     b</span><br><span class="line">c     c</span><br><span class="line">d     d</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h4 id="Series按照指定索引查找元素"><a href="#Series按照指定索引查找元素" class="headerlink" title="Series按照指定索引查找元素"></a>Series按照指定索引查找元素</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s4[&apos;a&apos;]      </span><br><span class="line">a    1</span><br><span class="line">a    a</span><br></pre></td></tr></table></figure>
<h4 id="Series切片"><a href="#Series切片" class="headerlink" title="Series切片"></a>Series切片</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s1</span><br><span class="line">0    a</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    d</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; s1[1:3]</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h3 id="Series运算"><a href="#Series运算" class="headerlink" title="Series运算"></a>Series运算</h3><h4 id="Series加法运算"><a href="#Series加法运算" class="headerlink" title="Series加法运算"></a>Series加法运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s1</span><br><span class="line">0    a</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    d</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; s2</span><br><span class="line">0    e</span><br><span class="line">1    f</span><br><span class="line">2    g</span><br><span class="line">3    h</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; s1.add(s2)</span><br><span class="line">0    ae</span><br><span class="line">1    bf</span><br><span class="line">2    cg</span><br><span class="line">3    dh</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; s2.add(s1)</span><br><span class="line">0    ea</span><br><span class="line">1    fb</span><br><span class="line">2    gc</span><br><span class="line">3    hd</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h4 id="Series减法运算"><a href="#Series减法运算" class="headerlink" title="Series减法运算"></a>Series减法运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s4</span><br><span class="line">a     4</span><br><span class="line">bb    3</span><br><span class="line">c     2</span><br><span class="line">d     1</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.sub(s4)</span><br><span class="line">a    -3.0</span><br><span class="line">b     NaN</span><br><span class="line">bb    NaN</span><br><span class="line">c     1.0</span><br><span class="line">d     3.0</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h4 id="Series乘法运算"><a href="#Series乘法运算" class="headerlink" title="Series乘法运算"></a>Series乘法运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s4</span><br><span class="line">a     4</span><br><span class="line">bb    3</span><br><span class="line">c     2</span><br><span class="line">d     1</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.mul(s4)</span><br><span class="line">a     4.0</span><br><span class="line">b     NaN</span><br><span class="line">bb    NaN</span><br><span class="line">c     6.0</span><br><span class="line">d     4.0</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h4 id="Series除法运算"><a href="#Series除法运算" class="headerlink" title="Series除法运算"></a>Series除法运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s4</span><br><span class="line">a     4</span><br><span class="line">bb    3</span><br><span class="line">c     2</span><br><span class="line">d     1</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.div(s4)</span><br><span class="line">a     0.25</span><br><span class="line">b      NaN</span><br><span class="line">bb     NaN</span><br><span class="line">c     1.50</span><br><span class="line">d     4.00</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h4 id="Series求中位数"><a href="#Series求中位数" class="headerlink" title="Series求中位数"></a>Series求中位数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.median()</span><br><span class="line">2.5</span><br></pre></td></tr></table></figure>
<h4 id="Series求和"><a href="#Series求和" class="headerlink" title="Series求和"></a>Series求和</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.sum()</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<h4 id="Series最大值"><a href="#Series最大值" class="headerlink" title="Series最大值"></a>Series最大值</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.max()</span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<h4 id="Series最小值"><a href="#Series最小值" class="headerlink" title="Series最小值"></a>Series最小值</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s3</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; s3.min()</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<h4 id="Series缺失值的检测"><a href="#Series缺失值的检测" class="headerlink" title="Series缺失值的检测"></a>Series缺失值的检测</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; series_10 = pd.Series(&#123;&apos;a&apos;:10, &apos;b&apos;:20, &apos;c&apos;:30, &apos;d&apos;:40&#125;)</span><br><span class="line">&gt;&gt;&gt; series_10.index</span><br><span class="line">Index([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;], dtype=&apos;object&apos;)</span><br><span class="line">&gt;&gt;&gt; series_20 = pd.Series(series_10,index=new_index)</span><br><span class="line">&gt;&gt;&gt; series_20</span><br><span class="line">a    10.0</span><br><span class="line">b    20.0</span><br><span class="line">c    30.0</span><br><span class="line">d    40.0</span><br><span class="line">e     NaN</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<p><code>isnull</code>和<code>notnull</code>检测Series中的缺失值, 返回bool类型的Series值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; pd.isnull(ser_10) </span><br><span class="line">a    False</span><br><span class="line">b    False</span><br><span class="line">c    False</span><br><span class="line">dtype: bool</span><br><span class="line">&gt;&gt;&gt; pd.isnull(ser_20)</span><br><span class="line">a    False</span><br><span class="line">b    False</span><br><span class="line">c    False</span><br><span class="line">d     True</span><br><span class="line">dtype: bool</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pd.notnull(ser_10)</span><br><span class="line">a    True</span><br><span class="line">b    True</span><br><span class="line">c    True</span><br><span class="line">dtype: bool</span><br><span class="line">&gt;&gt;&gt; pd.notnull(ser_20)</span><br><span class="line">a     True</span><br><span class="line">b     True</span><br><span class="line">c     True</span><br><span class="line">d    False</span><br><span class="line">dtype: bool</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; ser_20</span><br><span class="line">a    10.0</span><br><span class="line">b    20.0</span><br><span class="line">c    30.0</span><br><span class="line">d     NaN</span><br><span class="line">dtype: float64</span><br><span class="line">&gt;&gt;&gt; ser_20[pd.isnull(ser_20)]  # 相当于过滤掉缺失值</span><br><span class="line">d   NaN</span><br><span class="line">dtype: float64</span><br><span class="line">&gt;&gt;&gt; ser_20[pd.notnull(ser_20)]</span><br><span class="line">a    10.0</span><br><span class="line">b    20.0</span><br><span class="line">c    30.0</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure></p>
<h4 id="Series自动对齐"><a href="#Series自动对齐" class="headerlink" title="Series自动对齐"></a>Series自动对齐</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; series_1 = pd.Series([1,2,3,4], index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</span><br><span class="line">&gt;&gt;&gt; series_2 = pd.Series([4,3,2,1,5], index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;])</span><br><span class="line">&gt;&gt;&gt; series_1 + series_2  </span><br><span class="line">a    5.0</span><br><span class="line">b    5.0</span><br><span class="line">c    5.0</span><br><span class="line">d    5.0</span><br><span class="line">e    NaN</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h3 id="Series及其name属性"><a href="#Series及其name属性" class="headerlink" title="Series及其name属性"></a>Series及其name属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; series_1.name = &quot;hello world&quot;</span><br><span class="line">&gt;&gt;&gt; series_1.index.name = &apos;xxoo&apos;</span><br><span class="line">&gt;&gt;&gt; series_1</span><br><span class="line">xxoo</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">Name: hello world, dtype: int64</span><br></pre></td></tr></table></figure>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><h4 id="通过NumPy数组创建DataFrame"><a href="#通过NumPy数组创建DataFrame" class="headerlink" title="通过NumPy数组创建DataFrame"></a>通过NumPy数组创建DataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; dates = pd.date_range(&apos;today&apos;, periods=6)</span><br><span class="line">&gt;&gt;&gt; dates</span><br><span class="line">DatetimeIndex([&apos;2019-05-07 11:19:46.800556&apos;, &apos;2019-05-08 11:19:46.800556&apos;,</span><br><span class="line">               &apos;2019-05-09 11:19:46.800556&apos;, &apos;2019-05-10 11:19:46.800556&apos;,</span><br><span class="line">               &apos;2019-05-11 11:19:46.800556&apos;, &apos;2019-05-12 11:19:46.800556&apos;],</span><br><span class="line">              dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)</span><br><span class="line">&gt;&gt;&gt; num_arr = np.random.randn(6,4)</span><br><span class="line">&gt;&gt;&gt; num_arr</span><br><span class="line">array([[ 0.73018504, -0.92169922, -1.37867963, -1.40692501],</span><br><span class="line">       [ 1.61475442,  0.07078674,  0.59075899, -1.994313  ],</span><br><span class="line">       [ 0.00830036,  0.20939717,  0.50323969,  0.43514028],</span><br><span class="line">       [ 0.23984788,  0.15418066, -1.06420981,  0.58894686],</span><br><span class="line">       [ 1.30096803,  0.87426295, -0.56629328, -0.14877761],</span><br><span class="line">       [ 0.94481793, -1.04911144, -1.39403504,  0.77847438]])</span><br><span class="line">&gt;&gt;&gt; columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;]</span><br><span class="line">&gt;&gt;&gt; df1 = pd.DataFrame(num_arr, dates, columns)</span><br><span class="line">&gt;&gt;&gt; df1</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699 -1.378680 -1.406925</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181 -1.064210  0.588947</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263 -0.566293 -0.148778</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111 -1.394035  0.778474</span><br></pre></td></tr></table></figure>
<h4 id="通过dict创建DataFrame"><a href="#通过dict创建DataFrame" class="headerlink" title="通过dict创建DataFrame"></a>通过dict创建DataFrame</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; data = &#123;&apos;animal&apos;: [&apos;cat&apos;, &apos;cat&apos;, &apos;snake&apos;, &apos;dog&apos;, &apos;dog&apos;, &apos;cat&apos;, &apos;snake&apos;, &apos;cat&apos;, &apos;dog&apos;, &apos;dog&apos;],</span><br><span class="line">...         &apos;age&apos;: [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],</span><br><span class="line">...         &apos;visits&apos;: [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],</span><br><span class="line">...         &apos;priority&apos;: [&apos;yes&apos;, &apos;yes&apos;, &apos;no&apos;, &apos;yes&apos;, &apos;no&apos;, &apos;no&apos;, &apos;no&apos;, &apos;yes&apos;, &apos;no&apos;, &apos;no&apos;]&#125;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; labels = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;]</span><br><span class="line">&gt;&gt;&gt; df2 = pd.DataFrame(data, index=labels)</span><br><span class="line">&gt;&gt;&gt; df2</span><br><span class="line">  animal  age  visits priority</span><br><span class="line">a    cat  2.5       1      yes</span><br><span class="line">b    cat  3.0       3      yes</span><br><span class="line">c  snake  0.5       2       no</span><br><span class="line">d    dog  NaN       3      yes</span><br><span class="line">e    dog  5.0       2       no</span><br><span class="line">f    cat  2.0       3       no</span><br><span class="line">g  snake  4.5       1       no</span><br><span class="line">h    cat  NaN       1      yes</span><br><span class="line">i    dog  7.0       2       no</span><br><span class="line">j    dog  3.0       1       no</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; data = &#123;&apos;成绩&apos;:[90, 90, 99], &apos;姓名&apos;: [&apos;Tony&apos;,&apos;Wayne&apos;,&apos;Moon&apos;], &apos;爱好&apos;:[&apos;篮球&apos;,&apos;排球&apos;,&apos;乒乓球&apos;]&#125;  </span><br><span class="line">&gt;&gt;&gt; df_1 = pd.DataFrame(data)</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">   成绩     姓名   爱好</span><br><span class="line">0  90   Tony   篮球</span><br><span class="line">1  90  Wayne   排球</span><br><span class="line">2  99   Moon  乒乓球</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df_1.index</span><br><span class="line">RangeIndex(start=0, stop=3, step=1)</span><br><span class="line">&gt;&gt;&gt; df_1.index = [&apos;xx&apos;,&apos;oo&apos;,&apos;xxoo&apos;]</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩     姓名   爱好</span><br><span class="line">xx    90   Tony   篮球</span><br><span class="line">oo    90  Wayne   排球</span><br><span class="line">xxoo  99   Moon  乒乓球</span><br></pre></td></tr></table></figure>
<h4 id="DataFrame通过二维数组创建"><a href="#DataFrame通过二维数组创建" class="headerlink" title="DataFrame通过二维数组创建"></a>DataFrame通过二维数组创建</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1 = pd.DataFrame([[&apos;Wayne&apos;, &apos;Tony&apos;, &apos;Moon&apos;],[90, 80, 70]])</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">       0     1     2</span><br><span class="line">0  Wayne  Tony  Moon</span><br><span class="line">1     90    80    70</span><br><span class="line">&gt;&gt;&gt; df_1 = pd.DataFrame([[&apos;Wayne&apos;, &apos;Tony&apos;, &apos;Moon&apos;],[90, 80, 70]], index=[&apos;hello&apos;, &apos;world&apos;])</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">           0     1     2</span><br><span class="line">hello  Wayne  Tony  Moon</span><br><span class="line">world     90    80    70</span><br><span class="line">&gt;&gt;&gt; df_1 = pd.DataFrame([[&apos;Wayne&apos;, &apos;Tony&apos;, &apos;Moon&apos;],[90, 80, 70]], index=[&apos;hello&apos;, &apos;world&apos;], columns=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;])</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">           a     b     c</span><br><span class="line">hello  Wayne  Tony  Moon</span><br><span class="line">world     90    80    70</span><br></pre></td></tr></table></figure>
<h3 id="DataFrame获取数据"><a href="#DataFrame获取数据" class="headerlink" title="DataFrame获取数据"></a>DataFrame获取数据</h3><h4 id="查看index-columns和values"><a href="#查看index-columns和values" class="headerlink" title="查看index, columns和values"></a>查看index, columns和values</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1.index</span><br><span class="line">Index([&apos;hello&apos;, &apos;world&apos;], dtype=&apos;object&apos;)</span><br><span class="line">&gt;&gt;&gt; df_1.columns</span><br><span class="line">Index([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], dtype=&apos;object&apos;)</span><br><span class="line">&gt;&gt;&gt; df_1.values</span><br><span class="line">array([[&apos;Wayne&apos;, &apos;Tony&apos;, &apos;Moon&apos;],</span><br><span class="line">       [90, 80, 70]], dtype=object)</span><br></pre></td></tr></table></figure>
<h4 id="查看前几行和后几行"><a href="#查看前几行和后几行" class="headerlink" title="查看前几行和后几行"></a>查看前几行和后几行</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df2.head()</span><br><span class="line">  animal  age  visits priority</span><br><span class="line">a    cat  2.5       1      yes</span><br><span class="line">b    cat  3.0       3      yes</span><br><span class="line">c  snake  0.5       2       no</span><br><span class="line">d    dog  NaN       3      yes</span><br><span class="line">e    dog  5.0       2       no</span><br><span class="line">&gt;&gt;&gt; df2.tail()</span><br><span class="line">  animal  age  visits priority</span><br><span class="line">f    cat  2.0       3       no</span><br><span class="line">g  snake  4.5       1       no</span><br><span class="line">h    cat  NaN       1      yes</span><br><span class="line">i    dog  7.0       2       no</span><br><span class="line">j    dog  3.0       1       no</span><br></pre></td></tr></table></figure>
<h4 id="通过标签查询单列"><a href="#通过标签查询单列" class="headerlink" title="通过标签查询单列"></a>通过标签查询单列</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699 -1.378680 -1.406925</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181 -1.064210  0.588947</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263 -0.566293 -0.148778</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111 -1.394035  0.778474</span><br><span class="line">&gt;&gt;&gt; df1[&apos;A&apos;]</span><br><span class="line">2019-05-07 11:19:46.800556    0.730185</span><br><span class="line">2019-05-08 11:19:46.800556    1.614754</span><br><span class="line">2019-05-09 11:19:46.800556    0.008300</span><br><span class="line">2019-05-10 11:19:46.800556    0.239848</span><br><span class="line">2019-05-11 11:19:46.800556    1.300968</span><br><span class="line">2019-05-12 11:19:46.800556    0.944818</span><br><span class="line">Freq: D, Name: A, dtype: float64</span><br></pre></td></tr></table></figure>
<h4 id="通过标签查询多列"><a href="#通过标签查询多列" class="headerlink" title="通过标签查询多列"></a>通过标签查询多列</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1[[&apos;A&apos;,&apos;B&apos;]]</span><br><span class="line">                                   A         B</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111</span><br></pre></td></tr></table></figure>
<h4 id="通过位置查询"><a href="#通过位置查询" class="headerlink" title="通过位置查询"></a>通过位置查询</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1.iloc[1:3]</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br></pre></td></tr></table></figure>
<h4 id="查看DataFrame的统计数据"><a href="#查看DataFrame的统计数据" class="headerlink" title="查看DataFrame的统计数据"></a>查看DataFrame的统计数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1.describe()</span><br><span class="line">              A         B         C         D</span><br><span class="line">count  6.000000  6.000000  6.000000  6.000000</span><br><span class="line">mean   0.806479 -0.110364 -0.551537 -0.291242</span><br><span class="line">std    0.613343  0.736756  0.902708  1.149970</span><br><span class="line">min    0.008300 -1.049111 -1.394035 -1.994313</span><br><span class="line">25%    0.362432 -0.673578 -1.300062 -1.092388</span><br><span class="line">50%    0.837501  0.112484 -0.815252  0.143181</span><br><span class="line">75%    1.211931  0.195593  0.235856  0.550495</span><br><span class="line">max    1.614754  0.874263  0.590759  0.778474</span><br></pre></td></tr></table></figure>
<h3 id="DataFrame基本操作"><a href="#DataFrame基本操作" class="headerlink" title="DataFrame基本操作"></a>DataFrame基本操作</h3><h4 id="DataFrame转置"><a href="#DataFrame转置" class="headerlink" title="DataFrame转置"></a>DataFrame转置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699 -1.378680 -1.406925</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181 -1.064210  0.588947</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263 -0.566293 -0.148778</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111 -1.394035  0.778474</span><br><span class="line">&gt;&gt;&gt; df1.T</span><br><span class="line">   2019-05-07 11:19:46.800556  2019-05-08 11:19:46.800556  ...  2019-05-11 11:19:46.800556  2019-05-12 11:19:46.800556</span><br><span class="line">A                    0.730185                    1.614754  ...                    1.300968                    0.944818</span><br><span class="line">B                   -0.921699                    0.070787  ...                    0.874263                   -1.049111</span><br><span class="line">C                   -1.378680                    0.590759  ...                   -0.566293                   -1.394035</span><br><span class="line">D                   -1.406925                   -1.994313  ...                   -0.148778                    0.778474</span><br><span class="line"></span><br><span class="line">[4 rows x 6 columns]</span><br></pre></td></tr></table></figure>
<h4 id="DataFrame按列排序"><a href="#DataFrame按列排序" class="headerlink" title="DataFrame按列排序"></a>DataFrame按列排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699 -1.378680 -1.406925</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181 -1.064210  0.588947</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263 -0.566293 -0.148778</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111 -1.394035  0.778474</span><br><span class="line">&gt;&gt;&gt; df1.sort_values(by=&apos;A&apos;)</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181 -1.064210  0.588947</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699 -1.378680 -1.406925</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111 -1.394035  0.778474</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263 -0.566293 -0.148778</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br></pre></td></tr></table></figure>
<h4 id="DataFrame切片"><a href="#DataFrame切片" class="headerlink" title="DataFrame切片"></a>DataFrame切片</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df1</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-07 11:19:46.800556  0.730185 -0.921699 -1.378680 -1.406925</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br><span class="line">2019-05-10 11:19:46.800556  0.239848  0.154181 -1.064210  0.588947</span><br><span class="line">2019-05-11 11:19:46.800556  1.300968  0.874263 -0.566293 -0.148778</span><br><span class="line">2019-05-12 11:19:46.800556  0.944818 -1.049111 -1.394035  0.778474</span><br><span class="line">&gt;&gt;&gt; df1[1:3]</span><br><span class="line">                                   A         B         C         D</span><br><span class="line">2019-05-08 11:19:46.800556  1.614754  0.070787  0.590759 -1.994313</span><br><span class="line">2019-05-09 11:19:46.800556  0.008300  0.209397  0.503240  0.435140</span><br></pre></td></tr></table></figure>
<h4 id="Dataframe副本copy"><a href="#Dataframe副本copy" class="headerlink" title="Dataframe副本copy"></a>Dataframe副本copy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df3 = df2.copy()</span><br><span class="line">&gt;&gt;&gt; df3</span><br><span class="line">  animal  age  visits priority</span><br><span class="line">a    cat  2.5       1      yes</span><br><span class="line">b    cat  3.0       3      yes</span><br><span class="line">c  snake  0.5       2       no</span><br><span class="line">d    dog  NaN       3      yes</span><br><span class="line">e    dog  5.0       2       no</span><br><span class="line">f    cat  2.0       3       no</span><br><span class="line">g  snake  4.5       1       no</span><br><span class="line">h    cat  NaN       1      yes</span><br><span class="line">i    dog  7.0       2       no</span><br><span class="line">j    dog  3.0       1       no</span><br></pre></td></tr></table></figure>
<h4 id="列添加"><a href="#列添加" class="headerlink" title="列添加"></a>列添加</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1[&apos;location&apos;] = [&apos;SH&apos;, &apos;BJ&apos;, &apos;GZ&apos;]</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩     姓名   爱好 location</span><br><span class="line">xx    90   Tony   篮球       SH</span><br><span class="line">oo    90  Wayne   排球       BJ</span><br><span class="line">xxoo  99   Moon  乒乓球       GZ</span><br></pre></td></tr></table></figure>
<h4 id="列删除"><a href="#列删除" class="headerlink" title="列删除"></a>列删除</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩     姓名   爱好 location</span><br><span class="line">xx    90   Tony   篮球       SH</span><br><span class="line">oo    90  Wayne   排球       BJ</span><br><span class="line">xxoo  99   Moon  乒乓球       GZ</span><br><span class="line">&gt;&gt;&gt; df_1.pop(&apos;爱好&apos;)</span><br><span class="line">xx       篮球</span><br><span class="line">oo       排球</span><br><span class="line">xxoo    乒乓球</span><br><span class="line">Name: 爱好, dtype: object</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩     姓名 location</span><br><span class="line">xx    90   Tony       SH</span><br><span class="line">oo    90  Wayne       BJ</span><br><span class="line">xxoo  99   Moon       GZ</span><br></pre></td></tr></table></figure>
<h4 id="列修改"><a href="#列修改" class="headerlink" title="列修改"></a>列修改</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1[&apos;location&apos;] = [&apos;SJZ&apos;,&apos;TY&apos;,&apos;ZZ&apos;]</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩     姓名 location</span><br><span class="line">xx    90   Tony      SJZ</span><br><span class="line">oo    90  Wayne       TY</span><br><span class="line">xxoo  99   Moon       ZZ</span><br></pre></td></tr></table></figure>
<h4 id="行的获取"><a href="#行的获取" class="headerlink" title="行的获取"></a>行的获取</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩     姓名 location</span><br><span class="line">xx    90   Tony      SJZ</span><br><span class="line">oo    90  Wayne       TY</span><br><span class="line">xxoo  99   Moon       ZZ</span><br><span class="line">&gt;&gt;&gt; df_1.ix[&apos;xx&apos;]</span><br><span class="line">成绩            90</span><br><span class="line">姓名          Tony</span><br><span class="line">location     SJZ</span><br><span class="line">Name: xx, dtype: object</span><br><span class="line">&gt;&gt;&gt; df_1.loc[&apos;xx&apos;]</span><br><span class="line">成绩            90</span><br><span class="line">姓名          Tony</span><br><span class="line">location     SJZ</span><br><span class="line">Name: xx, dtype: object</span><br></pre></td></tr></table></figure>
<h4 id="行增加"><a href="#行增加" class="headerlink" title="行增加"></a>行增加</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1.ix[&apos;ooxx&apos;] = [88, &apos;Bessie&apos;, &apos;QHD&apos;]</span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩      姓名 location</span><br><span class="line">xx    90    Tony      SJZ</span><br><span class="line">oo    90   Wayne       TY</span><br><span class="line">xxoo  99    Moon       ZZ</span><br><span class="line">ooxx  88  Bessie      QHD</span><br></pre></td></tr></table></figure>
<h4 id="行修改"><a href="#行修改" class="headerlink" title="行修改"></a>行修改</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1.ix[&apos;ooxx&apos;] = [88, &apos;Bessie&apos;, &apos;SH&apos;] </span><br><span class="line">&gt;&gt;&gt; df_1</span><br><span class="line">      成绩      姓名 location</span><br><span class="line">xx    90    Tony      SJZ</span><br><span class="line">oo    90   Wayne       TY</span><br><span class="line">xxoo  99    Moon       ZZ</span><br><span class="line">ooxx  88  Bessie       SH</span><br></pre></td></tr></table></figure>
<h4 id="行删除"><a href="#行删除" class="headerlink" title="行删除"></a>行删除</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df_1.drop(&apos;xxoo&apos;)  </span><br><span class="line">      成绩      姓名 location</span><br><span class="line">xx    90    Tony      SJZ</span><br><span class="line">oo    90   Wayne       TY</span><br><span class="line">ooxx  88  Bessie       SH</span><br></pre></td></tr></table></figure>
<h3 id="索引对象"><a href="#索引对象" class="headerlink" title="索引对象"></a>索引对象</h3><h2 id="pandas基本功能"><a href="#pandas基本功能" class="headerlink" title="pandas基本功能"></a>pandas基本功能</h2><h3 id="数据文件读取和文本数据读取"><a href="#数据文件读取和文本数据读取" class="headerlink" title="数据文件读取和文本数据读取"></a>数据文件读取和文本数据读取</h3><p>通过pandas.read_xx相关函数可以读取文件中的数据, 并形成DataFrame, 常用的方法为read_csv, 主要读取文本类型的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat data.csv</span><br><span class="line">wayne,tony,bessie</span><br><span class="line">19,20,21</span><br><span class="line">1,1,0</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pd.read_csv(&apos;data.txt&apos;, sep=&apos;;&apos;, header=None)</span><br><span class="line">       0     1       2</span><br><span class="line">0  wayne  tony  bessie</span><br><span class="line">1     19    20      21</span><br><span class="line">2      1     1       0</span><br><span class="line"></span><br><span class="line">cat data.csv </span><br><span class="line">name,age,sex</span><br><span class="line">wayne,19,1</span><br><span class="line">tony,20,1</span><br><span class="line">bessie,21,0</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pd.read_csv(&apos;data.csv&apos;)</span><br><span class="line">     name  age  sex</span><br><span class="line">0   wayne   19    1</span><br><span class="line">1    tony   20    1</span><br><span class="line">2  bessie   21    0</span><br></pre></td></tr></table></figure>
<h3 id="索引-选取和数据过滤"><a href="#索引-选取和数据过滤" class="headerlink" title="索引, 选取和数据过滤"></a>索引, 选取和数据过滤</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df01 = pd.read_csv(&apos;data.csv&apos;)</span><br><span class="line">&gt;&gt;&gt; df01[df01.columns[1:]]</span><br><span class="line">   age  source</span><br><span class="line">0   18    98.5</span><br><span class="line">1   21    78.2</span><br><span class="line">2   24    98.5</span><br><span class="line">3   20    89.2</span><br></pre></td></tr></table></figure>
<h4 id="缺省值NaN的处理方法"><a href="#缺省值NaN的处理方法" class="headerlink" title="缺省值NaN的处理方法"></a>缺省值NaN的处理方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cat data1.txt</span><br><span class="line">name,age,salary,gender</span><br><span class="line">Tom,NaN,456.7,M </span><br><span class="line">Merry,34,345.6,NaN</span><br><span class="line">Gerry,NaN,NaN,NaN</span><br><span class="line">John,23,NaN,M</span><br><span class="line">Joe ,18,385.6,F</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df2 = pd.read_csv(&apos;data1.csv&apos;, sep=&apos;,&apos;)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df2.isnull()</span><br><span class="line">    name    age  salary  gender</span><br><span class="line">0  False   True   False   False</span><br><span class="line">1  False  False   False    True</span><br><span class="line">2  False   True    True    True</span><br><span class="line">3  False  False    True   False</span><br><span class="line">4  False  False   False   False</span><br><span class="line">&gt;&gt;&gt; df2.notnull()</span><br><span class="line">   name    age  salary  gender</span><br><span class="line">0  True  False    True    True</span><br><span class="line">1  True   True    True   False</span><br><span class="line">2  True  False   False   False</span><br><span class="line">3  True   True   False    True</span><br><span class="line">4  True   True    True    True</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df01 = pd.DataFrame(np.random.randint(1,9,size=(4,4)))</span><br><span class="line">&gt;&gt;&gt; df01</span><br><span class="line">   0  1  2  3</span><br><span class="line">0  1  5  8  4</span><br><span class="line">1  3  3  6  8</span><br><span class="line">2  5  7  2  4</span><br><span class="line">3  5  5  6  4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># &gt;&gt;&gt; df01 = pd.DataFrame(np.random.randint(1,9,size=(4,4)), index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</span><br><span class="line"># &gt;&gt;&gt; df01</span><br><span class="line">#    0  1  2  3</span><br><span class="line"># a  4  2  6  7</span><br><span class="line"># b  3  7  5  6</span><br><span class="line"># c  8  5  3  2</span><br><span class="line"># d  6  7  4  4</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df01.ix[1:3,1] = np.NaN  # 将第一行到第三行的第一列修改为NaN</span><br><span class="line">&gt;&gt;&gt; df01</span><br><span class="line">   0    1  2  3</span><br><span class="line">0  3  7.0  6  3</span><br><span class="line">1  2  NaN  1  5</span><br><span class="line">2  3  NaN  1  4</span><br><span class="line">3  8  NaN  8  2</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df01.dropna()  # 将NaN都删除掉, 默认只要包含NaN就删除掉</span><br><span class="line">   0    1  2  3</span><br><span class="line">0  3  7.0  6  3</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df01.dropna(how=&apos;all&apos;)</span><br><span class="line">   0    1  2  3</span><br><span class="line">0  3  7.0  6  3</span><br><span class="line">1  2  NaN  1  5</span><br><span class="line">2  3  NaN  1  4</span><br><span class="line">3  8  NaN  8  2</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df01</span><br><span class="line">   0    1  2  3</span><br><span class="line">0  3  7.0  6  3</span><br><span class="line">1  2  NaN  1  5</span><br><span class="line">2  3  NaN  1  4</span><br><span class="line">3  8  NaN  8  2</span><br><span class="line">&gt;&gt;&gt; df01.dropna(axis=1)</span><br><span class="line">   0  2  3</span><br><span class="line">0  3  6  3</span><br><span class="line">1  2  1  5</span><br><span class="line">2  3  1  4</span><br><span class="line">3  8  8  2</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from pandas import DataFrame</span><br><span class="line">&gt;&gt;&gt; df = DataFrame(np.random.randn(7,3))</span><br><span class="line">&gt;&gt;&gt; df.ix[:4,1] = np.nan  # 第0行到第4行的第1列全都置为NaN</span><br><span class="line">&gt;&gt;&gt; df.ix[:2,2] = np.nan  # 第0行到第2行的第2列全都置为NaN</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">          0         1         2</span><br><span class="line">0 -0.041760       NaN       NaN</span><br><span class="line">1 -0.005338       NaN       NaN</span><br><span class="line">2 -0.728414       NaN       NaN</span><br><span class="line">3 -1.836972       NaN  1.114158</span><br><span class="line">4  1.737801       NaN -0.103160</span><br><span class="line">5 -0.968829 -0.573951  0.503281</span><br><span class="line">6 -0.565628 -0.889926 -1.307314</span><br><span class="line">&gt;&gt;&gt; df.fillna(0)  # 将所有的NaN填充为0</span><br><span class="line">          0         1         2</span><br><span class="line">0 -0.041760  0.000000  0.000000</span><br><span class="line">1 -0.005338  0.000000  0.000000</span><br><span class="line">2 -0.728414  0.000000  0.000000</span><br><span class="line">3 -1.836972  0.000000  1.114158</span><br><span class="line">4  1.737801  0.000000 -0.103160</span><br><span class="line">5 -0.968829 -0.573951  0.503281</span><br><span class="line">6 -0.565628 -0.889926 -1.307314</span><br><span class="line">&gt;&gt;&gt; df.fillna(&#123;1:0.5, 2:-1, 3:1&#125;)  # 将第1列的NaN填充为0.5, 将第2列的NaN填充为-1, 将第3列的NaN填充为1</span><br><span class="line">          0         1         2</span><br><span class="line">0 -0.041760  0.500000 -1.000000</span><br><span class="line">1 -0.005338  0.500000 -1.000000</span><br><span class="line">2 -0.728414  0.500000 -1.000000</span><br><span class="line">3 -1.836972  0.500000  1.114158</span><br><span class="line">4  1.737801  0.500000 -0.103160</span><br><span class="line">5 -0.968829 -0.573951  0.503281</span><br><span class="line">6 -0.565628 -0.889926 -1.307314</span><br></pre></td></tr></table></figure>
<h4 id="常用的数学统计方法"><a href="#常用的数学统计方法" class="headerlink" title="常用的数学统计方法"></a>常用的数学统计方法</h4><div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>计算非NA值的数量</td>
</tr>
<tr>
<td>describe</td>
<td>针对Series或各个DataFrame列计算总的统计值</td>
</tr>
<tr>
<td>min/max</td>
<td></td>
</tr>
<tr>
<td>argmin/argmax</td>
<td>计算能够获取到最小值和最大值的索引位置(整数)</td>
</tr>
<tr>
<td>idxmin/idxmax</td>
<td>算能够获取到最小值和最大值的索引值</td>
</tr>
<tr>
<td>quantile</td>
<td>计算样本的分位数(0或1)</td>
</tr>
<tr>
<td>sum</td>
<td>值的总和</td>
</tr>
<tr>
<td>mean</td>
<td>值的平均数</td>
</tr>
<tr>
<td>median</td>
<td>值的中位数</td>
</tr>
<tr>
<td>mad</td>
<td>根据平均值计算平均绝对距离差</td>
</tr>
<tr>
<td>var</td>
<td>样本值的方差</td>
</tr>
<tr>
<td>std</td>
<td>样本值的标准差</td>
</tr>
<tr>
<td>cumsum</td>
<td>样本值的累计和</td>
</tr>
<tr>
<td>cummin/cummax</td>
<td>样本的累计最小值和最大值</td>
</tr>
<tr>
<td>cumprod</td>
<td>样本值的累计积</td>
</tr>
<tr>
<td>pct_change</td>
<td>百分数的变化</td>
</tr>
</tbody>
</table>
</div>
<h4 id="相关系数与协方差"><a href="#相关系数与协方差" class="headerlink" title="相关系数与协方差"></a>相关系数与协方差</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; df = DataFrame(&#123;&apos;GDP&apos;:[12, 23, 34, 45, 56], &apos;air_temperature&apos;: [23, 25, 26, 27, 30], &apos;year&apos;:[&apos;2001&apos;,&apos;2002&apos;,&apos;2003&apos;,&apos;2004&apos;,&apos;2005&apos;]&#125;)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">   GDP  air_temperature  year</span><br><span class="line">0   12               23  2001</span><br><span class="line">1   23               25  2002</span><br><span class="line">2   34               26  2003</span><br><span class="line">3   45               27  2004</span><br><span class="line">4   56               30  2005</span><br><span class="line">&gt;&gt;&gt; df.corr()</span><br><span class="line">                      GDP  air_temperature</span><br><span class="line">GDP              1.000000         0.977356</span><br><span class="line">air_temperature  0.977356         1.000000</span><br><span class="line">&gt;&gt;&gt; df.cov()</span><br><span class="line">                   GDP  air_temperature</span><br><span class="line">GDP              302.5             44.0</span><br><span class="line">air_temperature   44.0              6.7</span><br></pre></td></tr></table></figure>
<h4 id="唯一值-值计数以及成员资格"><a href="#唯一值-值计数以及成员资格" class="headerlink" title="唯一值, 值计数以及成员资格"></a>唯一值, 值计数以及成员资格</h4><ul>
<li>unique用于获取Series中的唯一值数组(去重后的数组)</li>
<li>value_counts用于计算一个Series中各值的出现的频率</li>
<li>isin用于判断矢量化集合中的成员资格, 可用于选取Series中或者DataFrame中列数据的子集</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from pandas import Series</span><br><span class="line">&gt;&gt;&gt; ser = Series([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;b&apos;,&apos;c&apos;,&apos;a&apos;,&apos;d&apos;,&apos;e&apos;,&apos;b&apos;])</span><br><span class="line">&gt;&gt;&gt; ser.unique()</span><br><span class="line">array([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;], dtype=object)</span><br><span class="line">&gt;&gt;&gt; ser.value_counts()</span><br><span class="line">b    3</span><br><span class="line">a    2</span><br><span class="line">c    2</span><br><span class="line">d    1</span><br><span class="line">e    1</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; ser.value_counts(ascending=False)  # 降序排列</span><br><span class="line">b    3</span><br><span class="line">a    2</span><br><span class="line">c    2</span><br><span class="line">d    1</span><br><span class="line">e    1</span><br><span class="line">dtype: int64</span><br><span class="line">&gt;&gt;&gt; ser.value_counts(ascending=True)  # 升序排列</span><br><span class="line">e    1</span><br><span class="line">d    1</span><br><span class="line">c    2</span><br><span class="line">a    2</span><br><span class="line">b    3</span><br><span class="line">dtype: int64</span><br><span class="line"></span><br><span class="line"># DataFrame去重</span><br><span class="line">&gt;&gt;&gt; df = DataFrame(&#123;&apos;order_id&apos;:[&apos;1001&apos;,&apos;1002&apos;,&apos;1003&apos;,&apos;1004&apos;,&apos;1005&apos;], &apos;member_id&apos;:[&apos;m01&apos;,&apos;m01&apos;,&apos;m02&apos;,&apos;m01&apos;,&apos;m02&apos;], &apos;order_amt&apos;:[345, 312.2, 123, 250.2, 235]&#125;)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">  order_id member_id  order_amt</span><br><span class="line">0     1001       m01      345.0</span><br><span class="line">1     1002       m01      312.2</span><br><span class="line">2     1003       m02      123.0</span><br><span class="line">3     1004       m01      250.2</span><br><span class="line">4     1005       m02      235.0</span><br><span class="line">&gt;&gt;&gt; df[&apos;member_id&apos;].unique()</span><br><span class="line">array([&apos;m01&apos;, &apos;m02&apos;], dtype=object)</span><br><span class="line"></span><br><span class="line"># 成员资格判断</span><br><span class="line">&gt;&gt;&gt; ser</span><br><span class="line">0    a</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    b</span><br><span class="line">4    c</span><br><span class="line">5    a</span><br><span class="line">6    d</span><br><span class="line">7    e</span><br><span class="line">8    b</span><br><span class="line">dtype: object</span><br><span class="line">&gt;&gt;&gt; mask = ser.isin([&apos;b&apos;,&apos;c&apos;])</span><br><span class="line">&gt;&gt;&gt; mask</span><br><span class="line">0    False</span><br><span class="line">1     True</span><br><span class="line">2     True</span><br><span class="line">3     True</span><br><span class="line">4     True</span><br><span class="line">5    False</span><br><span class="line">6    False</span><br><span class="line">7    False</span><br><span class="line">8     True</span><br><span class="line">dtype: bool</span><br><span class="line">&gt;&gt;&gt; ser[mask]</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    b</span><br><span class="line">4    c</span><br><span class="line">8    b</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<h3 id="算法运算和数据对齐"><a href="#算法运算和数据对齐" class="headerlink" title="算法运算和数据对齐"></a>算法运算和数据对齐</h3><h3 id="函数的应用和映射"><a href="#函数的应用和映射" class="headerlink" title="函数的应用和映射"></a>函数的应用和映射</h3><h3 id="层次索引"><a href="#层次索引" class="headerlink" title="层次索引"></a>层次索引</h3><ul>
<li>在某一个方向上拥有多个(两个或两个以上)索引级别</li>
<li>通过层次化索引, pandas能够以较低维度的形式处理高维度的数据</li>
<li>通过层次化索引, 可以按照层次统计数据</li>
<li>层次所以包括Series层次索引和DataFrame层次索引</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; data = Series([988.44, 95895, 3959, 32554, 1235], index=[[&apos;2001&apos;,&apos;2001&apos;,&apos;2001&apos;,&apos;2002&apos;,&apos;2002&apos;],[&apos;苹果&apos;,&apos;香蕉&apos;,&apos;西瓜&apos;,&apos;苹果&apos;,&apos;西瓜&apos;]])</span><br><span class="line">&gt;&gt;&gt; data</span><br><span class="line">2001  苹果      988.44</span><br><span class="line">      香蕉    95895.00</span><br><span class="line">      西瓜     3959.00</span><br><span class="line">2002  苹果    32554.00</span><br><span class="line">      西瓜     1235.00</span><br><span class="line">dtype: float64</span><br><span class="line">&gt;&gt;&gt; data01 = data.swaplevel().sort_index()  # 交换分层索引</span><br><span class="line">&gt;&gt;&gt; data01</span><br><span class="line">苹果  2001      988.44</span><br><span class="line">    2002    32554.00</span><br><span class="line">西瓜  2001     3959.00</span><br><span class="line">    2002     1235.00</span><br><span class="line">香蕉  2001    95895.00</span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df = DataFrame(&#123;&apos;year&apos;:[2001, 2001, 2002, 2002, 2003], &apos;fruit&apos;:[&apos;apple&apos;,&apos;banana&apos;,&apos;apple&apos;,&apos;banana&apos;,&apos;apple&apos;], &apos;production&apos;:[2345, 3124, 5668, 2532, 2135], &apos;profits&apos;:[233.44, 4452.2, 1225.2, 7845.2, 2352.2]&#125;)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">   year   fruit  production  profits</span><br><span class="line">0  2001   apple        2345   233.44</span><br><span class="line">1  2001  banana        3124  4452.20</span><br><span class="line">2  2002   apple        5668  1225.20</span><br><span class="line">3  2002  banana        2532  7845.20</span><br><span class="line">4  2003   apple        2135  2352.20</span><br><span class="line">&gt;&gt;&gt; df = df.set_index([&apos;year&apos;,&apos;fruit&apos;])  # 设置层次化索引</span><br><span class="line">             production  profits</span><br><span class="line">year fruit                      </span><br><span class="line">2001 apple         2345   233.44</span><br><span class="line">     banana        3124  4452.20</span><br><span class="line">2002 apple         5668  1225.20</span><br><span class="line">     banana        2532  7845.20</span><br><span class="line">2003 apple         2135  2352.20</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df.ix[2002,&apos;apple&apos;]  # 根据层次化索引进行取值</span><br><span class="line">production    5668.0</span><br><span class="line">profits       1225.2</span><br><span class="line">Name: (2002, apple), dtype: float64</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df.sum(level=&apos;year&apos;)  # 按照层次索引统计数据</span><br><span class="line">      production  profits</span><br><span class="line">year                     </span><br><span class="line">2001        5469  4685.64</span><br><span class="line">2002        8200  9070.40</span><br><span class="line">2003        2135  2352.20</span><br><span class="line">&gt;&gt;&gt; df.min(level=&quot;fruit&quot;)</span><br><span class="line">        production  profits</span><br><span class="line">fruit                      </span><br><span class="line">apple         2135   233.44</span><br><span class="line">banana        2532  4452.20</span><br><span class="line">&gt;&gt;&gt; df.min(level=[&apos;year&apos;,&apos;fruit&apos;])</span><br><span class="line">             production  profits</span><br><span class="line">year fruit                      </span><br><span class="line">2001 apple         2345   233.44</span><br><span class="line">     banana        3124  4452.20</span><br><span class="line">2002 apple         5668  1225.20</span><br><span class="line">     banana        2532  7845.20</span><br><span class="line">2003 apple         2135  2352.20</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/AvalancheM/article/details/81293149" target="_blank" rel="noopener">参考文档</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">64</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
