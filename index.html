<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Wenhua">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Wenhua">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Wenhua">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Wenhua</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wenhua</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Xception/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Xception/" itemprop="url">Xception</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-20T10:24:33+08:00">
                2020-02-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><h3 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h3><p><img src="/Xception/WechatIMG98.png" width="400px"></p>
<p>Inception架构的主要思想是找出如何让已有的稠密组件接近与覆盖卷积视觉网络中的最佳局部稀疏结构<br>为了避免patch校准问题, 现在滤波器大小限制在$1\ast1$, $3\ast3$和$5\ast5$, 主要为了方便, 不是必要的<br>另外在pooling层添加一个额外的并行pooling路径用于提高效率</p>
<p>Previous Layer, 输入层, 比如是$27\ast27\ast128$</p>
<p>先做一个$1\ast1$的卷积,  那么output为: $(27-1+1)/1=27, 27\ast27\ast?$<br>$3\ast3$的卷积, output为: $(27-3+1+2)/1=27, 27\ast27\ast?$<br>$5\ast5$的卷积, output为: $(27-5+1+4)/1=27, 27\ast27\ast?$<br>以上的三个卷积相当于从三个不同的方面并行的观察目标信息, 在这里比如$1\ast1$是10个, $3\ast3$是20个, $5\ast5$是5个,  那么输出到filter concatenation就是$27\ast27\ast35$, 就包含了小/中/大区域观测的结果</p>
<p>然而, 输入结果中有可能也包含着一些个性化的或者噪音数据或者异常信息, 所有要并行的做一个池化操作<br>池化的output为: (27-3+1+2)/1, 池化不改变深度, 所以是$27\ast27\ast128$</p>
<p>所以最后的层次为$27\ast27\ast(35+128)=27\ast27\ast163$</p>
<p>参数量为$(1\ast1\ast128+1)\ast10 + (3\ast3\ast128+1)\ast20 + (5\ast5\ast128+1)\ast5  = 40355$</p>
<p>这种结构参数比较多,  遂引出inception的改进结构</p>
<h3 id="Inception结构改进-Pointwise-Conv"><a href="#Inception结构改进-Pointwise-Conv" class="headerlink" title="Inception结构改进/Pointwise Conv"></a>Inception结构改进/Pointwise Conv</h3><p>架构的第二个主要思想: 在计算要求增加很多的地方应用维度缩减和预测. 即在$3\ast3$和$5\ast5$卷积前用一个$1\ast1$的卷积用于减少计算, 还用于修正线性激活. 如下图所示, 左边是加入维度缩减之前的, 右边是加入维度缩减之后的, 这就是Inception V1的网络架构<br><img src="/Xception/WechatIMG99.png" width="400px"></p>
<p>输入还是$27\ast27\ast128$, 假设核函数设置的都是10个</p>
<p>那么未改进的结构的总层数为$27\ast27\ast(10+10+10+128)=27\ast27\ast158$<br>参数量为$(1\ast1\ast128+1)\ast10 + (3\ast3\ast128+1)\ast10 + (5\ast5\ast128+1)\ast10 = 44830$<br>改进后的总层数为$27\ast27\ast40$<br>参数量为$(1\ast1\ast128+1)\ast10\ast4 + (3\ast3\ast10+1)\ast10 + (5\ast5\ast10+1)\ast10 = 8580$</p>
<p>改进后的结构就叫做Network-In-Network, 在上面的例子来说, 就是把128个方位的降为了40个方位</p>
<p>在这里$1 \ast 1$的卷积核就是Pointwise Convolution, 简称PW, 其目的主要在于减少维度, 还可以引入更多的非线性</p>
<h3 id="Kernel-Replace"><a href="#Kernel-Replace" class="headerlink" title="Kernel Replace"></a>Kernel Replace</h3><p><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Inception V2</a>和<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Inception V3</a>为了进一步降低卷积参数采用小卷积来代替大卷积</p>
<p><img src="/Xception/wechat_20200220113141.png" width="300px"></p>
<p>大尺寸的卷积核可以带来更大的感受野, 但也意味着会产生更多的参数, 比如$5 \ast 5$卷积核的参数有25个, $3 \ast 3$ 卷积核的参数有9个, 前者是后者的$\frac{25}{9}=2.78$倍. 因此, GoogLeNet团队提出可以用2个连续的$3 \ast 3$卷积层组成的小网络来代替单个的$5 \ast 5$卷积层, 即在保持感受野范围的同时又减少了参数量. 除了规整的的正方形, 还有分解版本的$3 \ast 3 = 3 \ast 1 + 1 \ast 3$，这个效果在深度较深的情况下比规整的卷积核更好(feature map大小建议在12到20之间)</p>
<h2 id="Group-Conv-Depthwise-Separable-Conv"><a href="#Group-Conv-Depthwise-Separable-Conv" class="headerlink" title="Group Conv/Depthwise Separable Conv"></a>Group Conv/Depthwise Separable Conv</h2><h3 id="Group-COnv"><a href="#Group-COnv" class="headerlink" title="Group COnv"></a>Group COnv</h3><p>Group Conv分组卷积, 可以用来切分网络, 以便在多个GPU上运行</p>
<p><img src="/Xception/wechat_20200220104838.png" width="400px"></p>
<p>对于输入为$I \ast H \ast W$大小的input features, 经过O个$K \ast K \ast I$大小的卷积核之后, 输出的output features通道数为O, 卷积参数量为$O \ast K \ast K \ast I$</p>
<p>Group Conv, 先对输入的input features进行分组, 然后对每组分别进行卷积. 仍然假设input feature map的尺寸为$I \ast H \ast W$, 输出output features的通道数为O, 假设分成G个组, 则每个input feature map的数量为$\frac{I}{G}$, 每组的输出的feature map的数量为$\frac{O}{G}$, 那么每个卷积核的尺寸为$K \ast K \ast \frac{I}{G}$, 每组卷积核的数量为$\frac{O}{G}$, 卷积核的总数仍然为O.</p>
<p>卷积核只与自己同组的input feature map进行卷积, 最后的卷积的总参数量为$O \ast K \ast K \ast \frac{I}{G}$, 由此可以看到总参数量减少到了原来的$\frac{1}{G}$</p>
<p>Group Conv的用途如下所示:</p>
<ul>
<li>减少参数量, 分成G组, 则参数量减少为原来的$\frac{1}{G}$</li>
<li>Group Conv可以看成是一种structured sparse, 每个卷积核的尺寸由$K \ast K \ast I$变成了$K \ast K \ast \frac{I}{G}$, 那么减少的参数量为$(I - \frac{I}{G})$, 可以视其为0(<label style="color:red">考虑该效果是否与L1/L2正则类似</label>)</li>
</ul>
<p>Depthwise Convolution(DW)是Group Conv的一种特例. 当分组数量等于input feature map的数量, output feature map数量也等于input feature map数量, 即当$I = O = G$, 且O个卷积核中每个的尺寸为$K \ast K \ast 1$时, Group Conv 就成了Depthwise Conv. </p>
<h3 id="Depthwise-Separable-Conv"><a href="#Depthwise-Separable-Conv" class="headerlink" title="Depthwise Separable Conv"></a>Depthwise Separable Conv</h3><p>Depthwise Separable Convolution则是DW + PW的组合, 参数量会进一步缩小, 如下图所示:</p>
<p><img src="/Xception/wechat_20200220111937.png" width="400px"></p>
<h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><p>Xception就是在Inception + Depthwise Separable Conv</p>
<h3 id="Inception-Hypothesis"><a href="#Inception-Hypothesis" class="headerlink" title="Inception Hypothesis"></a>Inception Hypothesis</h3><p>从Inception的各种演进架构可以看出, Inception V1是从多尺寸卷积核角度来观察input feature map, Inception V3是从参数量和计算量角度来尝试改进结构的. </p>
<p>然而Inception V3也可以理解为: 通过显式地将操作分解为一系列独立的通道维度和空间维度的学习, 从而使得学习的过程更加简单和高效.</p>
<p>Inception V1里各个卷积核需要同时学习空间上的相关性和通道相关性, 结合了spatial dimentions和channels dimentions.</p>
<p>Inception V3先通过一组$1 \ast 1$PW卷积来学习通道相关性, 将输入数据映射到多个单独的小空间(降维), 之后对所有这些小空间, 通过常规的$3 \ast 3$卷积和$5 \ast 5$卷积来学习空间相关性. </p>
<p>Inception背后的基本假设是, 通道相关性和空间相关性是可以分离相互之间的耦合性的, 于是就得到了Xception: 将通道相关性和空间相关性分开学习的结构.</p>
<h3 id="Extreme-Inception"><a href="#Extreme-Inception" class="headerlink" title="Extreme Inception"></a>Extreme Inception</h3><p>Inception中, 特征是通过$1 \ast 1$, $3 \ast 3$, $5 \ast 5$, max pooling等进行提取的, Inception结构将特征类型的选择留给网络自己训练, 即同一个输入交由几种特征提取方式, 之后做concat. Inception V3结果图如下:</p>
<p><img src="/Xception/wechat_20200220143307.png" width="300px"></p>
<p>对Inception-V3进行简化, 去除Inception-V3中的avg-pooling, 输入的下一步操作就都是$1 \ast 1$的PW卷积了:</p>
<p><img src="/Xception/wechat_20200220143746.png" width="300px"></p>
<p>继续演进, 3个PW卷积核统一起来变成共用一个PW卷积, 后面的三个$3 \ast 3$卷积核则分别负责Group Conv</p>
<p><img src="/Xception/wechat_20200220143755.png" width="300px"></p>
<p>对卷积后的每个channel分别进行$3 \ast 3$卷积操作(Depthwise Conv, DW), 最后concat, 就是Extreme Inception:</p>
<p><img src="/Xception/wechat_20200220144303.png" width="300px"></p>
<p>在Extreme Inception模块中, 用于学习空间相关性的$3 \ast 3$的DW卷积, 和用于学习通道相关性的$1 \ast 1$PW卷积之间, 不使用非线性激活函数, 收敛过程更快, 准确率更高.</p>
<h3 id="Xception-1"><a href="#Xception-1" class="headerlink" title="Xception"></a>Xception</h3><p>Xception常用版本是将DW和PW互换位置.</p>
<p>Extreme Inception先进行$1 \ast 1$的PW卷积, 再进行$3 \ast 3$的DW卷积.<br>Depthwise Seprable Conv先进行$3 \ast 3$的DW卷积, 再进行$1 \ast 1$的PW卷积.</p>
<p>DW与标准卷积操作的计算量比较如下:</p>
<script type="math/tex; mode=display">P_{DW} = I \ast K \ast K + I \ast O</script><script type="math/tex; mode=display">P_{Normal} = I \ast K \ast K \ast O</script><script type="math/tex; mode=display">\displaystyle \frac{P_{DW}}{P_{Normal}} = \frac{1}{O} + \frac{1}{K^2} \approx \frac{1}{K^2}</script><p>其中I为输入通道数, O为输出通道数, K为标准卷积和大小. 当使用$3 \ast 3$的卷积核时, 参数量约等于标准卷积核的$\frac{1}{9}$, 大大的减少了参数量, 从而加快了训练速度. 最终其网络结构图如下图所示:</p>
<p><img src="/Xception/wechat_20200220150054.png" width="300px"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/xuanyuyt/p/11329998.html#_label6" target="_blank" rel="noopener">深度学习笔记</a><br><a href="https://cloud.tencent.com/developer/news/60074" target="_blank" rel="noopener">深度学习背景下的神经网络架构演变</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/20191207/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/20191207/" itemprop="url">20191207</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-09T10:49:04+08:00">
                2019-12-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/aura/" itemprop="url" rel="index">
                    <span itemprop="name">aura</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="如何用极大似然概率解释交叉熵loss"><a href="#如何用极大似然概率解释交叉熵loss" class="headerlink" title="如何用极大似然概率解释交叉熵loss"></a>如何用极大似然概率解释交叉熵loss</h2><h3 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h3><p>Logistic/Sigmoid函数, $\displaystyle p = h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$, p是个概率值</p>
<script type="math/tex; mode=display">g(z) = \frac{1}{1 + e^{-z}}</script><script type="math/tex; mode=display">\begin{align}
g^\prime(z) = &(\frac{1}{1 + e^{-z}})^\prime = \frac{e^{-z}}{(1 + e^{-z})^2}\\
&=\frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{(1 + e^{-z})} = \frac{1}{1 + e^{-z}} \cdot (1 - \frac{1}{1 + e^{-z}})\\
&=g(z) \cdot (1 - g(z))\\
\end{align}</script><script type="math/tex; mode=display">y = \begin{cases}0\\
1\end{cases}</script><script type="math/tex; mode=display">\hat{y} = \begin{cases}1, P(\hat{y} = 1) > p\\
0, P(\hat{y} = 0) > p\end{cases}</script><p><img src="/20191207/WechatIMG26.png" width="300px"></p>
<h3 id="Logistic回归及似然函数"><a href="#Logistic回归及似然函数" class="headerlink" title="Logistic回归及似然函数"></a>Logistic回归及似然函数</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y=1</th>
<th>y=0</th>
</tr>
</thead>
<tbody>
<tr>
<td>$p(y \mid x)$</td>
<td>$\theta$</td>
<td>$1 - \theta$</td>
</tr>
</tbody>
</table>
</div>
<p>假设:</p>
<script type="math/tex; mode=display">P(y=1|x;\theta) = h_\theta(x)</script><script type="math/tex; mode=display">P(y=0|x;\theta) = 1 - h_\theta(x)</script><script type="math/tex; mode=display">P(y|x;\theta) = h_\theta(x) \ast (1 - h_\theta(x)) = (h_\theta(x))^y(1 - h_\theta(x))^{(1-y)}</script><p>似然函数:</p>
<script type="math/tex; mode=display">\begin{align}L(\theta) &= p(\overrightarrow y \mid X;\theta) = \displaystyle \prod_{i=1}^{m}{p(y^{(i)}|x^{(i)}; \theta)}\\
&=\displaystyle \prod_{i=1}^{m} (h_\theta(x^{(i)}))^y{^{(i)}}(1 - h_\theta(x^{(i)}))^{(1-y^{(i)})}\\
\end{align}</script><p>最大似然/极大似然函数的随机梯度<br>对数似然函数为:</p>
<script type="math/tex; mode=display">l(\theta) = logL(\theta) = \displaystyle \sum_{i=1}^{m}(y^{(i)}log h_\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)})))</script><h3 id="极大似然估计与Logistic回归损失函数"><a href="#极大似然估计与Logistic回归损失函数" class="headerlink" title="极大似然估计与Logistic回归损失函数"></a>极大似然估计与Logistic回归损失函数</h3><script type="math/tex; mode=display">L(\theta) = \displaystyle \prod_{i=1}^{m}{p(y^{(i)}|x^{(i)}; \theta)} = \displaystyle \prod_{i=1}^{m}{p_{i}^{y^{(i)}}(1-p_i)^{(1-y^{(i)})}}, \quad p_i = h_\theta(x^{(i)}) = \frac{1}{1 + e^{\displaystyle - \theta^{T} x^{(i)}}}</script><script type="math/tex; mode=display">l(\theta) = lnL(\theta) = \displaystyle \sum_{i=1}^{m} ln[p_{i}^{y^{(i)}}(1-p_i)^{(1-y^{(i)})}]</script><script type="math/tex; mode=display">\begin{align}loss &= - l(\theta)\\
&= - \displaystyle \sum_{i=1}^{m}[y^{(i)}ln(p_i) + (1-y^{(i)})ln(1-p_i)] \\
\end{align}</script><p>如果我们对m个样本取平均, 得到如下:</p>
<script type="math/tex; mode=display">- \frac{1}{m} \displaystyle \sum_{i=1}^{m}[y^{(i)}ln(p_i) + (1-y^{(i)})ln(1-p_i)]</script><p>即cross entropy</p>
<p><a href="https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_log-likelihood" target="_blank" rel="noopener">详情参考</a></p>
<h2 id="硬币问题为何不使用MLP而直接假设正反面概率p"><a href="#硬币问题为何不使用MLP而直接假设正反面概率p" class="headerlink" title="硬币问题为何不使用MLP而直接假设正反面概率p?"></a>硬币问题为何不使用MLP而直接假设正反面概率p?</h2><ol>
<li>对于硬币游戏, 硬币只有正面和反面两种情况, 那么对于同一个硬币来说, 正面的概率是$p$的话, 那么反面的概率就是$1-p$, 根据一个已知的硬币观测序列, 可以直接将观测到的正反面情况带入相应的概率公式, 通过梯度下降得到极值;</li>
<li>而MLP分为输入层, 隐藏层, 输出层, 层级之间使用全连接, 需要将特征值传入进去, 通过带有结果的数据来训练神经网络的weight和bias, 本质上是将线性不可分的数据通过增加隐藏层构造切分平面使之线性科锋, 解决的问题基本上是分类问题.</li>
</ol>
<h2 id="面对未知分类问题，存在A-B-C三个模型，如何确定使用哪一个模型？"><a href="#面对未知分类问题，存在A-B-C三个模型，如何确定使用哪一个模型？" class="headerlink" title="面对未知分类问题，存在A,B,C三个模型，如何确定使用哪一个模型？"></a>面对未知分类问题，存在A,B,C三个模型，如何确定使用哪一个模型？</h2><ul>
<li>模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择</li>
<li>在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见)</li>
<li>模型的选择会尽可能多的选择算法进行执行, 并比较执行结果</li>
<li>模型的测试一般从以下几个方面来进行比较, 分别是准确率, 召回率, 精准率, F值<ul>
<li>准确率(Accuracy) = 提取出的正确样本数/总样本数</li>
<li>召回率(Recall) = 正确的正例样本数/样本中的正例样本数- 覆盖率</li>
<li>精确度(Precision) = 正确的正例样本数/预测为正例的样本数</li>
<li>F值 = Precision <em> Recall </em> 2 / (Precision + Recall), 即F值是精准度和召回率的调和平均值</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr>
<td>真实值</td>
<td>正例</td>
<td>true positive真正例(A)</td>
<td>false negative假负例(B)</td>
</tr>
<tr>
<td></td>
<td>负例</td>
<td>false positive假正例(C)</td>
<td>true negative真负例(D)</td>
</tr>
</tbody>
</table>
</div>
<p>true positive(hit)真正例, 确实是正例<br>true negative(Correct Rejection)真负例, 确实是负例<br>false positive(False Alarm)假正例, 本来真实值是负例, 被预测为正例了, 虚报, 干扰报警<br>false negative(Miss)假负例, 本来真实值是正例, 被预测为负例了, 即没有预测出来</p>
<p>击中(Hit)(报准)和正确拒绝(Correct Rejection)是正确反应, 虚报(False Alarm)和漏报(Miss)是错误反应</p>
<p>A和D预测正确, B和C预测错误, 那么计算结果为:</p>
<script type="math/tex; mode=display">Accuracy = (A + D) / (A + B + C + D)</script><script type="math/tex; mode=display">Recall = A / (A + B)</script><script type="math/tex; mode=display">Precision = A / (A + C)</script><script type="math/tex; mode=display">\displaystyle F = \frac{Precision \ast Recall \ast 2}{Precision + Recall}</script><p>举个例子, 真实值中, 正例为80, 负例为20; 预测值中, 正例为90, 负例为10, 然而, 在模型的实际预测中, 原本有75个真正的正例, 有15个是假正例, 5个假负例和5个真负例, 如下图所示:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例90</td>
<td>负例10</td>
</tr>
<tr>
<td>真实值</td>
<td>正例80</td>
<td>真正例(A)75</td>
<td>假负例(B)5</td>
</tr>
<tr>
<td></td>
<td>负例20</td>
<td>假正例(C)15</td>
<td>真负例(D)5</td>
</tr>
</tbody>
</table>
</div>
<p>Accuracy和Recall是一对互斥的关系, Accuracy在增大的时候Recall是在减小的</p>
<h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p>ROC(Receiver Operating Characteristic), 描述的是分类混淆矩阵中FPR-TPR之间的相对变化情况.<br>纵轴是TPR(True Positive Rate), 横轴是FPR(False Positive Rate)</p>
<p>如果二元分类器输出的是对正样本对一个分类概率值, 当取不同阈值时会得到不同当混淆矩阵， 对应于ROC曲线上当一个点, ROC曲线就反应了FPR和TPR之间权衡当情况, 通俗的说, 即在TPR随着FPR递增的情况下, 谁增长的更快, 快多少的问题. TPR增长的越快, 曲线越往上弯曲, AUC就越大, 反应了模型的分类性能就越好. 当正负样本不平衡时, 这种模型评价方式比起一般当精确度评价方式的好处尤其显著.</p>
<h3 id="AUC-Area-Under-Curve"><a href="#AUC-Area-Under-Curve" class="headerlink" title="AUC(Area Under Curve)"></a>AUC(Area Under Curve)</h3><p>AUC被定义为ROC曲线下的面积, 显然这个面积的数值不会大于1. 由于ROC曲线一般都处于<code>y=x</code>这条直线的上方, 所以AUC的取之范围在0.5和1之间. AUC作为数值可以直观的评价分类器的好坏, 值越大越好.</p>
<p>AUC的值一般要求在0.7以上.</p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>回归结果度量</p>
<ul>
<li>explained_varicance_score, 可解释方差的回归评分函数</li>
<li>mean_absolute_error, 平均绝对误差</li>
<li>mean_squared_error, 平均平方误差</li>
</ul>
<h3 id="模型评估总结-分类算法评估方式"><a href="#模型评估总结-分类算法评估方式" class="headerlink" title="模型评估总结_分类算法评估方式"></a>模型评估总结_分类算法评估方式</h3><div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>scikit-learn函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>精确度</td>
<td>from sklearn.metrics import precision_score</td>
</tr>
<tr>
<td>Recall</td>
<td>召回率</td>
<td>from sklearn.metrics import recall_score</td>
</tr>
<tr>
<td>F1</td>
<td>F1指标</td>
<td>from sklearn.metrics import f1_score</td>
</tr>
<tr>
<td>Confusion Matrix</td>
<td>混淆矩阵</td>
<td>from sklearn.metrics import confusion_matrix</td>
</tr>
<tr>
<td>ROC</td>
<td>ROC曲线</td>
<td>from sklearn.metrics import roc</td>
</tr>
<tr>
<td>AUC</td>
<td>ROC曲线下的面积</td>
<td>from sklearn.metrics import auc</td>
</tr>
<tr>
<td>Mean Square Error(MSE, RMSE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_squared_error</td>
</tr>
<tr>
<td>Absolute Error(MAE, RAE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_absolute_error, median_absolute_error</td>
</tr>
<tr>
<td>R-Squared</td>
<td>平均方差</td>
<td>from sklearn.metrics import r2_score</td>
</tr>
</tbody>
</table>
</div>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/20191130/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/20191130/" itemprop="url">20191130</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-03T09:59:25+08:00">
                2019-12-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/aura/" itemprop="url" rel="index">
                    <span itemprop="name">aura</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><h3 id="在scope例子中加入tensorboard-并观察可视化图与没有scope的变化"><a href="#在scope例子中加入tensorboard-并观察可视化图与没有scope的变化" class="headerlink" title="在scope例子中加入tensorboard, 并观察可视化图与没有scope的变化"></a>在scope例子中加入tensorboard, 并观察可视化图与没有scope的变化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">with tf.device(&apos;/cpu:0&apos;):</span><br><span class="line">    with tf.variable_scope(&apos;foo&apos;):</span><br><span class="line">        x_init1 = tf.get_variable(&apos;init_x&apos;, [10], dtype=tf.float32, initializer=tf.random_normal_initializer())[0]</span><br><span class="line">        x = tf.Variable(initial_value=x_init1, name=&apos;x&apos;)</span><br><span class="line">        y = tf.placeholder(dtype=tf.float32, name=&apos;y&apos;)</span><br><span class="line">        z = x + y</span><br><span class="line"></span><br><span class="line">    with tf.variable_scope(&apos;bar&apos;):</span><br><span class="line">        a = tf.constant(3.0) + 4.0</span><br><span class="line"></span><br><span class="line">    w = z * a</span><br><span class="line"></span><br><span class="line"># 开始利用tf.summary记录图的信息, 需要展示的信息</span><br><span class="line"></span><br><span class="line">tf.summary.scalar(&apos;scalar_x_init1&apos;, x_init1)</span><br><span class="line">tf.summary.scalar(name=&apos;scalar_x&apos;, tensor=x)</span><br><span class="line">tf.summary.scalar(name=&apos;scalar_y&apos;, tensor=y)</span><br><span class="line">tf.summary.scalar(name=&apos;scalar_z&apos;, tensor=z)</span><br><span class="line">tf.summary.scalar(name=&apos;scalar_w&apos;, tensor=w)</span><br><span class="line"></span><br><span class="line"># update操作, 这里只能更新x</span><br><span class="line">assign_op = tf.assign(x, x + 1)</span><br><span class="line"></span><br><span class="line">with tf.control_dependencies([assign_op]):</span><br><span class="line">    with tf.device(&apos;/gpu:0&apos;):</span><br><span class="line">        out = x * y</span><br><span class="line">        tf.summary.scalar(name=&apos;scalar_out&apos;, tensor=out)</span><br><span class="line"></span><br><span class="line">with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:</span><br><span class="line">    # merge所有的summary, 触发所有的输出操作</span><br><span class="line">    merged_summary = tf.summary.merge_all()</span><br><span class="line">    # 得到文件的输出对象</span><br><span class="line">    writer = tf.summary.FileWriter(&apos;./result&apos;, sess.graph)</span><br><span class="line">    # 初始化</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    # print</span><br><span class="line"></span><br><span class="line">    for i in range(1, 5):</span><br><span class="line">        summary, r_out, r_x, r_w = sess.run([merged_summary, out, x, w], feed_dict=&#123;y: i&#125;)</span><br><span class="line">        writer.add_summary(summary, i)</span><br><span class="line">        print(&quot;&#123;&#125;, &#123;&#125;, &#123;&#125;&quot;.format(r_out, r_x, r_w))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir tensorflow_excise/result</span><br></pre></td></tr></table></figure>
<p><img src="/20191130/WechatIMG171.png" width="300px"></p>
<h3 id="MNIST-DNN中single-layer-py和single-layer-optimization-py的结果是有差距-寻找原因"><a href="#MNIST-DNN中single-layer-py和single-layer-optimization-py的结果是有差距-寻找原因" class="headerlink" title="MNIST DNN中single-layer.py和single-layer-optimization.py的结果是有差距, 寻找原因"></a>MNIST DNN中single-layer.py和single-layer-optimization.py的结果是有差距, 寻找原因</h3><p><img src="/20191130/WechatIMG172.png" width="300px"></p>
<h2 id="编程题"><a href="#编程题" class="headerlink" title="编程题"></a>编程题</h2><h3 id="MNIST-DNN例子中实现cross-entropy"><a href="#MNIST-DNN例子中实现cross-entropy" class="headerlink" title="MNIST DNN例子中实现cross entropy;"></a>MNIST DNN例子中实现cross entropy;</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"># by cangye@hotmail.com</span><br><span class="line"># 引入库</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 获取数据</span><br><span class="line">mnist = input_data.read_data_sets(&quot;../data/&quot;, one_hot=True)</span><br><span class="line"># 定义全链接层函数</span><br><span class="line">def full_layer(input_tensor, out_dim, name=&apos;full&apos;):</span><br><span class="line">    with tf.variable_scope(name):</span><br><span class="line">        shape = input_tensor.get_shape().as_list()</span><br><span class="line">        W = tf.get_variable(&apos;W&apos;, (shape[1], out_dim), dtype=tf.float32,</span><br><span class="line">                            initializer=tf.truncated_normal_initializer(stddev=0.1))</span><br><span class="line">        b = tf.get_variable(&apos;b&apos;, [out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0))</span><br><span class="line">        out = tf.matmul(input_tensor, W) + b</span><br><span class="line">    return tf.nn.sigmoid(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def model(net, out_dim):</span><br><span class="line">    net = full_layer(net, out_dim, &quot;full_layer1&quot;)</span><br><span class="line">    return net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义输入</span><br><span class="line">with tf.variable_scope(&quot;inputs&quot;):</span><br><span class="line">    x = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">    label = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line"># 引入模型</span><br><span class="line">y = model(x, 10)</span><br><span class="line"># 定义损失函数</span><br><span class="line"># loss = tf.reduce_mean(tf.square(y - label))</span><br><span class="line">ce = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=y)</span><br><span class="line">loss = tf.reduce_mean(ce)</span><br><span class="line"># 用梯度迭代算法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"># 用于验证</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(label, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"># 定义会话</span><br><span class="line">sess = tf.Session()</span><br><span class="line"># 初始化所有变量</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"># 迭代过程</span><br><span class="line">train_writer = tf.summary.FileWriter(&quot;mnist-logdir&quot;, sess.graph)</span><br><span class="line">for itr in range(10000):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, label: batch_ys&#125;)</span><br><span class="line">    if itr % 10 == 0:</span><br><span class="line">        print(&quot;step:%6d  accuracy:&quot; % itr, sess.run(accuracy, feed_dict=&#123;x: mnist.test.images,</span><br><span class="line">                                                                         label: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line"># 这下面的部分用于绘图</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib as mpl</span><br><span class="line"></span><br><span class="line">mpl.style.use(&apos;fivethirtyeight&apos;)</span><br><span class="line"># 获取W取值</span><br><span class="line">with tf.variable_scope(&quot;full_layer1&quot;, reuse=True):</span><br><span class="line">    W = tf.get_variable(&quot;W&quot;)</span><br><span class="line">W = sess.run(W.value())</span><br><span class="line"># 绘图过程</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(221)</span><br><span class="line">ax.matshow(np.reshape(W[:, 1], [28, 28]), cmap=plt.get_cmap(&quot;Purples&quot;))</span><br><span class="line">ax = fig.add_subplot(222)</span><br><span class="line">ax.matshow(np.reshape(W[:, 2], [28, 28]), cmap=plt.get_cmap(&quot;Purples&quot;))</span><br><span class="line">ax = fig.add_subplot(223)</span><br><span class="line">ax.matshow(np.reshape(W[:, 3], [28, 28]), cmap=plt.get_cmap(&quot;Purples&quot;))</span><br><span class="line">ax = fig.add_subplot(224)</span><br><span class="line">ax.matshow(np.reshape(W[:, 4], [28, 28]), cmap=plt.get_cmap(&quot;Purples&quot;))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="在IRIS例子中实现cross-entropy代替MSE和sigmoid"><a href="#在IRIS例子中实现cross-entropy代替MSE和sigmoid" class="headerlink" title="在IRIS例子中实现cross entropy代替MSE和sigmoid"></a>在IRIS例子中实现cross entropy代替MSE和sigmoid</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"># by cangye@hotmail.com</span><br><span class="line"># TensorFlow入门实例</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.contrib.slim as slim</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def variable_summaries(var, name=&quot;layer&quot;):</span><br><span class="line">    with tf.variable_scope(name):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(&apos;mean&apos;, mean)</span><br><span class="line">        with tf.name_scope(&apos;stddev&apos;):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(&apos;stddev&apos;, stddev)</span><br><span class="line">        tf.summary.scalar(&apos;max&apos;, tf.reduce_max(var))</span><br><span class="line">        tf.summary.scalar(&apos;min&apos;, tf.reduce_min(var))</span><br><span class="line">        tf.summary.histogram(&apos;histogram&apos;, var)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&quot;../data/iris.data.csv&quot;)</span><br><span class="line">c_name = set(data.name.values)</span><br><span class="line">print(c_name)</span><br><span class="line">iris_label = np.zeros([len(data.name.values), len(c_name)])</span><br><span class="line">iris_data = data.values[:, :-1]</span><br><span class="line">iris_data = iris_data - np.mean(iris_data, axis=0)</span><br><span class="line">iris_data = iris_data / np.max(iris_data, axis=0)</span><br><span class="line">train_data = []</span><br><span class="line">train_data_label = []</span><br><span class="line">test_data = []</span><br><span class="line">test_data_label = []</span><br><span class="line">for idx, itr_name in enumerate(c_name):</span><br><span class="line">    datas_t = iris_data[data.name.values == itr_name, :]</span><br><span class="line">    labels_t = np.zeros([len(datas_t), len(c_name)])</span><br><span class="line">    labels_t[:, idx] = 1</span><br><span class="line">    train_data.append(datas_t[:30])</span><br><span class="line">    train_data_label.append(labels_t[:30])</span><br><span class="line">    test_data.append(datas_t[30:])</span><br><span class="line">    test_data_label.append(labels_t[30:])</span><br><span class="line">train_data = np.concatenate(train_data)</span><br><span class="line">train_data_label = np.concatenate(train_data_label)</span><br><span class="line">test_data = np.concatenate(test_data)</span><br><span class="line">test_data_label = np.concatenate(test_data_label)</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 4], name=&quot;input_x&quot;)</span><br><span class="line">label = tf.placeholder(tf.float32, [None, 3], name=&quot;input_y&quot;)</span><br><span class="line"># 对于sigmoid激活函数而言，效果可能并不理想</span><br><span class="line">net = slim.fully_connected(x, 4, activation_fn=tf.nn.relu, scope=&apos;full1&apos;, reuse=False)</span><br><span class="line">net = tf.contrib.layers.batch_norm(net)</span><br><span class="line">net = slim.fully_connected(net, 8, activation_fn=tf.nn.relu, scope=&apos;full2&apos;, reuse=False)</span><br><span class="line">net = tf.contrib.layers.batch_norm(net)</span><br><span class="line">net = slim.fully_connected(net, 8, activation_fn=tf.nn.relu, scope=&apos;full3&apos;, reuse=False)</span><br><span class="line">net = tf.contrib.layers.batch_norm(net)</span><br><span class="line">net = slim.fully_connected(net, 4, activation_fn=tf.nn.relu, scope=&apos;full4&apos;, reuse=False)</span><br><span class="line">net = tf.contrib.layers.batch_norm(net)</span><br><span class="line">y = slim.fully_connected(net, 3, activation_fn=tf.nn.sigmoid, scope=&apos;full5&apos;, reuse=False)</span><br><span class="line"></span><br><span class="line">ce = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=y)</span><br><span class="line">loss = tf.reduce_mean(ce)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(label, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"># train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)</span><br><span class="line">var_list_w = [var for var in tf.trainable_variables() if &quot;w&quot; in var.name]</span><br><span class="line">var_list_b = [var for var in tf.trainable_variables() if &quot;b&quot; in var.name]</span><br><span class="line">gradient_w = optimizer.compute_gradients(loss, var_list=var_list_w)</span><br><span class="line">gradient_b = optimizer.compute_gradients(loss, var_list=var_list_b)</span><br><span class="line">for idx, itr_g in enumerate(gradient_w):</span><br><span class="line">    variable_summaries(itr_g[0], &quot;layer%d-w-grad&quot; % idx)</span><br><span class="line">for idx, itr_g in enumerate(gradient_b):</span><br><span class="line">    variable_summaries(itr_g[0], &quot;layer%d-b-grad&quot; % idx)</span><br><span class="line">for idx, itr_g in enumerate(var_list_w):</span><br><span class="line">    variable_summaries(itr_g, &quot;layer%d-w&quot; % idx)</span><br><span class="line">for idx, itr_g in enumerate(var_list_b):</span><br><span class="line">    variable_summaries(itr_g, &quot;layer%d-b&quot; % idx)</span><br><span class="line">train_step = optimizer.apply_gradients(gradient_w + gradient_b)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">train_writer = tf.summary.FileWriter(&quot;logdir-bn&quot;, sess.graph)</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">for itr in range(600):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: train_data, label: train_data_label&#125;)</span><br><span class="line">    if itr % 30 == 0:</span><br><span class="line">        acc1 = sess.run(accuracy, feed_dict=&#123;x: train_data, label: train_data_label&#125;)</span><br><span class="line">        acc2 = sess.run(accuracy, feed_dict=&#123;x: test_data, label: test_data_label&#125;)</span><br><span class="line">        print(&quot;step:&#123;:6d&#125;  train:&#123;:.3f&#125; test:&#123;:.3f&#125;&quot;.format(itr, acc1, acc2))</span><br><span class="line">        summary = sess.run(merged, feed_dict=&#123;x: train_data, label: train_data_label&#125;)</span><br><span class="line">        train_writer.add_summary(summary, itr)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/20191116/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/20191116/" itemprop="url">20191116</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-17T08:49:31+08:00">
                2019-11-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/aura/" itemprop="url" rel="index">
                    <span itemprop="name">aura</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="思考-硬币"><a href="#思考-硬币" class="headerlink" title="思考: 硬币"></a>思考: 硬币</h2><p>硬币a和硬币b, 抛硬币a朝上的概率为0.7, 抛硬币b朝上的概率为0.4</p>
<p>问题1: 从一个黑盒中随机取出一个, 向上抛10次, 结果有6次朝上, 那么拿的是哪个硬币, 为什么?<br>问题2: 分别来自a和b的概率是多少?<br>问题3: 向上抛20次, 有12次正面朝上的概率是多少?</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><div class="table-container">
<table>
<thead>
<tr>
<th>硬币编号</th>
<th>硬币朝上</th>
<th>硬币朝下</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>0.7</td>
<td>0.3</td>
</tr>
<tr>
<td>b</td>
<td>0.4</td>
<td>0.6</td>
</tr>
</tbody>
</table>
</div>
<p>设向上抛硬币a和b是事件A,  那么,<br>抛硬币a朝上的概率为0.7, 记为$P(A_1) = 0.7$, 抛硬币b朝上的概率为0.4, 记为$P(A_2) = 0.4$<br>向上抛10次结果有6次朝上为事件B, 记为$P(B)$</p>
<script type="math/tex; mode=display">\because P(A_1) * 10 = 7 > 6, P(A_2) * 10 = 4 < 6</script><script type="math/tex; mode=display">\therefore 拿的是硬币a</script><p>或者</p>
<script type="math/tex; mode=display">P(B|A_1) = 0.7^6 \ast 0.3^4 = 0.125749</script><script type="math/tex; mode=display">P(B|A_2) = 0.4^6 \ast 0.6^4 = 0.0005308416</script><script type="math/tex; mode=display">\therefore 拿的是硬币a</script><script type="math/tex; mode=display">P(B) = P(B|A_1) + P(B|A_2) = 0.125749 + 0.0005308416 = 0.1262798416</script><script type="math/tex; mode=display">P(A_1|B) = \frac{P(B|A_1) \ast P(A_1)}{P(B)} = \frac{0.7^6*0.3^4*0.7}{P(B)} = 0.00528</script><script type="math/tex; mode=display">P(A_2|B) = \frac{P(B|A_2) \ast P(A_2)}{P(B)} = \frac{0.4^6*0.6^4*0.4}{P(B)} = 0.00168</script><script type="math/tex; mode=display">\therefore 来自a的概率为0.00528, 来自b的概率为0.00168</script><p>向上抛20次, 12次正面朝上的概率为:</p>
<script type="math/tex; mode=display">P = C_{20}^{12} \ast 0.7^{12} \ast 0.3^8 + 0.4^{12} \ast 0.6^8 = 0.599576</script><h2 id="思考-为什么SVD之后cos距离最近"><a href="#思考-为什么SVD之后cos距离最近" class="headerlink" title="思考: 为什么SVD之后cos距离最近"></a>思考: 为什么SVD之后cos距离最近</h2><p>几何中夹角余弦可用来衡量两个向量方向的差异, 所以可以用余弦相似度来衡量样本向量之间的差异<br>所以在文本做完SVD之后, 用余弦相似度来衡量输入文本中每个文档(即每句话)之间的相似度</p>
<h2 id="编程-病情预测"><a href="#编程-病情预测" class="headerlink" title="编程: 病情预测"></a>编程: 病情预测</h2><p>6位建筑工人历史病例数据如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>症状</th>
<th>职业</th>
<th>疾病</th>
</tr>
</thead>
<tbody>
<tr>
<td>打喷嚏</td>
<td>护士</td>
<td>感冒</td>
</tr>
<tr>
<td>打喷嚏</td>
<td>农夫</td>
<td>过敏</td>
</tr>
<tr>
<td>头痛</td>
<td>建筑工人</td>
<td>脑震荡</td>
</tr>
<tr>
<td>头痛</td>
<td>建筑工人</td>
<td>感冒</td>
</tr>
<tr>
<td>打喷嚏</td>
<td>教师</td>
<td>感冒</td>
</tr>
<tr>
<td>头痛</td>
<td>教师</td>
<td>脑震荡</td>
</tr>
</tbody>
</table>
</div>
<p>根据以上历史数据, 预测新来的一位打喷嚏的建筑工人患感冒的概率</p>
<h3 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h3><p>从上述数据, 可以直观的得到, 症状列表为$[“打喷嚏”, “头痛”]$, 职业列表为$[“护士”, “建筑工人”, “教师”, “农夫”]$, 疾病列表为$[“感冒”, “过敏”, “脑震荡”]$</p>
<h3 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h3><p>如果从2种症状判断为3种疾病的关系表如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>症状</th>
<th>疾病</th>
</tr>
</thead>
<tbody>
<tr>
<td>打喷嚏</td>
<td>感冒</td>
</tr>
<tr>
<td>打喷嚏</td>
<td>过敏</td>
</tr>
<tr>
<td>头痛</td>
<td>脑震荡</td>
</tr>
<tr>
<td>头痛</td>
<td>感冒</td>
</tr>
<tr>
<td>打喷嚏</td>
<td>感冒</td>
</tr>
<tr>
<td>头痛</td>
<td>脑震荡</td>
</tr>
</tbody>
</table>
</div>
<p>统计如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>症状</th>
<th>疾病</th>
<th>次数</th>
</tr>
</thead>
<tbody>
<tr>
<td>打喷嚏</td>
<td>感冒</td>
<td>2</td>
</tr>
<tr>
<td>打喷嚏</td>
<td>过敏</td>
<td>1</td>
</tr>
<tr>
<td>头痛</td>
<td>脑震荡</td>
<td>2</td>
</tr>
<tr>
<td>头痛</td>
<td>感冒</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>或者如下(将行列转置一下):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>T1(症状-疾病表)</th>
<th>感冒</th>
<th>过敏</th>
<th>脑震荡</th>
</tr>
</thead>
<tbody>
<tr>
<td>打喷嚏</td>
<td>2</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>头痛</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>如果从4种职业判断为3种疾病的关系表如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>职业</th>
<th>疾病</th>
<th>次数</th>
</tr>
</thead>
<tbody>
<tr>
<td>护士</td>
<td>感冒</td>
<td>1</td>
</tr>
<tr>
<td>农夫</td>
<td>过敏</td>
<td>1</td>
</tr>
<tr>
<td>建筑工人</td>
<td>脑震荡</td>
<td>1</td>
</tr>
<tr>
<td>建筑工人</td>
<td>感冒</td>
<td>1</td>
</tr>
<tr>
<td>教师</td>
<td>感冒</td>
<td>1</td>
</tr>
<tr>
<td>教师</td>
<td>脑震荡</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>或者如下(将行列转置一下):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>T2(职业-疾病表)</th>
<th>感冒</th>
<th>过敏</th>
<th>脑震荡</th>
</tr>
</thead>
<tbody>
<tr>
<td>护士</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>农夫</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>建筑工人</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>教师</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>从T1(症状-疾病表), 得到:</p>
<p>P(感冒|打喷嚏) = 2/3, P(打喷嚏|感冒) = 2/3<br>P(感冒|头痛) = 1/3, P(头痛|感冒) = 1/3</p>
<p>P(过敏|打喷嚏) = 1, P(打喷嚏|过敏) = 1/1<br>P(过敏|头痛) = 0, P(头痛|过敏) = 0/1</p>
<p>P(脑震荡|打喷嚏) = 0, P(打喷嚏|脑震荡) = 0/2<br>P(脑震荡|头痛) = 1, P(头痛|脑震荡) = 2/2</p>
<p>P(打喷嚏) = 3/6<br>P(头痛) = 3/6</p>
<p>P(感冒) = 3/6<br>P(过敏) = 1/6<br>P(脑震荡) = 2/6</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>P(疾病&#124;症状)概率表</th>
<th>感冒</th>
<th>过敏</th>
<th>脑震荡</th>
</tr>
</thead>
<tbody>
<tr>
<td>打喷嚏</td>
<td>2/3</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>头痛</td>
<td>1/3</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>从T2(职业-疾病表), 得到:</p>
<p>P(感冒|护士) = 1/1, P(护士|感冒) = 1/3<br>P(过敏|护士) = 0/1, P(护士|过敏) = 0/1<br>P(脑震荡|护士) = 0/1, P(护士|脑震荡) = 0/2 </p>
<p>P(感冒|农夫) = 0/1, P(农夫|感冒) = 0/3<br>P(过敏|农夫) = 1/1, P(农夫|过敏) = 1/1<br>P(脑震荡|农夫) = 0/1, P(农夫|脑震荡) = 0/2 </p>
<p>P(感冒|建筑工人) = 1/2, P(建筑工人|感冒) = 1/3<br>P(过敏|建筑工人) = 0/2, P(建筑工人|过敏) = 0/1<br>P(脑震荡|建筑工人) = 1/2, P(建筑工人|脑震荡) = 1/2</p>
<p>P(感冒|教师) = 1/2, P(教师|感冒) = 1/3<br>P(过敏|教师) = 0/2, P(教师|过敏) = 0/1<br>P(脑震荡|教师) = 1/2, P(教师|脑震荡) = 1/2</p>
<p>P(护士) = 1/6<br>P(农夫) = 1/6<br>P(建筑工人) = 2/6<br>P(教师) = 2/6</p>
<p>P(感冒) = 3/6<br>P(过敏) = 1/6<br>P(脑震荡) = 2/6</p>
<h3 id="计算结果"><a href="#计算结果" class="headerlink" title="计算结果"></a>计算结果</h3><script type="math/tex; mode=display">\begin{align} P(感冒|建筑工人, 打喷嚏) &= \frac{P(建筑工人, 打喷嚏|感冒) \ast P(感冒)}{P(建筑工人, 打喷嚏)} \\
&假设建筑工人和打喷嚏两件事相互独立\\
&= \frac{P(建筑工人|感冒) \ast P(打喷嚏|感冒) \ast P(感冒)}{P(建筑工人) \ast P(打喷嚏)} \\
&= \frac{ 1/3 \ast 2/3 \ast 3/6 }{ 2/6 \ast 3/6 } \\
&= \frac{2}{3}
\end{align}</script><h3 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h3><p>history.csv<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">症状,职业,疾病</span><br><span class="line">打喷嚏,护士,感冒</span><br><span class="line">打喷嚏,农夫,过敏</span><br><span class="line">头痛,建筑工人,脑震荡</span><br><span class="line">头痛,建筑工人,感冒</span><br><span class="line">打喷嚏,教师,感冒</span><br><span class="line">头痛,教师,脑震荡</span><br></pre></td></tr></table></figure></p>
<p>predict_sick.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">症状列表:</span><br><span class="line">[“打喷嚏”,“头痛”]</span><br><span class="line">职业列表:</span><br><span class="line">[“护士”,“建筑工人”,“教师”,“农夫”]</span><br><span class="line">疾病列表:</span><br><span class="line">[“感冒”,“过敏”,“脑震荡”]</span><br><span class="line">历史数据:</span><br><span class="line">症状, 职业, 疾病</span><br><span class="line">打喷嚏, 护士, 感冒</span><br><span class="line">打喷嚏, 农夫, 过敏</span><br><span class="line">头痛, 建筑工人, 脑震荡</span><br><span class="line">头痛, 建筑工人, 感冒</span><br><span class="line">打喷嚏, 教师, 感冒</span><br><span class="line">头痛, 教师, 脑震荡</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&apos;history.csv&apos;)</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line">sickness = data[&quot;疾病&quot;].value_counts()</span><br><span class="line">occupation = data[&quot;职业&quot;].value_counts()</span><br><span class="line">symptom = data[&quot;症状&quot;].value_counts()</span><br><span class="line">print(sickness)</span><br><span class="line">print(occupation)</span><br><span class="line">print(symptom)</span><br><span class="line"></span><br><span class="line"># P(打喷嚏)</span><br><span class="line">p_pengti = symptom[&quot;打喷嚏&quot;] / sum(symptom)</span><br><span class="line">print(f&quot;p_pengti:\n &#123;p_pengti&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># P(建筑工人)</span><br><span class="line">p_worker = occupation[&quot;建筑工人&quot;] / sum(occupation)</span><br><span class="line">print(f&quot;p_worker:\n &#123;p_worker&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># P(感冒)</span><br><span class="line">p_cold = sickness[&quot;感冒&quot;] / sum(sickness)</span><br><span class="line">print(f&quot;p_cold:\n &#123;p_cold&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># P(建筑工人|感冒)</span><br><span class="line">df_worker_cold = data[[&quot;职业&quot;, &quot;疾病&quot;]]</span><br><span class="line">print(f&quot;df_worker_cold:\n &#123;df_worker_cold&#125;&quot;)</span><br><span class="line">for index, row in df_worker_cold.iterrows():</span><br><span class="line">    if row[&quot;疾病&quot;] == &quot;感冒&quot;:</span><br><span class="line">        df_worker_cold[&quot;疾病&quot;][index] = 1</span><br><span class="line">        if row[&quot;职业&quot;] == &quot;建筑工人&quot;:</span><br><span class="line">            df_worker_cold[&quot;职业&quot;][index] = 1</span><br><span class="line">        else:</span><br><span class="line">            df_worker_cold[&quot;职业&quot;][index] = 0</span><br><span class="line">    else:</span><br><span class="line">        df_worker_cold[&quot;疾病&quot;][index] = 0</span><br><span class="line">        if row[&quot;职业&quot;] == &quot;建筑工人&quot;:</span><br><span class="line">            df_worker_cold[&quot;职业&quot;][index] = 1</span><br><span class="line">        else:</span><br><span class="line">            df_worker_cold[&quot;职业&quot;][index] = 0</span><br><span class="line"></span><br><span class="line">print(f&quot;df_worker_cold:\n &#123;df_worker_cold&#125;&quot;)</span><br><span class="line">p_worker_cold = (df_worker_cold[&quot;职业&quot;] &amp; df_worker_cold[&quot;疾病&quot;]).sum() / df_worker_cold[&quot;疾病&quot;].sum()</span><br><span class="line">print(f&quot;p_worker_cold:\n &#123;p_worker_cold&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># P(打喷嚏|感冒)</span><br><span class="line">df_pengti_cold = data[[&quot;症状&quot;, &quot;疾病&quot;]]</span><br><span class="line">print(f&quot;df_pengti_cold:\n &#123;df_pengti_cold&#125;&quot;)</span><br><span class="line">for i, j in df_pengti_cold.iterrows():</span><br><span class="line">    # print(i, j)</span><br><span class="line">    if j[&quot;疾病&quot;].strip() == &quot;感冒&quot;:</span><br><span class="line">        df_pengti_cold[&quot;疾病&quot;][i] = 1</span><br><span class="line">        if j[&quot;症状&quot;] == &quot;打喷嚏&quot;:</span><br><span class="line">            df_pengti_cold[&quot;症状&quot;][i] = 1</span><br><span class="line">        else:</span><br><span class="line">            df_pengti_cold[&quot;症状&quot;][i] = 0</span><br><span class="line">    else:</span><br><span class="line">        df_pengti_cold[&quot;疾病&quot;][i] = 0</span><br><span class="line">        if j[&quot;症状&quot;] == &quot;打喷嚏&quot;:</span><br><span class="line">            df_pengti_cold[&quot;症状&quot;][i] = 1</span><br><span class="line">        else:</span><br><span class="line">            df_pengti_cold[&quot;症状&quot;][i] = 0</span><br><span class="line"></span><br><span class="line">print(f&quot;df_pengti_cold:\n &#123;df_pengti_cold&#125;&quot;)</span><br><span class="line">p_pengti_cold = (df_pengti_cold[&quot;症状&quot;] &amp; df_pengti_cold[&quot;疾病&quot;]).sum() / df_pengti_cold[&quot;疾病&quot;].sum()</span><br><span class="line">print(f&quot;p_pengti_cold:\n &#123;p_pengti_cold&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># P(感冒|建筑工人,打喷嚏)</span><br><span class="line">p_cold_worker_and_pengti = (p_worker_cold * p_pengti_cold * p_cold) / (p_worker * p_pengti)</span><br><span class="line">print(f&quot;p_cold_worker_and_pengti:\n &#123;p_cold_worker_and_pengti&#125;&quot;)</span><br></pre></td></tr></table></figure></p>
<p><img src="/20191116/WechatIMG168.png" width="300px"></p>
<h2 id="编程-在SVD基础上进行TF-IDF"><a href="#编程-在SVD基础上进行TF-IDF" class="headerlink" title="编程: 在SVD基础上进行TF-IDF"></a>编程: 在SVD基础上进行TF-IDF</h2><ul>
<li>将一个文本当作一个无序的数据集合, 文本特征可以采用文本中的词条T进行体现, 那么文本中出现的所有词条及其出现的次数就可以体现文档的特征</li>
<li>TF-IDF, 词条的重要性随着它在文件中出现的次数成正比增加, 但同时会随着它在语料库中出现的频率成反比下降;<ul>
<li>词条在文本中出现的次数越多, 表示该词条对该文本的重要性越高</li>
<li>词条在所有文本中出现的次数越少, 说明这个词条对文本的重要性越高</li>
<li>TF(词频)指某个词条在文本中出现的次数, 一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)</li>
<li>IDF(逆向文件频率)指一条词条重要性的度量, 一般计算方式为总文件数目除以包含该词语之文件的数目, 再将得到的商取对数得到. TF-IDF即TF*IDF</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}</script><script type="math/tex; mode=display">逆文档频率(IDF) = log(\frac{语料库的文档总数}{包含该词的文档数 + 1})</script><p><strong>注意</strong>: IDF取对数时，分母$+1$是为了方式分母为0</p>
<p>tf-idf-svd.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pylab</span><br><span class="line">import re</span><br><span class="line">from scipy import linalg</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line"></span><br><span class="line"># 文档</span><br><span class="line">documents = [</span><br><span class="line">    &quot;Roronoa Zoro, nicknamed \&quot;Pirate Hunter\&quot; Zoro, is a fictional character in the One Piece franchise created by Eiichiro Oda.&quot;,</span><br><span class="line">    &quot;In the story, Zoro is the first to join Monkey D. Luffy after he is saved from being executed at the Marine Base. &quot;,</span><br><span class="line">    &quot;Zoro is an expert swordsman who uses three swords for his Three Sword Style, but is also capable of the one and two-sword styles. &quot;,</span><br><span class="line">    &quot;Zoro seems to be more comfortable and powerful using three swords, but he also uses one sword or two swords against weaker enemies.&quot;,</span><br><span class="line">    &quot;In One Piece, Luffy sails from the East Blue to the Grand Line in search of the legendary treasure One Piece to succeed Gol D. Roger as the King of the Pirates. &quot;,</span><br><span class="line">    &quot;Luffy is the captain of the Straw Hat Pirates and along his journey, he recruits new crew members with unique abilities and personalities. &quot;,</span><br><span class="line">    &quot;Luffy often thinks with his stomach and gorges himself to comical levels. &quot;,</span><br><span class="line">    &quot;However, Luffy is not as naive as many people believe him to be, showing more understanding in situations than people often expect. &quot;,</span><br><span class="line">    &quot;Knowing the dangers ahead, Luffy is willing to risk his life to reach his goal to become the King of the Pirates, and protect his crew.&quot;,</span><br><span class="line">    &quot;Adopted and raised by Navy seaman turned tangerine farmer Bellemere, Nami and her older sister Nojiko, have to witness their mother being murdered by the infamous Arlong.&quot;,</span><br><span class="line">    &quot;Nami, still a child but already an accomplished cartographer who dreams of drawing a complete map of the world, joins the pirates, hoping to eventually buy freedom for her village. &quot;,</span><br><span class="line">    &quot;Growing up as a pirate-hating pirate, drawing maps for Arlong and stealing treasure from other pirates, Nami becomes an excellent burglar, pickpocket and navigator with an exceptional ability to forecast weather.&quot;,</span><br><span class="line">    &quot;After Arlong betrays her, and he and his gang are defeated by the Straw Hat Pirates, Nami joins the latter in pursuit of her dream.&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">print(f&apos;len(documents): &#123;len(documents)&#125;&apos;)</span><br><span class="line"># 停用词</span><br><span class="line">stopwords = [&apos;a&apos;, &apos;an&apos;, &apos;after&apos;, &apos;also&apos;, &apos;and&apos;, &apos;as&apos;, &apos;be&apos;, &apos;being&apos;, &apos;but&apos;, &apos;by&apos;, &apos;d&apos;, &apos;for&apos;, &apos;from&apos;, &apos;he&apos;, &apos;her&apos;,</span><br><span class="line">             &apos;his&apos;, &apos;in&apos;, &apos;is&apos;, &apos;more&apos;, &apos;of&apos;, &apos;often&apos;, &apos;the&apos;, &apos;to&apos;, &apos;who&apos;, &apos;with&apos;, &apos;people&apos;]</span><br><span class="line"># 要去除的标点符号的正则表达式</span><br><span class="line">punctuation_regex = &apos;[,.;&quot;]+&apos;</span><br><span class="line"># 构建一个字典, key是单词, value是单词出现的文档编号</span><br><span class="line">dictionary = &#123;&#125;</span><br><span class="line"></span><br><span class="line"># 当前处理的文档编号</span><br><span class="line">currentDocId = 0</span><br><span class="line"></span><br><span class="line"># 语料库的文档总数</span><br><span class="line">documents_total = len(documents)</span><br><span class="line"># 文档的总词数, 用来记录每篇文档的总词数</span><br><span class="line">documents_len = dict()</span><br><span class="line"></span><br><span class="line"># 依次处理每篇文档</span><br><span class="line">for d in documents:</span><br><span class="line">    words = d.split()</span><br><span class="line">    # 初始化每篇文档总词数</span><br><span class="line">    words_count = 0</span><br><span class="line"></span><br><span class="line">    for w in words:</span><br><span class="line">        # 去标点</span><br><span class="line">        w = re.sub(punctuation_regex, &apos;&apos;, w.lower())</span><br><span class="line">        if w in stopwords:</span><br><span class="line">            continue</span><br><span class="line">        elif w in dictionary:</span><br><span class="line">            dictionary[w].append(currentDocId)</span><br><span class="line">        else:</span><br><span class="line">            dictionary[w] = [currentDocId]</span><br><span class="line"></span><br><span class="line">        words_count += 1</span><br><span class="line">    documents_len[currentDocId] = words_count</span><br><span class="line">    currentDocId += 1</span><br><span class="line"></span><br><span class="line">print(f&quot;dictionary: &#123;dictionary&#125;&quot;)</span><br><span class="line">print(f&quot;语料库的文档总数documents_total: &#123;documents_total&#125;&quot;)</span><br><span class="line">print(f&quot;每篇文档的总词数documents_len: &#123;documents_len&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># tf = 某个词在文章中出现的次数 / 文章的总词数</span><br><span class="line"># idf = log (语料库的文档总数 / (包含该词的文档数 + 1))</span><br><span class="line">idf_dict = dict()</span><br><span class="line">print(f&quot;documents: &#123;documents&#125;&quot;)</span><br><span class="line">import math</span><br><span class="line">for k, v in dictionary.items():</span><br><span class="line">    idf_dict[k] = math.log2(float(documents_total) / float((len(v) + 1)))</span><br><span class="line"></span><br><span class="line"># 至少出现在两个文档中的单词选为关键词</span><br><span class="line">keywords = [k for k in dictionary.keys() if len(dictionary[k]) &gt; 1]</span><br><span class="line">print(f&apos;keywords: &#123;len(keywords)&#125;, &#123;keywords&#125;&apos;)</span><br><span class="line">keywords.sort()</span><br><span class="line">print(f&apos;keywords: &#123;len(keywords)&#125;, &#123;keywords&#125;&apos;)</span><br><span class="line"></span><br><span class="line"># 生成word-document矩阵[19, 13]</span><br><span class="line">X = np.zeros([len(keywords), currentDocId])</span><br><span class="line">print(f&apos;X: &#123;X&#125;&apos;)</span><br><span class="line">for i, k in enumerate(keywords):</span><br><span class="line">    countWordDocument = &#123;&#125;</span><br><span class="line">    for d in dictionary[k]:</span><br><span class="line">        if d in countWordDocument:</span><br><span class="line">            countWordDocument[d] += 1</span><br><span class="line">        else:</span><br><span class="line">            countWordDocument[d] = 1</span><br><span class="line">    for d, v in countWordDocument.items():</span><br><span class="line">        tf = float(v) / (documents_len[d])</span><br><span class="line">        idf = idf_dict[k]</span><br><span class="line">        tf_idf = tf / idf</span><br><span class="line">        X[i, d] += tf_idf</span><br><span class="line">        # X[i, d] += 1</span><br><span class="line"></span><br><span class="line">print(f&apos;X: &#123;X&#125;&apos;)</span><br><span class="line"></span><br><span class="line"># 奇异值分解</span><br><span class="line">U, sigma, V = linalg.svd(X, full_matrices=True)</span><br><span class="line"></span><br><span class="line">print(&quot;U:\n&quot;, U.shape, &quot;\n&quot;, U, &quot;\n&quot;)</span><br><span class="line">print(&quot;SIGMA:\n&quot;, sigma.shape, sigma, &quot;\n&quot;)</span><br><span class="line">print(&quot;V:\n&quot;, V.shape, V, &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line"># 得到降维(降到targetDimension维)后单词与文档的坐标表示</span><br><span class="line">targetDimension = 2</span><br><span class="line">U2 = U[0:, 0:targetDimension]</span><br><span class="line">V2 = V[0:targetDimension, 0:]</span><br><span class="line">sigma2 = np.diag(sigma[0:targetDimension])</span><br><span class="line"></span><br><span class="line">print(f&quot;U.shape: &#123;U.shape&#125;, sigma.shape: &#123;sigma.shape&#125;, V.shape: &#123;V.shape&#125;&quot;)</span><br><span class="line">print(f&quot;U2.shape: &#123;U2.shape&#125;, sigma2.shape: &#123;sigma2.shape&#125;, V2.shape: &#123;V2.shape&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 对比原始矩阵与降维结果</span><br><span class="line">X2 = np.dot(np.dot(U2, sigma2), V2)</span><br><span class="line">print(&quot;X:\n&quot;, X)</span><br><span class="line">print(&quot;X2:\n&quot;, X2)</span><br><span class="line"></span><br><span class="line"># 开始画图</span><br><span class="line">pyplot.title(&quot;LSA&quot;)</span><br><span class="line">pyplot.xlabel(u&apos;x&apos;)</span><br><span class="line">pyplot.ylabel(u&apos;y&apos;)</span><br><span class="line"></span><br><span class="line"># 绘制单词表示的点</span><br><span class="line"># U2的每一行包含了每个单词的坐标表示(维度是targetDimension)，此处使用前两个维度的坐标画图</span><br><span class="line">for i in range(len(U2)):</span><br><span class="line">    pylab.text(U2[i][0], U2[i][1], keywords[i], fontsize=10)</span><br><span class="line">    print(&quot;(&quot;, U2[i][0], &quot;,&quot;, U2[i][1], &quot;)&quot;, keywords[i])</span><br><span class="line">x = U2.T[0]</span><br><span class="line">y = U2.T[1]</span><br><span class="line">pylab.plot(x, y, &apos;.&apos;)</span><br><span class="line"></span><br><span class="line"># 绘制文档表示的点</span><br><span class="line"># V2的每一列包含了每个文档的坐标表示(维度是targetDimension)，此处使用前两个维度的坐标画图</span><br><span class="line">Dkey = []</span><br><span class="line">for i in range(len(V2[0])):</span><br><span class="line">    pylab.text(V2[0][i], V2[1][i], (&apos;D%d&apos; % (i + 1)), fontsize=10)</span><br><span class="line">    print(&quot;(&quot;, V2[0][i], &quot;,&quot;, V2[1][i], &quot;)&quot;, (&apos;D%d&apos; % (i + 1)))</span><br><span class="line">    Dkey.append(&apos;D%d&apos; % (i + 1))</span><br><span class="line">x = V[0]</span><br><span class="line">y = V[1]</span><br><span class="line">pylab.plot(x, y, &apos;x&apos;)</span><br><span class="line"></span><br><span class="line">pylab.savefig(&quot;tf-idf-svd.png&quot;, dpi=100)</span><br><span class="line"></span><br><span class="line">exit()</span><br></pre></td></tr></table></figure></p>
<p><img src="/20191116/tf-idf-svd.png" width="300px"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/梯度下降求解二元函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/梯度下降求解二元函数/" itemprop="url">梯度下降求解二元函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-01T10:08:07+08:00">
                2019-11-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f_2d(x, y):</span><br><span class="line">    return x ** 2 + 3 * x + y ** 2 + 8 * y + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def df_2d(x, y):</span><br><span class="line">    return 2 * x + 3, 2 * y + 8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, y = 4, 4</span><br><span class="line">learning_rate = 0.01</span><br><span class="line"></span><br><span class="line">for itr in range(200):</span><br><span class="line">    v_x, v_y = df_2d(x, y)</span><br><span class="line">    x, y = x - learning_rate * v_x, y - learning_rate * v_y</span><br><span class="line">    print(x, y, f_2d(x, y))</span><br><span class="line"></span><br><span class="line">x = np.linspace(-10, 10)</span><br><span class="line">y = np.linspace(-10, 10)</span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.gca(projection=&apos;3d&apos;)</span><br><span class="line">surf = ax.plot_surface(X, Y, f_2d(X, Y))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/机器学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/机器学习/" itemprop="url">机器学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-31T08:18:24+08:00">
                2019-10-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="机器学习分类维度一"><a href="#机器学习分类维度一" class="headerlink" title="机器学习分类维度一"></a>机器学习分类维度一</h2><ul>
<li>有监督学习</li>
<li>无监督学习</li>
<li>半监督学习</li>
</ul>
<h2 id="机器学习分类维度二"><a href="#机器学习分类维度二" class="headerlink" title="机器学习分类维度二"></a>机器学习分类维度二</h2><ul>
<li>分类</li>
<li>聚类</li>
<li>回归</li>
<li>关联规则</li>
</ul>
<h2 id="模型训练及测试"><a href="#模型训练及测试" class="headerlink" title="模型训练及测试"></a>模型训练及测试</h2><ul>
<li>模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择</li>
<li>在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见)</li>
<li>模型的选择会尽可能多的选择算法进行执行, 并比较执行结果 </li>
<li>模型的测试一般从以下几个方面来进行比较, 分别是准确率, 召回率, 精准率, F值 <ul>
<li>准确率(Accuracy) = 提取出的正确样本数/总样本数</li>
<li>召回率(Recall) = 正确的正例样本数/样本中的正例样本数- 覆盖率</li>
<li>精确度(Precision) = 正确的正例样本数/预测为正例的样本数</li>
<li>F值 = Precision <em> Recall </em> 2 / (Precision + Recall), 即F值是精准度和召回率的调和平均值</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr>
<td>真实值</td>
<td>正例</td>
<td>true positive真正例(A)</td>
<td>false negative假负例(B)</td>
</tr>
<tr>
<td></td>
<td>负例</td>
<td>false positive假正例(C)</td>
<td>true negative真负例(D)</td>
</tr>
</tbody>
</table>
</div>
<p>true positive(hit)真正例, 确实是正例<br>true negative(Correct Rejection)真负例, 确实是负例<br>false positive(False Alarm)假正例, 本来真实值是负例, 被预测为正例了, 虚报, 干扰报警<br>false negative(Miss)假负例, 本来真实值是正例, 被预测为负例了, 即没有预测出来</p>
<p>击中(Hit)(报准)和正确拒绝(Correct Rejection)是正确反应, 虚报(False Alarm)和漏报(Miss)是错误反应</p>
<p>A和D预测正确, B和C预测错误, 那么计算结果为:</p>
<script type="math/tex; mode=display">Accuracy = (A + D) / (A + B + C + D)</script><script type="math/tex; mode=display">Recall = A / (A + B)</script><script type="math/tex; mode=display">Precision = A / (A + C)</script><script type="math/tex; mode=display">\displaystyle F = \frac{Precision \ast Recall \ast 2}{Precision + Recall}</script><p>举个例子, 真实值中, 正例为80, 负例为20; 预测值中, 正例为90, 负例为10, 然而, 在模型的实际预测中, 原本有75个真正的正例, 有15个是假正例, 5个假负例和5个真负例, 如下图所示:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例90</td>
<td>负例10</td>
</tr>
<tr>
<td>真实值</td>
<td>正例80</td>
<td>真正例(A)75</td>
<td>假负例(B)5</td>
</tr>
<tr>
<td></td>
<td>负例20</td>
<td>假正例(C)15</td>
<td>真负例(D)5</td>
</tr>
</tbody>
</table>
</div>
<p>Accuracy和Recall是一对互斥的关系, Accuracy在增大的时候Recall是在减小的</p>
<p><a href="/ai/AI/precision和accuracy的区别/">precision和accuracy的区别</a></p>
<h2 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h2><p>ROC(Receiver Operating Characteristic), 描述的是分类混淆矩阵中FPR-TPR之间的相对变化情况.<br>纵轴是TPR(True Positive Rate), 横轴是FPR(False Positive Rate)</p>
<p>如果二元分类器输出的是对正样本对一个分类概率值, 当取不同阈值时会得到不同当混淆矩阵， 对应于ROC曲线上当一个点, ROC曲线就反应了FPR和TPR之间权衡当情况, 通俗的说, 即在TPR随着FPR递增的情况下, 谁增长的更快, 快多少的问题. TPR增长的越快, 曲线越往上弯曲, AUC就越大, 反应了模型的分类性能就越好. 当正负样本不平衡时, 这种模型评价方式比起一般当精确度评价方式的好处尤其显著. </p>
<h3 id="AUC-Area-Under-Curve"><a href="#AUC-Area-Under-Curve" class="headerlink" title="AUC(Area Under Curve)"></a>AUC(Area Under Curve)</h3><p>AUC被定义为ROC曲线下的面积, 显然这个面积的数值不会大于1. 由于ROC曲线一般都处于<code>y=x</code>这条直线的上方, 所以AUC的取之范围在0.5和1之间. AUC作为数值可以直观的评价分类器的好坏, 值越大越好.</p>
<p>AUC的值一般要求在0.7以上.</p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>回归结果度量</p>
<ul>
<li>explained_varicance_score, 可解释方差的回归评分函数</li>
<li>mean_absolute_error, 平均绝对误差</li>
<li>mean_squared_error, 平均平方误差</li>
</ul>
<h3 id="模型评估总结-分类算法评估方式"><a href="#模型评估总结-分类算法评估方式" class="headerlink" title="模型评估总结_分类算法评估方式"></a>模型评估总结_分类算法评估方式</h3><div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>scikit-learn函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>精确度</td>
<td>from sklearn.metrics import precision_score</td>
</tr>
<tr>
<td>Recall</td>
<td>召回率</td>
<td>from sklearn.metrics import recall_score</td>
</tr>
<tr>
<td>F1</td>
<td>F1指标</td>
<td>from sklearn.metrics import f1_score</td>
</tr>
<tr>
<td>Confusion Matrix</td>
<td>混淆矩阵</td>
<td>from sklearn.metrics import confusion_matrix</td>
</tr>
<tr>
<td>ROC</td>
<td>ROC曲线</td>
<td>from sklearn.metrics import roc</td>
</tr>
<tr>
<td>AUC</td>
<td>ROC曲线下的面积</td>
<td>from sklearn.metrics import auc</td>
</tr>
<tr>
<td>Mean Square Error(MSE, RMSE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_squared_error</td>
</tr>
<tr>
<td>Absolute Error(MAE, RAE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_absolute_error, median_absolute_error</td>
</tr>
<tr>
<td>R-Squared</td>
<td>平均方差</td>
<td>from sklearn.metrics import r2_score</td>
</tr>
</tbody>
</table>
</div>
<h2 id="数据清洗和转换"><a href="#数据清洗和转换" class="headerlink" title="数据清洗和转换"></a>数据清洗和转换</h2><ul>
<li>实际生产环境中机器学习比较耗时的一部分</li>
<li>大部分机器学习模型所处理的都是特征, 特征通常是输入变量所对应的可用于模型的数值表示</li>
<li>大部分情况下, 收集到的数据需要经过预处理之后才能够被算法所用, 预处理的操作包括以下几个部分:<ul>
<li>数据过滤</li>
<li>处理数据缺失</li>
<li>处理可能的异常, 错误或异常值</li>
<li>合并多个数据源数据</li>
<li>数据汇总</li>
</ul>
</li>
<li>对数据进行初步的预处理, 需要将其转换为一种适合机器学习模型的表示形式, 对许多模型来说, 这种表示就是包含数值数据的向量或者矩阵<ul>
<li>将类别数据编码成为对应的数值表示(一般使用1-of-k方法)-dumy</li>
<li>从文本数据中提取有用的数据(一般使用词袋法或者TF-IDF)</li>
<li>处理图像或者音频数据(像素, 声波, 音频, 振幅&lt;傅立叶变换&gt;)</li>
<li>数值数据转换为类别数据以减少变量的值, 比如年龄分段</li>
<li>对数值数据进行转换, 比如对数转换</li>
<li>对特征进行正则化, 标准化, 以保证同一模型的不同输入变量的值域相同</li>
<li>对现有变量进行组合或转换以生成新特征, 比如平均数, 可以做虚拟变量不断尝试</li>
</ul>
</li>
</ul>
<h3 id="类型特征转换1-of-k-哑编码"><a href="#类型特征转换1-of-k-哑编码" class="headerlink" title="类型特征转换1-of-k(哑编码)"></a>类型特征转换1-of-k(哑编码)</h3><ul>
<li>将非数值型的特征值转换为数值型的数据</li>
<li>假设变量的取值有k个, 如果对这些值用1到k编序, 则可用维度为k对向量来表示一个变量对值. 在这样的向量里, 该取值所对应的序号所在的元素为1, 其他元素均为0.</li>
</ul>
<h3 id="文本数据抽取"><a href="#文本数据抽取" class="headerlink" title="文本数据抽取"></a>文本数据抽取</h3><ul>
<li>将一个文本当作一个无序的数据集合, 文本特征可以采用文本中的词条T进行体现, 那么文本中出现的所有词条及其出现的次数就可以体现文档的特征</li>
<li>TF-IDF, 词条的重要性随着它在文件中出现的次数成正比增加, 但同时会随着它在语料库中出现的频率成反比下降;<ul>
<li>词条在文本中出现的次数越多, 表示该词条对该文本的重要性越高</li>
<li>词条在所有文本中出现的次数越少, 说明这个词条对文本的重要性越高</li>
<li>TF(词频)指某个词条在文本中出现的次数, 一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)</li>
<li>IDF(逆向文件频率)指一条词条重要性的度量, 一般计算方式为总文件数目除以包含该词语之文件的数目, 再将得到的商取对数得到. TF-IDF即TF*IDF</li>
</ul>
</li>
</ul>
<p><a href="https://blog.csdn.net/asialee_bird/article/details/81486700" target="_blank" rel="noopener">TF-IDF算法介绍及实现</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line">import math</span><br><span class="line">import operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loadDataSet():</span><br><span class="line">    dataset = [ [&apos;my&apos;, &apos;dog&apos;, &apos;has&apos;, &apos;flea&apos;, &apos;problems&apos;, &apos;help&apos;, &apos;please&apos;],    # 切分的词条</span><br><span class="line">                   [&apos;maybe&apos;, &apos;not&apos;, &apos;take&apos;, &apos;him&apos;, &apos;to&apos;, &apos;dog&apos;, &apos;park&apos;, &apos;stupid&apos;],</span><br><span class="line">                   [&apos;my&apos;, &apos;dalmation&apos;, &apos;is&apos;, &apos;so&apos;, &apos;cute&apos;, &apos;I&apos;, &apos;love&apos;, &apos;him&apos;],</span><br><span class="line">                   [&apos;stop&apos;, &apos;posting&apos;, &apos;stupid&apos;, &apos;worthless&apos;, &apos;garbage&apos;],</span><br><span class="line">                   [&apos;mr&apos;, &apos;licks&apos;, &apos;ate&apos;, &apos;my&apos;, &apos;steak&apos;, &apos;how&apos;, &apos;to&apos;, &apos;stop&apos;, &apos;him&apos;],</span><br><span class="line">                   [&apos;quit&apos;, &apos;buying&apos;, &apos;worthless&apos;, &apos;dog&apos;, &apos;food&apos;, &apos;stupid&apos;] ]</span><br><span class="line">    classVec = [0, 1, 0, 1, 0, 1]  # 类别标签向量，1代表好，0代表不好</span><br><span class="line">    return dataset, classVec</span><br><span class="line"></span><br><span class="line">def feature_select(list_words):</span><br><span class="line">    doc_frequency = defaultdict(int)</span><br><span class="line">    # print(type(doc_frequency))</span><br><span class="line">    for word_list in list_words:</span><br><span class="line">        for i in word_list:</span><br><span class="line">            # print(doc_frequency[i])</span><br><span class="line">            doc_frequency[i] += 1</span><br><span class="line">            # print(doc_frequency[i])</span><br><span class="line">            # print(&quot;===================&quot;)</span><br><span class="line"></span><br><span class="line">    print(doc_frequency, doc_frequency.values())</span><br><span class="line">    word_tf = &#123;&#125;</span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        word_tf[i] = doc_frequency[i] / sum(doc_frequency.values())</span><br><span class="line">  </span><br><span class="line">    doc_num = len(list_words)</span><br><span class="line">    word_idf = &#123;&#125;</span><br><span class="line">    word_doc = defaultdict(int)</span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        for j in list_words:</span><br><span class="line">            if i in j:</span><br><span class="line">                word_doc[i] += 1</span><br><span class="line">    </span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        word_idf[i] = math.log(doc_num/(word_doc[i] + 1))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    word_tf_idf = &#123;&#125;</span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        word_tf_idf[i] = word_tf[i] * word_idf[i]</span><br><span class="line"></span><br><span class="line">    # print(word_tf_idf)</span><br><span class="line">    dict_feature_select = sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)</span><br><span class="line">    return dict_feature_select</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&apos;__main__&apos;:</span><br><span class="line">    data_list,label_list=loadDataSet() #加载数据</span><br><span class="line">    # print(data_list, label_list)</span><br><span class="line">    features=feature_select(data_list) #所有词的TF-IDF值</span><br><span class="line">    print(features)</span><br><span class="line">    print(len(features))</span><br></pre></td></tr></table></figure>
<h2 id="回归算法"><a href="#回归算法" class="headerlink" title="回归算法"></a>回归算法</h2><p>回归算法是一种有监督算法, </p>
<ul>
<li>线性回归</li>
<li>Logistic回归</li>
<li>Softmax回归</li>
<li>梯度下降</li>
<li>特征抽取</li>
<li>线性回归案例</li>
</ul>
<h3 id="回归算法认知"><a href="#回归算法认知" class="headerlink" title="回归算法认知"></a>回归算法认知</h3><p>通过房屋面积预测房价, 如下图所示:<br><img src="/机器学习/WechatIMG18.png" width="300px"><br>如果影响房价的还有房间数量, 那么:<br><img src="/机器学习/WechatIMG19.png" width="300px"><br><img src="/机器学习/WechatIMG20.png" width="300px"><br>那么问题的解决依赖于如下的方程:</p>
<script type="math/tex; mode=display">\begin{align}
h_{\theta}(x) &= \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n\\ 
&=\theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n\\
&=\theta_{0}1 + \theta_1x_1 + \cdots + \theta_nx_n\\
&=\sum_{i=0}^{n}\theta_ix_i = \theta^Tx
\end{align}</script><p>最后就是要计算出$\theta$的值, 并选择最优的$\theta$值构成算法公式</p>
<p>那么它的似然函数为:</p>
<script type="math/tex; mode=display">y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}</script><ul>
<li>误差$\epsilon^{(i)}(i \le i \le n)$是独立同分布的, 服从均值为0, 方差为某定值$\sigma^2$的高斯分布(查看<a href="https://baike.baidu.com/item/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/829451?fr=aladdin" target="_blank" rel="noopener">中心极限定理</a>)</li>
<li>在实际中, 很多随机现象可以看作是众多因素的独立影响的综合反应, 往往服从正态分布</li>
</ul>
<h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><script type="math/tex; mode=display">y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}</script><script type="math/tex; mode=display">p(\epsilon^{(i)}) = \frac{1}{\sigma \sqrt{2\pi}} e^{(- \frac{(\epsilon^{(i)})^2}{2\sigma^2})}</script><script type="math/tex; mode=display">p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sigma \sqrt{2\pi}}exp^{(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})}</script><p>当给定某个样本值x的时候, 那么得到的实际值y的概率是多少</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>误差</th>
<th>概率</th>
<th>预测值是实际值的概率</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\epsilon \downarrow$</td>
<td>p $\rightarrow$ 1</td>
<td>预测值是实际值的概率越大</td>
</tr>
<tr>
<td>$\epsilon \uparrow$</td>
<td>p $\rightarrow$ 0</td>
<td>预测值是实际值的概率越大</td>
</tr>
</tbody>
</table>
</div>
<p>上述只是一个样本, 概率越等于1越好,  如果对m个样本的概率进行相乘得到联合概率(样本与样本之间独立的), 这样就得到一个似然函数</p>
<script type="math/tex; mode=display">\begin{align}
L(\theta) &= \prod_{i=1}^{m}p(y^{(i)} | x^{(i)}; \theta)\\
&=\prod_{i=1}^{m} \frac{1}{\sigma \sqrt{2\pi}}exp^{(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})}\\
\end{align}</script><h3 id="对数似然-目标函数及最小二乘法"><a href="#对数似然-目标函数及最小二乘法" class="headerlink" title="对数似然, 目标函数及最小二乘法"></a>对数似然, 目标函数及最小二乘法</h3><script type="math/tex; mode=display">\begin{align}
l(\theta) &= \log L(\theta)\\
&= \log\prod_{i=1}^{m} \frac{1}{\sigma \sqrt{2\pi}}exp^{(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})}\\
&= m\log \frac{1}{\sigma \sqrt{2\pi}} - \frac{1}{2\sigma^2} \sum_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2\\
\end{align}</script><script type="math/tex; mode=display">loss(y_i, \widehat{y_i}) = J(\theta) = \frac{1}{2}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><h3 id="theta-的求解过程"><a href="#theta-的求解过程" class="headerlink" title="$\theta$的求解过程"></a>$\theta$的求解过程</h3><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><script type="math/tex; mode=display">\theta_0x_0^{(1)} + \theta_1x_1^{(1)} + ... + \theta_nx_n^{(1)} = \widehat{y^{(1)}}</script><script type="math/tex; mode=display">\theta_0x_0^{(2)} + \theta_1x_1^{(2)} + ... + \theta_nx_n^{(2)} = \widehat{y^{(2)}}</script><script type="math/tex; mode=display">\widehat{y} = 

\begin{bmatrix}
{\widehat{y^{(1)}}}\\
{\widehat{y^{(2)}}}\\
{\vdots}\\
{\widehat{y^{(m)}}}\\
\end{bmatrix}
=
\begin{bmatrix}
{x_0^{(1)}}&{x_1^{(1)}}&{\cdots}&{x_n^{(1)}}\\
{x_0^{(2)}}&{x_1^{(2)}}&{\cdots}&{x_n^{(2)}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{x_0^{(m)}}&{x_1^{(m)}}&{\cdots}&{x_n^{(m)}}\\
\end{bmatrix}

\begin{bmatrix}
{\theta_0}\\
{\theta_1}\\
{\vdots}\\
{\theta_m}\\
\end{bmatrix}
= 
X\theta</script><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} = \frac{1}{2}(X\theta - Y)^T (X\theta - Y) \rightarrow min_{\theta}J(\theta)</script><p><strong>即让$J(\theta)$最小的时候, $\theta$的取值</strong>, 这就是最小二乘法</p>
<script type="math/tex; mode=display">\begin{align}
\nabla_{\theta}J(\theta) &= \nabla_{\theta} \frac{1}{2}(X\theta - Y)^T (X\theta - Y)\\
&= \nabla_{\theta} \frac{1}{2}(\theta^TX^T - Y^T)(X\theta - Y)\\
&= \nabla_{\theta} \frac{1}{2}(\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta + Y^TY)\\
&= \frac{1}{2}(X^TX\theta + (\theta^TX^TX)^T - X^TY - (Y^TX)^T)\\
&= \frac{1}{2}(2X^TX\theta -2X^TY )\\
&= X^TX\theta - X^TY\\
\end{align}</script><script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><h3 id="最小二乘法的参数最优解"><a href="#最小二乘法的参数最优解" class="headerlink" title="最小二乘法的参数最优解"></a>最小二乘法的参数最优解</h3><ul>
<li><p>参数解析式</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><p>X为特征矩阵, Y为目标属性, 如果一个矩阵可逆, 那么它的行列式大于0才可以</p>
</li>
<li><p>最小二乘法的使用要求矩阵$X^TX$是可逆的; 为了防止不可逆或者过拟合问题的存在, 可以增加额外的数据影响, 导致最终的矩阵是可逆的</p>
<script type="math/tex; mode=display">\theta = (X^TX + \lambda I)^{-1}X^TY</script></li>
</ul>
<p>证明如下:</p>
<script type="math/tex; mode=display">\because (A)^T(A) \ge 0</script><script type="math/tex; mode=display">\therefore (x\mu)^T(x\mu) \ge 0, (向量\mu \ne 0)</script><script type="math/tex; mode=display">\therefore \mu^Tx^Tx\mu \ge 0</script><p>而$x^Tx$是半正定矩阵, 要想$\mu^TA\mu &gt; 0$<br>而$\mu^Tx^Tx\mu + \mu^T\mu &gt; 0$</p>
<script type="math/tex; mode=display">\therefore \mu^T(x^Tx + \lambda I)\mu > 0</script><script type="math/tex; mode=display">\therefore X^TX + \lambda I是正定矩阵, 正定矩阵一定可逆</script><p>那么中间的一定是大于0的, 就一定是正定矩阵，而正定矩阵是可逆的</p>
<p>另外一种理解方式</p>
<script type="math/tex; mode=display">对于X\theta=Y, 要想求\theta, 必须要消掉X, 要消掉X就会用到X的逆X^{-1}</script><script type="math/tex; mode=display">要想求X^{-1}, 一定是方阵才行, \therefore X^TX\theta = X^TY</script><script type="math/tex; mode=display">X^TX是方阵, 然后再整体求逆, \therefore (X^TX)^{-1}X^TX\theta = (X^TX)^{-1}X^TY</script><h3 id="目标损失函数-loss-cost-function"><a href="#目标损失函数-loss-cost-function" class="headerlink" title="目标损失函数(loss/cost function)"></a>目标损失函数(loss/cost function)</h3><ul>
<li>0-1损失函数 $J(\theta) = \begin{cases}1, Y \ne f(X)\\ 0, Y = f(X) \end{cases}$</li>
<li>感知损失函数 $J(\theta) = \begin{cases}1, \vert Y - f(X) \vert &gt; t\\ 0, \vert Y - f(X) \vert \leq t\end{cases}$</li>
<li>平方和损失函数 <script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}</script></li>
<li>绝对值损失函数<script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m}{\vert h_{\theta}(x^{(i)}) - y^{(i)} \vert}</script></li>
<li>对数损失函数<script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m}{(y^{(i)}\log h_{\theta}(x^{(i)}))}</script></li>
</ul>
<p><strong>0-1损失函数在分类中用的比较多</strong><br><strong>感知损失函数用的也比较多, 稍微一变就是SVM</strong><br><strong>对数损失函数会用在逻辑回归中</strong></p>
<h3 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h3><p><img src="/机器学习/WechatIMG24.png" width="300px"></p>
<p>误差分为期望误差($L(f)$)和方差误差($\Omega(f)$)<br><a href="https://baijiahao.baidu.com/s?id=1601092478839269810&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">偏差误差和方差之间的区别和联系</a></p>
<p>05.avi_30:00</p>
<p>多项式扩展就是将特征与特征之间进行融合，从而形成新的特征的一个过程; 从数学上来讲就是将低维度空间上的点映射到高维度空间中, 本身属于特征工程的一种操作.</p>
<p>如果模型在训练集上效果非常好, 而在训练集上效果不好, 那么认为这个时候存在过拟合的情况; 多项式扩展的时候, 如果指定的阶数比较大, 那么有可能导致过拟合, 从线性回归来讲, 认为训练出来的模型参数值越大, 就表示越存在过拟合的情况.</p>
<h3 id="线性回归的过拟合"><a href="#线性回归的过拟合" class="headerlink" title="线性回归的过拟合"></a>线性回归的过拟合</h3><ul>
<li>目标函数  $J(\theta) = \displaystyle \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}$</li>
<li>为了防止数据过拟合, 即$\theta$值的样本空间不能过大或者过小, 可以在目标函数增加一个平方和损失:<br>$J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda \displaystyle\sum_{i=1}^{n}\theta_{j}^2$, 前面的一项体现的是期望误差, 后面一项体现的是模型的复杂度误差</li>
<li><p>正则项(norm): $\lambda \displaystyle\sum_{i=1}^{n}\theta_{j}^2$, 此正则项叫做L2-norm, 此外还有L1-norm: $\lambda \displaystyle\sum_{i=1}^{n}\vert \theta_{j} \vert$</p>
<ul>
<li>L2-norm(Ridge回归, 岭回归): $J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda \displaystyle\sum_{i=1}^{n}\theta_{j}^2, \lambda &gt; 0$</li>
<li>L1-norm(LASSO回归): $J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda \displaystyle\sum_{i=1}^{n}\vert \theta_{j} \vert, \lambda &gt; 0$, LASSO回归中间很容易出现稀疏解, 即很多0的情况; L1-norm的求解方式为坐标轴下降法</li>
</ul>
</li>
</ul>
<p>为了解决过拟合问题, 可以选择在损失函数中加入惩罚项(对于系数过大的惩罚), 分为L1-norm和L2-norm</p>
<h3 id="模型效果判断"><a href="#模型效果判断" class="headerlink" title="模型效果判断"></a>模型效果判断</h3><ul>
<li>MSE, 误差平方和, 越趋近于0表示模型越拟合训练数据</li>
<li>RMSE, MSE的平方根, 作用同MSE</li>
<li>$R^2$, 取值范围$(-\infty, 1]$, 值越大表示模型越拟合训练数据; 最优解是1, 当模型预测为随机值的时候, 有可能为负; 若预测值恒为样本值期望, $R^2$为0</li>
<li>TSS, 总平方和, 表示样本之间的差异情况, 是伪方差的m倍</li>
<li>RSS, 残差平方和, 表示预测值和样本值之间的差异, 是MSE的m倍</li>
</ul>
<script type="math/tex; mode=display">MSE = \frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2</script><script type="math/tex; mode=display">RMSE = \sqrt{MSE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2}</script><script type="math/tex; mode=display">R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{m}(y_i - \hat{y_i})^2}{\sum_{i=1}^{m}{(y_i - \overline{y})^2}}</script><script type="math/tex; mode=display">\overline{y} = \frac{1}{m}\sum_{i=1}^my_i</script><h3 id="Elastic-Net"><a href="#Elastic-Net" class="headerlink" title="Elastic Net"></a>Elastic Net</h3><p>同时使用L1正则和L2正则的线性回归模型称为Elastic Net算法(弹性网络算法)</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda(p\sum_{j=1}^{n}{\vert \theta_{j} \vert} + (1-p)\sum_{j=1}^{n}{\theta^2}) \begin{cases}
\lambda > 0\\
p \in [0, 1]
\end{cases}</script><p>该公式中p表示选中L1-norm的概率是多少, 选中L2-norm的概率是多少</p>
<h3 id="机器学习调参"><a href="#机器学习调参" class="headerlink" title="机器学习调参"></a>机器学习调参</h3><ul>
<li>实际中对于各种算法模型, 需要获取$\theta$, $\lambda$, $p$的值, $\theta$的求解其实就是算法模型的求解, 一般不需要开发人员的参与(算法已经实现), 主要需要求解的是$\lambda$和$p$的值(超参), 此过程叫做<strong>调参</strong></li>
<li>交叉验证, 将训练数据分为多份, 其中一份进行数据验证并获取最优的超参$\lambda$和$p$, 比如十指交叉验证和五指交叉验证(scikit-learn中默认)</li>
</ul>
<p>注: $y_i$为样本值, $\hat{y_i}$为预测值, $\overline{y_i}$为均值</p>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><ul>
<li>目标函数$\theta$求解, $J(\theta) = \displaystyle \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_{\theta}{(x^{(i)})} - y^{(i)})^2}$</li>
<li>初始化$\theta$(随机初始化, 可以初始化为0)</li>
<li>沿着负梯度方向迭代, 更新后的$\theta$使$J(\theta)$更小, $\theta = \theta - \alpha \cdot \displaystyle \frac{\partial J(\theta)}{\partial \theta}$, $\alpha$为学习率或者步长</li>
</ul>
<p>求解如下:</p>
<script type="math/tex; mode=display">\begin{align}\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j}\displaystyle \frac{1}{2}{(h_{\theta}{(x)} - y)^2}\\
&= \frac{\partial}{\partial \theta_j} \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_{\theta}{(x^{(i)})} - y^{(i)})^2}\\
&= \frac{1}{2} \cdot 2 (h_{\theta}(x) - y) \frac{\partial}{\partial \theta_j} (h_{\theta}{(x)} - y)\\
&= (h_{\theta}(x) - y) \frac{\partial}{\partial \theta_j} \displaystyle \sum_{i=1}^{m}(h_{\theta}{(x^{(i)})} - y^{(i)})\\
&= (h_{\theta}(x) - y) \frac{\partial}{\partial \theta_j} \displaystyle \sum_{i=1}^{m}(\theta_i x_i - y^{(i)})\\
&= (h_{\theta}(x) - y) x_j\\
\end{align}</script><h4 id="批量梯度下降法-BGD"><a href="#批量梯度下降法-BGD" class="headerlink" title="批量梯度下降法(BGD)"></a>批量梯度下降法(BGD)</h4><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} J(\theta) = (h_{\theta}(x) - y) x_j</script><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_j} = \displaystyle \sum_{i=1}^{m}(h_{\theta}{(x^{(i)})} - y^{(i)})x_j^{(i)}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha \displaystyle \sum_{i=1}^{m}{(h_{\theta}{(x^{(i)})} - y^{(i)})x_j^{(i)}}</script><h4 id="随机梯度下降法-SGD"><a href="#随机梯度下降法-SGD" class="headerlink" title="随机梯度下降法(SGD)"></a>随机梯度下降法(SGD)</h4><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} J(\theta) = (h_{\theta}(x) - y) x_j</script><script type="math/tex; mode=display">for i=1 to \space m, \theta_j = \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}</script><h4 id="线性回归的扩展"><a href="#线性回归的扩展" class="headerlink" title="线性回归的扩展"></a>线性回归的扩展</h4><ul>
<li>线性回归针对的是$\theta$而言, 对于样本本身, 样本可以是非线性的<br><img src="/机器学习/WechatIMG27.png" width="300px"></li>
</ul>
<h3 id="局部加权回归"><a href="#局部加权回归" class="headerlink" title="局部加权回归"></a>局部加权回归</h3><p><img src="/机器学习/WechatIMG25.png" width="300px"></p>
<p>普通线性回归损失函数为:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><p>局部加权回归损失函数为:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m} w^{(i)}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><ul>
<li>$w^{(i)}$是权重, 根据预测的点与数据集中的点的距离来为数据集中的点赋权值, 当某点离要预测的点越远, 其权重越小, 否则越大. 常用值选择公式为:<script type="math/tex; mode=display">w^{(i)} = exp(- \frac{(x^{(i)} - \bar{x})^2}{2k^2})</script>该函数为指数衰减函数, 其中$k$为波长参数, 它控制了权值随距离下降的速率</li>
</ul>
<p><strong>一般实际工作中, 当线性模型的参数接近0的时候, 我们认为当前参数对应的那个特征属性在模型判断中没有太大的决策信息, 所以对于这样的属性我们可以删除; 一般情况下, 如果是手动删除的话, 选择小于$1e-4$的特征属性</strong></p>
<h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p>Logistic/Sigmoid函数, $\displaystyle p = h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$, p是个概率值</p>
<script type="math/tex; mode=display">g(z) = \frac{1}{1 + e^{-z}}</script><script type="math/tex; mode=display">\begin{align}
g^\prime(z) = &(\frac{1}{1 + e^{-z}})^\prime = \frac{e^{-z}}{(1 + e^{-z})^2}\\
&=\frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{(1 + e^{-z})} = \frac{1}{1 + e^{-z}} \cdot (1 - \frac{1}{1 + e^{-z}})\\
&=g(z) \cdot (1 - g(z))\\
\end{align}</script><script type="math/tex; mode=display">y = \begin{cases}0\\
1\end{cases}</script><script type="math/tex; mode=display">\hat{y} = \begin{cases}1, P(\hat{y} = 1) > p\\
0, P(\hat{y} = 0) > p\end{cases}</script><p><img src="/机器学习/WechatIMG26.png" width="300px"></p>
<h3 id="Logistic回归及似然函数"><a href="#Logistic回归及似然函数" class="headerlink" title="Logistic回归及似然函数"></a>Logistic回归及似然函数</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y=1</th>
<th>y=0</th>
</tr>
</thead>
<tbody>
<tr>
<td>$p(y \mid x)$</td>
<td>$\theta$</td>
<td>$1 - \theta$</td>
</tr>
</tbody>
</table>
</div>
<p>假设: </p>
<script type="math/tex; mode=display">P(y=1|x;\theta) = h_\theta(x)</script><script type="math/tex; mode=display">P(y=0|x;\theta) = 1 - h_\theta(x)</script><script type="math/tex; mode=display">P(y|x;\theta) = h_\theta(x) \ast (1 - h_\theta(x)) = (h_\theta(x))^y(1 - h_\theta(x))^{(1-y)}</script><p>似然函数:</p>
<script type="math/tex; mode=display">\begin{align}L(\theta) &= p(\overrightarrow y \mid X;\theta) = \displaystyle \prod_{i=1}^{m}{p(y^{(i)}|x^{(i)}; \theta)}\\
&=\displaystyle \prod_{i=1}^{m} (h_\theta(x^{(i)}))^y{^{(i)}}(1 - h_\theta(x^{(i)}))^{(1-y^{(i)})}\\
\end{align}</script><p>最大似然/极大似然函数的随机梯度<br>对数似然函数为:</p>
<script type="math/tex; mode=display">l(\theta) = logL(\theta) = \displaystyle \sum_{i=1}^{m}(y^{(i)}log h_\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)})))</script><script type="math/tex; mode=display">\begin{align}\frac{\partial l(\theta)}{\partial \theta_j} &= \displaystyle \sum_{i=1}^{m}(\frac{y^{(i)}}{h_\theta(x^{(i)})} - \frac{1 - y^{(i)}}{1 - h_\theta(x^{(i)}) }) \cdot \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j}\\
&= \displaystyle \sum_{i=1}^{m}(\frac{y^{(i)}}{g(\theta^Tx^{(i)})} - \frac{1 - y^{(i)}}{1 - g(\theta^Tx^{(i)}) }) \cdot \frac{\partial g(\theta^Tx^{(i)})}{\partial \theta_j} \\
&= \displaystyle \sum_{i=1}^{m}(\frac{y^{(i)}}{g(\theta^Tx^{(i)})} - \frac{1 - y^{(i)}}{1 - g(\theta^Tx^{(i)}) }) \cdot g(\theta^Tx^{(i)})(1 - g(\theta^Tx^{(i)})) \cdot \frac{\partial \theta^Tx^{(i)}}{\partial \theta_j} \\
&=\displaystyle \sum_{i=1}^{m}(y^{(i)}(1 - g(\theta^Tx^{(i)}) - g(\theta^Tx^{(i)})(1 - y^{(i)})) \cdot \frac{\partial \theta^Tx^{(i)}}{\partial \theta_j}\\
&=\displaystyle \sum_{i=1}^{m}(y^{(i)}(1 - g(\theta^Tx^{(i)}) - g(\theta^Tx^{(i)})(1 - y^{(i)})) \cdot x_{j}^{(i)}\\
&=\displaystyle \sum_{i=1}^{m}(y^{(i)} - g(\theta^Tx^{(i)})) \cdot x_{j}^{(i)}\\
\end{align}</script><h3 id="Logistic回归-theta-参数求解"><a href="#Logistic回归-theta-参数求解" class="headerlink" title="Logistic回归$\theta$参数求解"></a>Logistic回归$\theta$参数求解</h3><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha \displaystyle \sum_{i=1}^{m}(y^{(i)} - g(\theta^Tx^{(i)})) \cdot x_{j}^{(i)}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha (y^{(i)} - g(\theta^Tx^{(i)})) \cdot x_{j}^{(i)}</script><h3 id="极大似然估计与Logistic回归损失函数"><a href="#极大似然估计与Logistic回归损失函数" class="headerlink" title="极大似然估计与Logistic回归损失函数"></a>极大似然估计与Logistic回归损失函数</h3><script type="math/tex; mode=display">L(\theta) = \displaystyle \prod_{i=1}^{m}{p(y^{(i)}|x^{(i)}; \theta)} = \displaystyle \prod_{i=1}^{m}{p_{i}^{y^{(i)}}(1-p_i)^{(1-y^{(i)})}}, \quad p_i = h_\theta(x^{(i)}) = \frac{1}{1 + e^{\displaystyle - \theta^{T} x^{(i)}}}</script><script type="math/tex; mode=display">l(\theta) = lnL(\theta) = \displaystyle \sum_{i=1}^{m} ln[p_{i}^{y^{(i)}}(1-p_i)^{(1-y^{(i)})}]</script><script type="math/tex; mode=display">\begin{align}loss &= - l(\theta)\\
&= - \displaystyle \sum_{i=1}^{m}[y^{(i)}ln(p_i) + (1-y^{(i)})ln(1-p_i)] \\
&= \displaystyle \sum_{i=1}^{m}[- y^{(i)}ln(h_\theta(x^{(i)})) - (1-y^{(i)})ln(1-h_\theta(x^{(i)}))] \\
\end{align}</script><h3 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h3><ul>
<li>softmax回归是logistic回归的一般化, 适用于K分类问题, 第K类的参数为向量$\theta_k$, 组成的二维矩阵为$\theta_{k \ast n}$</li>
<li>softmax函数的本质是将一个K维的任意实数向量压缩(映射)成另一个K维的实数向量, 其中向量中的每个元素取值都介于(0, 1)之间</li>
<li>softmax回归概率函数为:<script type="math/tex; mode=display">p(y=k|x;\theta) = \displaystyle \frac{e^{\theta_{k}^{T}x}}{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x}}}, \quad k = 1, 2, ..., K</script></li>
</ul>
<h3 id="softmax算法原理"><a href="#softmax算法原理" class="headerlink" title="softmax算法原理"></a>softmax算法原理</h3><script type="math/tex; mode=display">p(y=k|x;\theta) = \frac{e^{\theta_{k}^{T}x}}{\displaystyle \sum_{l=1}^{K}{e^{\theta_{l}^{T}x}}}, \quad k = 1, 2, ..., K</script><script type="math/tex; mode=display">h_\theta(x) = 
\begin{bmatrix}
p(y^{(i)}=1|x^{(i)}; \theta)\\
p(y^{(i)}=2|x^{(i)}; \theta)\\
{\vdots}\\
p(y^{(i)}=k|x^{(i)}; \theta)\\
\end{bmatrix} = \frac{1}{\displaystyle \sum_{j=1}^{k}{e^{\theta_{j}^{T}x^{(i)}}}}
\begin{bmatrix}
e^{\theta_{1}^{T}x}\\
e^{\theta_{2}^{T}x}\\
{\vdots}\\
e^{\theta_{k}^{T}x}\\
\end{bmatrix} \Rightarrow \theta = \begin{bmatrix}
{\theta_{11}}&{\theta_{12}}&{\cdots}&{\theta_{1n}}\\
{\theta_{21}}&{\theta_{22}}&{\cdots}&{\theta_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{\theta_{k1}}&{\theta_{k2}}&{\cdots}&{\theta_{kn}}\\
\end{bmatrix}</script><h3 id="softmax算法损失函数"><a href="#softmax算法损失函数" class="headerlink" title="softmax算法损失函数"></a>softmax算法损失函数</h3><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^{m} \sum_{j=1}^{k} I(y^{(i)}=j) ln(\displaystyle \frac{e^{\theta_{j}^{T}x^{(i)}}}{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}}}}), \quad I(y^{(i)} = j) = \begin{cases}
1, \quad y^{(i)}=j\\
0, \quad y^{(i)} \ne j\\
\end{cases}</script><h3 id="softmax算法梯度下降法求解"><a href="#softmax算法梯度下降法求解" class="headerlink" title="softmax算法梯度下降法求解"></a>softmax算法梯度下降法求解</h3><script type="math/tex; mode=display">\begin{align} \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j}[- I(y^{(i)}=j) ln(\displaystyle \frac{\displaystyle e^{\theta_{j}^{T}x^{(i)}}}{ \displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}} } })]\\
&= \frac{\partial}{\partial \theta_j} [- I(y^{(i)}=j) (\theta_{j}^{T}x^{(i)} - ln(\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}} }))]\\
&= - I(y^{(i)}=j) (x^{(i)} - \frac{ e^{\theta_{j}^{T}x^{(i)}} \cdot x^{(i)} }{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}}}})\\
&= - I(y^{(i)}=j) (1 - \frac{ e^{\theta_{j}^{T}x^{(i)}} }{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}}}}) x^{(i)}   \\
\end{align}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha \displaystyle \sum_{i=1}^{m} I(y^{(i)}=j)(1 - p(y^{(i)}=j|x^{(i)};\theta))x^{(i)}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha I(y^{(i)}=j)(1 - p(y^{(i)}=j|x^{(i)};\theta))x^{(i)}</script><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><ul>
<li>线性模型一般用于回归问题, Logistic和 softmax模型一般用于分类问题</li>
<li>求$\theta$的主要方式是梯度下降法, 梯度下降法是参数优化的重要手段, 主要是SGD, 适用于在线学习以及跳出局部极小值</li>
<li>Logistic和softmax回归是实践中解决分类问题的最重要的方法</li>
<li>广义线性模型对样本要求不必要服从正态分布, 只需要服从指数分布簇(二项分布, 泊松分布, 伯努利分布, 指数分布即可); 广义线性模型的自变量可以是连续的也可以是离散的</li>
</ul>
<h3 id="鸢尾花数据"><a href="#鸢尾花数据" class="headerlink" title="鸢尾花数据"></a>鸢尾花数据</h3><p><a href="https://blog.csdn.net/heuguangxu/article/details/80426437" target="_blank" rel="noopener">参考案例</a></p>
<h4 id="ravel和flatten"><a href="#ravel和flatten" class="headerlink" title="ravel和flatten"></a>ravel和flatten</h4><p>ravel是将多维数据转换为一维数据</p>
<h3 id="线性回归-多项式扩展"><a href="#线性回归-多项式扩展" class="headerlink" title="线性回归+多项式扩展"></a>线性回归+多项式扩展</h3><ul>
<li>如果数据本身不是线性关系, 那么直接使用线性回归模型效果不会体太好, 存在欠拟合</li>
<li>数据在低维空间中不是线性关系, 但是如果将数据映射到高维空间的时候, 数据就有可能变成线性关系, 从而就可以使用线性回归</li>
<li>如果映射的维度特别高, 那么数据就会完全变成线性的, 从而训练出来的模型会非常的契合训练数据; 但是实际上的数据可能会和训练数据存在一定的差距, 从而可能会导致模型在其他数据集上的效果不佳, 可能存在过拟合</li>
</ul>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><ul>
<li>表现形式: 模型在训练集上效果非常好, 但是在测试集上效果不好</li>
<li>产生原因: 进入算法模型训练的这个数据集特别契合算法模型, 导致训练出来的模型非常完美, 但是实际的数据一定存在和训练集数据不一致的地方, 这个不一致就导致模型在其他数据集上效果不佳</li>
<li>解决方案: L1-norm和L2-norm</li>
</ul>
<h3 id="Logistic回归-1"><a href="#Logistic回归-1" class="headerlink" title="Logistic回归"></a>Logistic回归</h3><ul>
<li>本质上是一个二分类算法, 计算的是样本X属于一个类别的概率p, 样本属于另外一个类别的概率为1-p</li>
<li>最终认为样本X属于概率最大的那一个类别 <h3 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h3></li>
<li>与Logistic回归的区别在于, softmax是一个多分类算法, 需要计算样本属于某一个类别的概率, 最终认为样本属于概率最大的那一个类别</li>
<li>softmax会为每个类别训练一个参数$\theta$向量, 所以在softmax中需要求解的参数其实是一个由k个向量组成的$\theta$矩阵</li>
</ul>
<h3 id="持久化-管道-模型效果评估-交叉验证"><a href="#持久化-管道-模型效果评估-交叉验证" class="headerlink" title="持久化, 管道, 模型效果评估, 交叉验证"></a>持久化, 管道, 模型效果评估, 交叉验证</h3><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><ul>
<li>理论/原理: 认为”物以类聚, 人以群分”, 相同/近似样本在样本空间中是比较接近的, 所以可以使用和当前样本比较接近的的其他样本的目标属性作为当前样本的预测值</li>
<li>预测规则:<ul>
<li>分类: 多数投票或者加权多数投票</li>
<li>回归: 平均值或者加权平均值</li>
<li>权重一般选择是与距离成反比</li>
</ul>
</li>
<li>相似度度量<br>需要找相似的样本, 认为样本的特征向量在空间中的点之间的距离体现了样本之间的相似性, 越近的点越相似, 一般使用欧几里得距离</li>
<li>寻找最近邻的样本<ul>
<li>暴力的方式, 计算所有样本到当前样本的距离, 然后再获取最近的k个样本</li>
<li>KD-Tree方式, 通过构建KD-Tree, 减少计算量</li>
</ul>
</li>
</ul>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>考虑: 虽然抽样数据在一定程度上体现了样本数据的特性, 所以我们用样本数据模型来预测它的全体数据. 但是数据与数据之间是存在一定的差异性的, 那么可以认为在当前数据集的模型有可能会出现不拟合生产环境中数据的情况 -&gt; 过拟合;在决策树中, 进行划分特征属性选择的时候, 如果选择最优, 表示这个划分在当前数据集上一定是最优的, 但是不一定在全体数据集上最优; 在随机森林中, 如果每个决策树都是选择最优的进行划分的话, 就会导致所有子模型(内部的决策树)很大程度/概率上会使用相同的划分属性进行数据的划分, 就会特别容易导致过拟合, 所以在随机森林中, 选择划分属性的时候一般使用随机的方式选择</p>
<p>如果涉及到权重的话, 一般都是将错误率/正确率/距离/MSE/MAE等指标进行转换得到的</p>
<h3 id="随机森林-Random-Forest-推广算法"><a href="#随机森林-Random-Forest-推广算法" class="headerlink" title="随机森林(Random Forest)推广算法"></a>随机森林(Random Forest)推广算法</h3><p>RF算法在实际应用中具有比较好的特性, 应用也比较广泛, 主要应用在分类, 回归, 特征转换, 异常点检测等, 常见的RF变种算法有:</p>
<ul>
<li>Extra Tree</li>
<li>Totally Random Trees Embedding(TRTE)</li>
<li>Isolation Forest</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/求解二元函数最小值/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/求解二元函数最小值/" itemprop="url">求解二元函数最小值</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-16T15:46:21+08:00">
                2019-10-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f_2d(x, y):</span><br><span class="line">    return x ** 2 + 3 * x + y ** 2 + 8 * y + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def df_2d(x, y):</span><br><span class="line">    return 2 * x + 3, 2 * y + 8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, y = 4, 4</span><br><span class="line">learning_rate = 0.01</span><br><span class="line"></span><br><span class="line">for itr in range(200):</span><br><span class="line">    v_x, v_y = df_2d(x, y)</span><br><span class="line">    x, y = x - learning_rate * v_x, y - learning_rate * v_y</span><br><span class="line">    print(x, y, f_2d(x, y))</span><br><span class="line"></span><br><span class="line">x = np.linspace(-10, 10)</span><br><span class="line">y = np.linspace(-10, 10)</span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.gca(projection=&apos;3d&apos;)</span><br><span class="line">surf = ax.plot_surface(X, Y, f_2d(X, Y))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/求解二元函数最小值/微信图片_20191016154735.png" width="500px"><br><img src="/求解二元函数最小值/微信图片_20191016154739.png" width="500px"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/数学概念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/数学概念/" itemprop="url">数学概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-16T11:22:13+08:00">
                2019-10-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><p>常见函数</p>
<ul>
<li>导数, <font color="red"><strong>梯度</strong></font></li>
<li>Taylor展开, <font color="red"><strong>Taylor公式</strong></font></li>
<li>概率论, <font color="red"><strong>联合概率</strong></font>, <font color="red"><strong>条件概率</strong></font>, <font color="red"><strong>全概率公式</strong></font>, <font color="red"><strong>贝叶斯公式</strong></font></li>
<li>期望, <font color="red"><strong>方差</strong></font>, <font color="red"><strong>协方差</strong></font></li>
<li>大数定理, <font color="red"><strong>中心极限定理</strong></font></li>
<li>估计法, <font color="red"><strong>最大似然估计(MLE)</strong></font></li>
<li><font color="red"><strong>向量</strong></font>, <font color="red"><strong>矩阵运算</strong></font>, 矩阵求导</li>
<li><font color="red"><strong>SVD</strong></font>, QR分解</li>
</ul>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度是一个向量, 表示某一个函数在该点处的<strong>方向导数</strong>沿着该方向取的最大值, 即函数在该点处沿着该方向变化最快, 变化率最大(即该梯度向量的模); 当函数为一维函数时, 梯度其实就是导数</p>
<script type="math/tex; mode=display">\nabla{f(x_1, x_2)} = (\frac{\partial{f(x_1, x_2)}}{x_1}, \frac{\partial{f(x_1, x_2)}}{x_2})</script><h3 id="Taylor公式"><a href="#Taylor公式" class="headerlink" title="Taylor公式"></a>Taylor公式</h3><ul>
<li>Taylor公式是用一个函数在某点的信息描述其附近取值的公式. 如果函数足够平滑, 在已知函数在某一点的各阶导数值的情况下, Taylor公式可以利用这些导数值来做系数构建一个多项式近似函数在这一点的领域中的值.</li>
<li>若函数$f(x)$在包含$x_0$的某个闭区间$[a, b]$上具有$n$阶函数, 且在开区间$(a, b)$上具有$n+1$阶函数, 则对闭区间$[a, b]$上任意一点$x$, 有Taylor公式如下:<script type="math/tex; mode=display">f(x) = \frac{f(x_0)}{0!} + \frac{f^{'}(x_0)}{1!}(x-x_0) + \frac{f^{''}(x_0)}{2!}(x-x_0)^2 + ... + \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)</script></li>
</ul>
<p>$f^{(n)}(x)$表示$f(x)$的$n$阶导数, $R_n(x)$是Taylor公式的余项, 是$(x-x_0)^n$的高阶无穷小</p>
<script type="math/tex; mode=display">sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} + ... + (-1)^{m-1}\frac{x^{2m-1}}{(2m-1)!} + R_{2m-1}(x)</script><p>计算近似值, 并估计误差值</p>
<script type="math/tex; mode=display">e = \lim_{x \rightarrow \infty}(1 + {\frac{1}{n})}^n</script><script type="math/tex; mode=display">y = e^x \Rightarrow y^{'} = y = e^x</script><script type="math/tex; mode=display">\begin{align}
e^x &\approx \sum_{k=0}^{n}{\frac{e^x_0}{k!}}(x-x_0)^k\\
&\stackrel{令x_0=0}{\Rightarrow} e^x \approx 1 + x + \frac{x^2}{2!} + ... + \frac{x_n}{n!}\\
&\stackrel{令x=1}{\Rightarrow} e \approx 1 + 1 + \frac{1}{2!} + \frac{1}{3!} + ... + \frac{1}{n!}\\
&\stackrel{令x=10}{\Rightarrow} e \approx 2.7182815\\
\end{align}</script><script type="math/tex; mode=display">\delta = \vert R_{10}\vert = \frac{1}{11!} + \frac{1}{12!} + ... = \frac{1}{11!}(1+\frac{1}{12}+\frac{1}{12*13}+...) \lt \frac{1}{11!}(1+\frac{1}{12}+\frac{1}{12^2}+...) = \frac{12}{11*11!} = 2.73 * 10^{-8}</script><h3 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h3><p>两个事件同时发生的概率, 计作: $P(AB)$, $P(A, B)$, 或者$P(A \cap B)$, 即为”事件A和事件B同时发生的概率”<br><img src="/数学概念/WechatIMG8.png" width="400px"></p>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>事件A在另外一件事情B已经发生的条件下发生的概率叫做条件概率, 表示为P(A|B), 即为”在B条件下A发生的概率”<br>一般情况下, P(A|B) &ne; P(A)<br>条件概率有如下三个特性</p>
<ul>
<li>非负性</li>
<li>可列性</li>
<li>可加性</li>
</ul>
<p><img src="/数学概念/WechatIMG7.png" width="400px"></p>
<script type="math/tex; mode=display">P(A_1|B) = \frac{P(A_1, B)}{P(B)}</script><p>如果将条件概率由两个事件推广到任意有穷多个事件时, 可以得到如下公式, 假设$A_1, A_2, …, A_n$为任意n个任意事件(n &ge; 2)), 而且$P(A_1A_2…A_n) &gt; 0$, 则</p>
<script type="math/tex; mode=display">P(A_1B) = P(A_1|B)P(B)</script><script type="math/tex; mode=display">P(A_1A_2) = P(A_1|A_2)P(A_2) = P(A_2)P(A_1|A_2)</script><p>即为, 事件$A_1$和事件$A_2$同时发生的概率等于事件$A_2$已经发生的条件下$A_1$发生的概率与事件$A_2$发生的概率的乘积</p>
<script type="math/tex; mode=display">P(A_2A_1) = P(A_2|A_1)P(A_1) = P(A_1)P(A_2|A_1)</script><p>即为, 事件$A_2$和事件$A_1$同时发生的概率等于事件$A_1$已经发生的条件下$A_2$发生的概率与事件$A_1$发生的概率的乘积</p>
<script type="math/tex; mode=display">P(A_1A_2) = P(A_2, A_1)</script><p>即为, 事件$A_1$和事件$A_2$同时发生的概率</p>
<script type="math/tex; mode=display">P(A_1|A_2)P(A_2) =  P(A_2|A_1)P(A_1)</script><p>即为, 事件$A_2$已经发生的条件下$A_1$发生的概率与事件$A_2$发生的概率的乘积 <strong>等于</strong> 事件$A_1$已经发生的条件下$A_2$发生的概率与事件$A_1$发生的概率的乘积</p>
<script type="math/tex; mode=display">P(A_1A_2A_3) = P(A_1A_2)P(A_3|(A_1A_2)) = P(A_1)P(A_2|A_1)P(A_3|(A_1A_2))</script><script type="math/tex; mode=display">P(A_1A_2...A_n) = P(A_1)P(A_2|A_1)...P(A_n|(A_1A_2...A_n))</script><h3 id="全概率"><a href="#全概率" class="headerlink" title="全概率"></a>全概率</h3><p>样本空间$\Omega$有一组事件$A_1, A_2, …, A_n$, 如果事件组满足以下两个条件, 那么事件组称为样本空间的一个划分:</p>
<ul>
<li>$\forall i &ne; j \in \{1, 2, …, n\}, A_iA_j = \phi$</li>
<li>$A_1 \cup A_2… \cup A_n = \Omega$ </li>
</ul>
<p>如果事件$\{A_j\}$是样本空间$\Omega$的一个划分, 且$P(A_j) &gt; 0$, 那么对于任意事件B, 全概率公式为:</p>
<script type="math/tex; mode=display">P(B) = \sum_{i=1}^{n}{P(A_i)P(B|A_i)}</script><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)P(A)}{P(B)}</script><p>设$A_1, A_2, …, A_n$是样本空间$\Omega$的一个划分, 如果对于任意事件B, 有$P(B) &gt; 0$, 那么</p>
<script type="math/tex; mode=display">P(A_i|B) = \frac{P(B|A_i)P(A_i)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\displaystyle \sum_{i=1}^{n}{P(A_i)P(B|A_i)}}</script><h3 id="概率公式总结"><a href="#概率公式总结" class="headerlink" title="概率公式总结"></a>概率公式总结</h3><script type="math/tex; mode=display">P(A|B) = \frac{P(A, B)}{P(B)}</script><script type="math/tex; mode=display">P(B) = \sum_{i=1}^{n}{P(A_i)P(B|A_i)}</script><script type="math/tex; mode=display">P(A_i|B) = \frac{P(A_i, B)}{P(B)}</script><script type="math/tex; mode=display">P(A_i|B) = \frac {P(A_i)P(B|A_i)} {\displaystyle \sum_{i=1}^{n}{P(A_i)P(B|A_i)} }</script><h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><p>期望即均值, 是概率加权下的”平均值”, 是每次可能结果的概率乘以其结果的总和, 反映的是随机变量平均取值大小, 常用符号$\mu$表示:</p>
<ul>
<li>连续性数据, $E(X) = \int_{-\infty}^{+\infty}xf(x)dx$</li>
<li>离散性数据, $E(X) = \sum_{i}{x_ip_i}$</li>
</ul>
<p>假设C为一个常数, X和Y是两个随机变量, 期望的性质如下:</p>
<ul>
<li>E(C) = C</li>
<li>E(CX) = CE(X)</li>
<li>E(X + Y) = E(X) + E(Y)</li>
<li>如果X和Y相互独立, 那么E(XY) = E(X)E(Y)</li>
<li>如果E(XY) = E(X)E(Y), 那么X和Y不相关</li>
</ul>
<h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>衡量随机变量或一组数据离散程度的度量, 是用来度量随机变量和其数学期望之间的偏离程度. 即方差是衡量原数据和期望/均值相差的度量值.</p>
<script type="math/tex; mode=display">Var(X) = D(X) = \sigma^2 = \frac{\sum{(X-\mu)^2}}{N}</script><script type="math/tex; mode=display">D(X) = \sum_{n=1}^{n}{P_i}{(X_i-\mu)^2}</script><script type="math/tex; mode=display">D(X) = \int_a^b(x-u)^2f(x)dx</script><script type="math/tex; mode=display">D(X) = E((X-E(X))^2) = E(X^2) - (E(x))^2</script><div class="table-container">
<table>
<thead>
<tr>
<th>X</th>
<th>2</th>
<th>4</th>
<th>6</th>
<th>8</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>$P(x)$</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">E(X) = 2*0.2 + 4*0.2 + 6*0.2 + 8*0.2 + 10*0.2 = 6</script><script type="math/tex; mode=display">\begin{align}
E(X^2) &= 2^2*0.2 + 4^2*0.2 + 6^2*0.2 + 8^2*0.2 + 10^2*0.2\\
&= 4*0.2 + 16*0.2 + 36*0.2 + 64*0.2 + 100*0.2\\
&= 220*0.2\\
&= 44
\end{align}</script><script type="math/tex; mode=display">D(X) = E(X^2) - (E(X))^2 = 44 - 6^2 = 8</script><p>假设C是一个常数, X和Y是两个随机变量, 那么方差的性质如下:</p>
<p>$D(C) = 0$<br>$D(CX) = C^2D(X)$<br>$D(C + X) = D(X)$<br>$D(X \pm Y) = D(X) + D(Y) \pm 2 Cov(X,Y)$</p>
<h3 id="标准差"><a href="#标准差" class="headerlink" title="标准差"></a>标准差</h3><p>标准差(Standard Deviation)是方差的算术平方根, 用符号$\sigma$表示.<br>标准差和方差都是测量离散程度最重要最常见的指标. 不同之处在于标准差和变量的计算单位是相同的, 比方差清楚, 因此在很多分析的时候用标准差. </p>
<script type="math/tex; mode=display">\sigma = \sqrt{D(X)} = \sqrt{\frac{\sum(X-\mu)^2}{N}}</script><h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>协方差常用于衡量两个变量的总体误差, 当两个变量相同的情况下, 协方差其实就是方差<br>如果X和Y是统计独立的, 那么二者之间的协方差为零; 如果协方差为零, 那么X和Y是不相关的.</p>
<script type="math/tex; mode=display">\begin{align}
Cov(X,Y) &= E[(X-E(X))(Y-E(Y))]\\
&= E[XY - XE(Y) - YE(X) + E(X)E(Y)]\\
&= E(XY) - E(X)E(Y) - E(Y)E(X) + E(X)E(Y)\\
&= E(XY) - E(X)E(Y)\\
\end{align}</script><p>假设C为一个常数, X和Y是两个随机变量, 那么协方差性质如下:</p>
<script type="math/tex; mode=display">Cov(X, Y) = Cov(Y, X)</script><script type="math/tex; mode=display">Cov(aX, bY) = abCov(X, Y)</script><script type="math/tex; mode=display">Cov(X_1 + X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y)</script><p>协方差是两个随机变量具有相同方向变化趋势的度量:</p>
<ul>
<li>若$Cov(X, Y) &gt; 0$, 则X和Y的变化趋势相同</li>
<li>若$Cov(X, Y) &lt; 0$, 则X和Y的变化趋势相反</li>
<li>若$Cov(X, Y) = 0$, 则X和Y则不相关, 即变化没有什么相关性</li>
</ul>
<p>对于n个随机向量($X_1, X_2, X_3…X_n$), 任意两个元素$X_i$和$X_j$都可以得到一个协方差, 从而形成一个n*n的矩阵即为协方差矩阵, 协方差矩阵为对称矩阵</p>
<script type="math/tex; mode=display">C_{ij} = E\{[E_i - E(X_i)][E_j - E(X_j)]\} = Cov(X_i, X_j)</script><script type="math/tex; mode=display">\begin{bmatrix}
{c_{11}}&{c_{12}}&{\cdots}&{c_{1n}}\\
{c_{21}}&{c_{22}}&{\cdots}&{c_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{c_{m1}}&{c_{m2}}&{\cdots}&{c_{mn}}\\
\end{bmatrix}</script><h3 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h3><ul>
<li>大数定律的意义在于, 随着样本容量n的增加, 样本平均数将接近于总体平均数(期望$\mu$), 所以在统计推断中, 一般都会使用样本平均数估计总体平均数的值.</li>
<li>使用一部分样本的平均值来代替整体样本的期望/均值, 出现偏差的可能是存在的, 但是当n足够大的时候, 偏差的可能性是非常小的, 当n无限大的时候, 这种可能性的概率基本为0</li>
<li>大数定律的作用就是为使用频率来估计概率提供来理论支持; 为使用部分数据来近似的模拟构建全部数据特征提供来理论支持</li>
</ul>
<h3 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><ul>
<li>中心极限定理(Central Limit Theorem), 假设${X_n}$为独立同分布的随机变量序列, 并有相同的期望$\mu$和方差$\sigma^2$, 则$X_n$服从中心极限定理, 且${Z_n}$为随机序列${X_n}$的规范和:<script type="math/tex; mode=display">Y_n = X_1 + X_2 + ... + X_n = \sum_{n=1}^{n}{X_i} \rightarrow N(n\mu, n\sigma^2)</script><script type="math/tex; mode=display">Z_n = \frac{Y_n - E(Y_n)}{\sqrt{D(Y_n)}} = \frac{Y_n - n\mu}{\sqrt{n}\sigma} \rightarrow(0, 1)</script></li>
<li>中心极限定理就是一般在同分布的情况下, 抽样样本值的规范和在总体数量趋于无穷时的极限分布近似于正态分布</li>
</ul>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><ul>
<li>最大似然估计(Maximum Likelihood Estimation, MLE)是一种具有理论性的参数估计方法. 基本思想: 当从模型总体随机抽取n组样本观测值后, 最合理的参数估计量应该使得从模型中抽取n组样本观测值的概率最大, 步骤如下:<ul>
<li>写出似然函数</li>
<li>对似然函数取对数, 并整理</li>
<li>求导数</li>
<li>解似然函数</li>
</ul>
</li>
</ul>
<p>设总体分布为$f(x, \theta)$, $X_n$为该总体采样得到的样本. 因为随机序列${X_n}$独立同分布, 则它们的联合密度函数为:</p>
<script type="math/tex; mode=display">L(x_1, x_2, ..., x_n) = \prod_{i=1}^{n}{f(x_i; \theta_1, \theta_2, ..., \theta_n)}</script><ul>
<li>$\theta$被看作固定但是未知的参数, 反过来, 因为样本的已经存在, 可以看作${X_n}$是固定的, $L(x, \theta)$是关于$\theta$的函数, 即似然函数</li>
<li>求参数$\theta$, 使得似然函数取最大值, 此为最大似然估计法</li>
</ul>
<p><a href="http://fangs.in/post/thinkstats/likelihood/" target="_blank" rel="noopener">似然与极大似然估计</a></p>
<h3 id="向量的运算"><a href="#向量的运算" class="headerlink" title="向量的运算"></a>向量的运算</h3><p>两个向量: $\overrightarrow{a} = (x_1, y_1)$, $\overrightarrow{b} = (x_2, y_2)$, 并且a和b之间的夹角为$\theta$</p>
<ul>
<li>数量积: 两个向量的数量积(内积, 点积)是一个数量/实数, 记作$\overrightarrow{a} \cdot \overrightarrow{b}$<script type="math/tex; mode=display">\overrightarrow{a} \cdot \overrightarrow{b} = \vert \overrightarrow{a} \vert * \vert \overrightarrow{b} \vert * cos\theta</script></li>
<li>向量积: 两个向量的向量积(外积, 叉积)是一个向量, 记作$\overrightarrow{a} \times \overrightarrow{b}$; 向量积即两个不共线非零向量所在平面的一组法向量<script type="math/tex; mode=display">\vert \overrightarrow{a} \times \overrightarrow{b} \vert = \vert \overrightarrow{a} \vert * \vert \overrightarrow{b} \vert * sin\theta</script></li>
</ul>
<h3 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h3><ul>
<li>假设A, B均为$m*n$阶矩阵, 那么<ul>
<li>$C = A \pm B$</li>
<li>$A + B = B +A$</li>
<li>$(A + B) + C = A + (B + C)$</li>
<li>$(\lambda\mu)A = \lambda(\mu A)$</li>
<li>$\lambda(A + B) = \lambda A + \lambda B$</li>
<li>$(AB)C = A(BC)$</li>
<li>$(A + B)C = AC + BC$</li>
<li>$C(A + B) = CA + CB$</li>
<li>$(A^T)^T = A$</li>
<li>$(\lambda A)^T = \lambda A^T$</li>
<li>$(AB)^T = B^TA^T$</li>
<li>$(A + B)^T = A^T + B^T$</li>
</ul>
</li>
</ul>
<h3 id="QR分解"><a href="#QR分解" class="headerlink" title="QR分解"></a>QR分解</h3><ul>
<li>QR分解是将矩阵分解为一个正交矩阵和一个上三角矩阵的乘积</li>
</ul>
<p><img src="/数学概念/QR分解示意图.png" width="400px"></p>
<ul>
<li>其中, Q为正交矩阵, $Q^TQ = I, R为上三角矩阵$</li>
<li>QR分解经常被用来解<a href="https://blog.csdn.net/qq_29721419/article/details/69676770" target="_blank" rel="noopener">线性最小二乘问题</a></li>
</ul>
<h3 id="SVD分解"><a href="#SVD分解" class="headerlink" title="SVD分解"></a>SVD分解</h3><ul>
<li>奇异值分解(Singular Value Decomposition)是一种重要的矩阵分解方法, 可以看作是对称矩阵在任意矩阵上的推广</li>
<li>假设A为一个$m*n$阶的实矩阵, 则存在一个分解使得:<script type="math/tex; mode=display">A_{m*n} = U_{m*n}\Sigma_{m*n}V_{m*n}^T</script><ul>
<li>通常奇异值由大到小排列, 这样$\Sigma$便能由A唯一确定了</li>
</ul>
</li>
</ul>
<h3 id="向量的导数"><a href="#向量的导数" class="headerlink" title="向量的导数"></a>向量的导数</h3><p>A为一个$m*n$阶的矩阵, $x$为$n\ast1$的列向量, 则$Ax$为$m\ast1$的列向量, 计作$\overrightarrow{y} = A \cdot \overrightarrow{x}$</p>
<script type="math/tex; mode=display">A = \begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\
\end{bmatrix}</script><script type="math/tex; mode=display">\overrightarrow{x} = \begin{bmatrix}
{x_{1}}\\
{x_{2}}\\
{\vdots}\\
{a_{n}}\\
\end{bmatrix}</script><script type="math/tex; mode=display">\overrightarrow{y} = \begin{bmatrix}
{a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n}\\
{a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n}\\
{\vdots}\\
{a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n}\\
\end{bmatrix}</script><script type="math/tex; mode=display">\frac{ \partial \overrightarrow{y} }{ \partial \overrightarrow{x} } = \frac{ \partial A\overrightarrow{x} }{ \partial \overrightarrow{x} } = 

\displaystyle \frac{\partial}{\partial \overrightarrow{x}} \begin{bmatrix}
{a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n}\\
{a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n}\\
{\vdots}\\
{a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n}\\
\end{bmatrix}

= \begin{bmatrix}
{a_{11}}&{a_{21}}&{\cdots}&{a_{m1}}\\
{a_{12}}&{a_{22}}&{\cdots}&{a_{m2}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{1n}}&{a_{2n}}&{\cdots}&{a_{mn}}\\
\end{bmatrix}

= A^T</script><h4 id="向量的导数-1"><a href="#向量的导数-1" class="headerlink" title="向量的导数"></a>向量的导数</h4><script type="math/tex; mode=display">\frac{\partial A \overrightarrow{x}}{\partial \overrightarrow{x}} = A^T</script><script type="math/tex; mode=display">\frac{\partial A \overrightarrow{x}}{\partial \overrightarrow{x}^T} = A</script><script type="math/tex; mode=display">\frac{\partial (\overrightarrow{x}^TA)}{\partial \overrightarrow{x}} = A</script><h4 id="标量对向量的导数"><a href="#标量对向量的导数" class="headerlink" title="标量对向量的导数"></a>标量对向量的导数</h4><ul>
<li>$A$为$n*n$的矩阵, $x$为$n \ast 1$的列向量, 计作$\overrightarrow{y} = \overrightarrow{x}^T \cdot A \cdot \overrightarrow{x}$</li>
<li>同理可得: <script type="math/tex; mode=display">\frac{\partial{y}}{\partial{\overrightarrow{x}}} = \frac{\partial(\overrightarrow{x}^T \cdot A \cdot \overrightarrow{x})}{\partial{\overrightarrow{x}}} = (A^T + A) \cdot \overrightarrow{x}</script></li>
<li>若A为对称矩阵, 则有<script type="math/tex; mode=display">\frac{\partial(\overrightarrow{x}^T A \overrightarrow{x})}{\partial{\overrightarrow{x}}} = 2A\overrightarrow{x}</script>推导过程如下:</li>
</ul>
<script type="math/tex; mode=display">A = \begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\
\end{bmatrix}</script><script type="math/tex; mode=display">\overrightarrow{x} = \begin{bmatrix}
{x_{1}}\\
{x_{2}}\\
{\vdots}\\
{a_{n}}\\
\end{bmatrix}</script><script type="math/tex; mode=display">\begin{align}
\overrightarrow{x}^T A \overrightarrow{x} 
&= (x_1, x_2,\cdots, x_n)
\begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{n1}}&{a_{n2}}&{\cdots}&{a_{nn}}\\
\end{bmatrix}
\begin{bmatrix}
{x_{1}}\\
{x_{2}}\\
{\vdots}\\
{a_{n}}\\
\end{bmatrix}\\
&=(x_1, x_2,\cdots, x_n)(\Sigma_{j=1}^{n}{a_{1j}{x_j}}, \Sigma_{j=1}^{n}{a_{2j}{x_j}}, \cdots, \Sigma_{j=1}^{n}{a_{nj}{x_j}})^T\\
&=x_1\Sigma_{j=1}^{n}{a_{1j}{x_j}} + x_2\Sigma_{j=1}^{n}{a_{2j}{x_j}} + \cdots + x_n\Sigma_{j=1}^{n}{a_{nj}{x_j}}\\
&=\Sigma_{i=1}^{n} \Sigma_{j=1}^{n} a_{ij} x_i x_j\\
\end{align}</script><script type="math/tex; mode=display">\begin{align}
\frac{\partial(\overrightarrow{x}^T A \overrightarrow{x})}{\overrightarrow{x}} &= \frac{\partial(\Sigma_{i=1}^{n} \Sigma_{j=1}^{n} a_{ij} x_i x_j)}{\overrightarrow{x}}\\
&=\frac{\partial}{\overrightarrow{x}}[\Sigma_{i=1}^{n}(a_{i1}x_ix_1 + a_{i2}x_ix_2 + \cdots + a_{in}x_ix_n)]\\
&= \frac{\partial}{\overrightarrow{x}}[\\
&a_{11}x_1x_1 + a_{12}x_1x_2 + \cdots + a_{1n}x_1x_n +\\
&a_{21}x_2x_1 + a_{22}x_2x_2 + \cdots + a_{2n}x_2x_n +\\
&\vdots\\
&a_{n1}x_nx_1 + a_{n2}x_nx_2 + \cdots + a_{nn}x_nx_n ]\\
&=\\
&[a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n +\\
&a_{21}x_2+\\
&\vdots\\
&a_{n1}x_n] +\\

[&\qquad\quad a_{12}x_1 +\\
&a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n +\\
&\vdots\\
&\qquad\quad a_{n2}x_n] +\\
& \vdots\\
&+ \\
[&\qquad \qquad \qquad \qquad \qquad a_{1n}x_1 +\\
 &\qquad \qquad \qquad \qquad \qquad a_{2n}x_2 +\\
 &\vdots\\
 &+ \\
 &a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n]\\

\end{align}</script><p>由上式得到</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
{2a_{11}}&{a_{12}}&{a_{13}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{0}&{0}&{\cdots}&{0}\\
{a_{31}}&{0}&{0}&{\cdots}&{0}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{m1}}&{0}&{0}&{\cdots}&{0}\\
\end{bmatrix}

+

\begin{bmatrix}
{0}&{a_{12}}&{0}&{\cdots}&{0}\\
{a_{21}}&{2a_{22}}&{a_{23}}&{\cdots}&{a_{2n}}\\
{0}&{a_{32}}&{0}&{\cdots}&{0}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{0}&{a_{n2}}&{0}&{\cdots}&{0}\\
\end{bmatrix} 

+ \cdots +

\begin{bmatrix}
{0}&{0}&{0}&{\cdots}&{a_{1n}}\\
{0}&{0}&{0}&{\cdots}&{a_{2n}}\\
{0}&{0}&{0}&{\cdots}&{a_{3n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{m1}}&{a_{m2}}&{a_{m3}}&{\cdots}&{2a_{mn}}\\
\end{bmatrix} 
= \Sigma_{j=1}^{n}(a_{ij} + a_{ji})x_j</script><h4 id="标量对方阵对导数"><a href="#标量对方阵对导数" class="headerlink" title="标量对方阵对导数"></a>标量对方阵对导数</h4><p>A为$n * n$的矩阵, $|A|$为A的行列式, 计算$\frac{\partial{|A|}}{\partial{A}}$</p>
<script type="math/tex; mode=display">\forall 1 \le i \le n, |A| = \Sigma_{j=1}^{n}a_{ij}(-1)^{i+j}M_{ij}</script><script type="math/tex; mode=display">\frac{\partial |A|}{\partial a_{ij}} = \frac{\partial(\Sigma_{j=1}^{n}a_{ij}(-1)^{i+j}M_{ij})}{\partial a_{ij}} = (-1)^{i+j}M_{ij} = A_{ji}^*</script><script type="math/tex; mode=display">\frac{\partial \lvert A \rvert}{\partial A} = (A^*)^T = \lvert A \rvert \cdot (A^{-1})^T</script><h2 id="模型训练及测试"><a href="#模型训练及测试" class="headerlink" title="模型训练及测试"></a>模型训练及测试</h2><ul>
<li>模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择</li>
<li>在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见)</li>
<li>模型的选择会尽可能多的选择算法进行执行, 并比较执行结果 </li>
<li>模型的测试一般从以下几个方面来进行比较, 分别是准确率, 召回率, 精准率, F值 <ul>
<li>准确率(Accuracy) = 提取出的正确样本数/总样本数</li>
<li>召回率(Recall) = 正确的正例样本数/样本中的正例样本数- 覆盖率</li>
<li>精确度(Precision) = 正确的正例样本数/预测为正例的样本数</li>
<li>F值 = Precision <em> Recall </em> 2 / (Precision + Recall), 即F值是精准度和召回率的调和平均值</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr>
<td>真实值</td>
<td>正例</td>
<td>true positive真正例(A)</td>
<td>false negative假负例(B)</td>
</tr>
<tr>
<td></td>
<td>负例</td>
<td>false positive假正例(C)</td>
<td>true negative真负例(D)</td>
</tr>
</tbody>
</table>
</div>
<p>true positive(hit)真正例, 确实是正例<br>true negative(Correct Rejection)真负例, 确实是负例<br>false positive(False Alarm)假正例, 本来真实值是负例, 被预测为正例了, 虚报, 干扰报警<br>false negative(Miss)假负例, 本来真实值是正例, 被预测为负例了, 即没有预测出来</p>
<p>击中(Hit)(报准)和正确拒绝(Correct Rejection)是正确反应, 虚报(False Alarm)和漏报(Miss)是错误反应</p>
<p>A和D预测正确, B和C预测错误, 那么计算结果为:</p>
<script type="math/tex; mode=display">Accuracy = (A + D) / (A + B + C + D)</script><script type="math/tex; mode=display">Recall = A / (A + B)</script><script type="math/tex; mode=display">Precision = A / (A + C)</script><script type="math/tex; mode=display">\displaystyle F = \frac{Precision \ast Recall \ast 2}{Precision + Recall}</script><p>举个例子, 真实值中, 正例为80, 负例为20; 预测值中, 正例为90, 负例为10, 然而, 在模型的实际预测中, 原本有75个真正的正例, 有15个是假正例, 5个假负例和5个真负例, 如下图所示:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例90</td>
<td>负例10</td>
</tr>
<tr>
<td>真实值</td>
<td>正例80</td>
<td>真正例(A)75</td>
<td>假负例(B)5</td>
</tr>
<tr>
<td></td>
<td>负例20</td>
<td>假正例(C)15</td>
<td>真负例(D)5</td>
</tr>
</tbody>
</table>
</div>
<p>Accuracy和Recall是一对互斥的关系, Accuracy在增大的时候Recall是在减小的</p>
<p><a href="/ai/AI/precision和accuracy的区别/">precision和accuracy的区别</a></p>
<h2 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h2><p>ROC(Receiver Operating Characteristic), 描述的是分类混淆矩阵中FPR-TPR之间的相对变化情况.<br>纵轴是TPR(True Positive Rate), 横轴是FPR(False Positive Rate)</p>
<p>如果二元分类器输出的是对正样本对一个分类概率值, 当取不同阈值时会得到不同当混淆矩阵， 对应于ROC曲线上当一个点, ROC曲线就反应了FPR和TPR之间权衡当情况, 通俗的说, 即在TPR随着FPR递增的情况下, 谁增长的更快, 快多少的问题. TPR增长的越快, 曲线越往上弯曲, AUC就越大, 反应了模型的分类性能就越好. 当正负样本不平衡时, 这种模型评价方式比起一般当精确度评价方式的好处尤其显著. </p>
<h3 id="AUC-Area-Under-Curve"><a href="#AUC-Area-Under-Curve" class="headerlink" title="AUC(Area Under Curve)"></a>AUC(Area Under Curve)</h3><p>AUC被定义为ROC曲线下的面积, 显然这个面积的数值不会大于1. 由于ROC曲线一般都处于<code>y=x</code>这条直线的上方, 所以AUC的取之范围在0.5和1之间. AUC作为数值可以直观的评价分类器的好坏, 值越大越好.</p>
<p>AUC的值一般要求在0.7以上.</p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>回归结果度量</p>
<ul>
<li>explained_varicance_score, 可解释方差的回归评分函数</li>
<li>mean_absolute_error, 平均绝对误差</li>
<li>mean_squared_error, 平均平方误差</li>
</ul>
<h3 id="模型评估总结-分类算法评估方式"><a href="#模型评估总结-分类算法评估方式" class="headerlink" title="模型评估总结_分类算法评估方式"></a>模型评估总结_分类算法评估方式</h3><div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>scikit-learn函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>精确度</td>
<td>from sklearn.metrics import precision_score</td>
</tr>
<tr>
<td>Recall</td>
<td>召回率</td>
<td>from sklearn.metrics import recall_score</td>
</tr>
<tr>
<td>F1</td>
<td>F1指标</td>
<td>from sklearn.metrics import f1_score</td>
</tr>
<tr>
<td>Confusion Matrix</td>
<td>混淆矩阵</td>
<td>from sklearn.metrics import confusion_matrix</td>
</tr>
<tr>
<td>ROC</td>
<td>ROC曲线</td>
<td>from sklearn.metrics import roc</td>
</tr>
<tr>
<td>AUC</td>
<td>ROC曲线下的面积</td>
<td>from sklearn.metrics import auc</td>
</tr>
<tr>
<td>Mean Square Error(MSE, RMSE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_squared_error</td>
</tr>
<tr>
<td>Absolute Error(MAE, RAE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_absolute_error, median_absolute_error</td>
</tr>
<tr>
<td>R-Squared</td>
<td>平均方差</td>
<td>from sklearn.metrics import r2_score</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">\overrightarrow{x}^T A \overrightarrow{x} = (x_1, x_2,\cdots, x_n)
\begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{n1}}&{a_{n2}}&{\cdots}&{a_{nn}}\\
\end{bmatrix}
\begin{bmatrix}
{x_{1}}\\
{x_{2}}\\
{\vdots}\\
{a_{n}}\\
\end{bmatrix}
=(x_1, x_2,\cdots, x_n)(\Sigma_{j=1}^{n}{a_{1j}{x_j}}, \Sigma_{j=1}^{n}{a_{2j}{x_j}}, \cdots, \Sigma_{j=1}^{n}{a_{nj}{x_j}})^T
=x_1\Sigma_{j=1}^{n}{a_{1j}{x_j}} + x_2\Sigma_{j=1}^{n}{a_{2j}{x_j}} + \cdots + x_n\Sigma_{j=1}^{n}{a_{nj}{x_j}}
=\Sigma_{i=1}^{n} \Sigma_{j=1}^{n} a_{ij} x_i x_j</script>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/鸢尾花数据分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/鸢尾花数据分类/" itemprop="url">鸢尾花数据分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-05T15:23:29+08:00">
                2019-10-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># 从sklearn导入数据集</span><br><span class="line">from sklearn import datasets</span><br><span class="line"># 从数据集导入鸢尾花数据集</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"># &apos;data&apos;, the data to learn, &apos;target&apos;, the classification labels,</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line"># test train split</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"># random_state is the random number generator</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)</span><br><span class="line"></span><br><span class="line"># Model training</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=5)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Predict</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Score</span><br><span class="line">score = knn.score(X_test, y_test)</span><br><span class="line">print(f&apos;score: &#123;score&#125;&apos;)</span><br><span class="line"></span><br><span class="line"># Cross validation</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">score = cross_val_score(knn, X, y, cv=5, scoring=&apos;accuracy&apos;)</span><br><span class="line">print(f&apos;score: &#123;score&#125;&apos;)</span><br><span class="line"></span><br><span class="line"># Parameter tuning</span><br><span class="line">k_range = range(1, 31)</span><br><span class="line">k_scores = []</span><br><span class="line">for k in k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    scores = cross_val_score(knn, X, y, cv=10, scoring=&apos;accuracy&apos;)</span><br><span class="line">    k_scores.append(scores.mean())</span><br><span class="line"></span><br><span class="line">print(f&apos;k_range: &#123;k_range&#125;&apos;)</span><br><span class="line">print(f&apos;k_scores: &#123;k_scores&#125;&apos;)</span><br><span class="line"></span><br><span class="line"># plot</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.plot(k_range, k_scores)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># model save</span><br><span class="line">import pickle</span><br><span class="line">with open(&apos;save/knn_iris.mdl&apos;, &apos;wb&apos;) as f:</span><br><span class="line">    pickle.dump(knn, f)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/有约束的最优化问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/有约束的最优化问题/" itemprop="url">有约束的最优化问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-16T10:10:41+08:00">
                2019-07-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="有约束的最优化问题"><a href="#有约束的最优化问题" class="headerlink" title="有约束的最优化问题"></a>有约束的最优化问题</h2><ul>
<li>最优化问题一般是指对于某一个函数而言, 求解在其指定作用域上的全局最小值问题, 一般分为三种情况, 这三种方法求出来的解都有可能有局部最小值, 只有当函数是凸函数的时候, 才可以得到全局最小值<ul>
<li>无约束问题, 一般求解方式为梯度下降法, 牛顿法, 坐标轴下降法, $ \displaystyle \min \limits_{x}f(x)$</li>
<li>等式约束条件, 求解方式为拉格朗日乘子法, $\displaystyle \min \limits_{x}f(x), s.t: h_k(x)=0, k=1,2,\cdots, p$</li>
<li>不等式约束条件, 求解方式为KKT条件<script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x), s.t: h_k(x)=0, k=1,2,\cdots, p, g_j(x) \le 0, j=1, 2, \cdots, q</script></li>
</ul>
</li>
</ul>
<h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h4><script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x), s.t: h_i(x)=0, i=1,2,\cdots, p</script><script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x) + \displaystyle \sum_{i=1}^{p} \alpha_i h_i(x); \quad \alpha_i \ne 0</script><h4 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h4><p>在优化问题中, 目标函数f(x)存在多种形式, 如果目标函数和约束条件都为变量x的线性函数, 则称问题为线性规划; 如果目标函数为二次函数, 则称优化问题为二次规划; 如果目标函数或者约束条件为非线性函数, 则称最优化问题为非线性规划. 每个线性规划问题都有一个对偶问题</p>
<ul>
<li>对偶问题的对偶是原问题</li>
<li>无论原始问题是否是凸的, 对偶问题都是凸优化问题</li>
<li>对偶问题可以给出原始问题的一个下界</li>
<li>当满足一定条件的时候, 原始问题和对偶问题的解是完美等价的</li>
</ul>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>KKT条件是泛拉格朗日乘子法的一种形式, 主要应用在当我们的优化函数存在不等值约束的情况下的一种最优化解决方式; KKT条件即满足不等值约束的条件</p>
<script type="math/tex; mode=display">\displaystyle \min \limits_{x}f(x); \quad s.t.: h_k(x)=0, k=1,2,\cdots,p; \quad g_j(x) \le 0, j=1, 2, \cdots, q</script><script type="math/tex; mode=display">L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{p}\alpha_ih_i(x) + \sum_{i=1}^{q}\beta_ig_i(x);\quad\alpha_i \ne 0, \beta_i \lt 0</script><script type="math/tex; mode=display">\min \limits_{x}L(x, \alpha, \beta)</script><h4 id="KKT条件总结"><a href="#KKT条件总结" class="headerlink" title="KKT条件总结"></a>KKT条件总结</h4><ul>
<li>拉格朗日取得可行解的充要条件, $\nabla_xL(x, \alpha, \beta) = 0$</li>
<li>将不等式约束转换后的一个约束, 称为松弛互补条件, $\beta_ig_i(x) = 0, \quad i=1,2,\cdots,q$</li>
<li>初始约束条件, $h_i(x)=0, \quad i=1,2,\cdots,p$</li>
<li>初始约束条件, $g_i(x)\le0,\quad i=1,2,\cdots,q$</li>
<li>不等式约束需要满足的条件, $\beta_i\ge0,\quad i=1,2,\cdots,q$</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">64</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
