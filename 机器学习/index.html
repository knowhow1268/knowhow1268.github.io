<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />










<meta name="description" content="机器学习分类维度一 有监督学习 无监督学习 半监督学习  机器学习分类维度二 分类 聚类 回归 关联规则  模型训练及测试 模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择 在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见) 模型的选择">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="http://yoursite.com/机器学习/index.html">
<meta property="og:site_name" content="Wenhua">
<meta property="og:description" content="机器学习分类维度一 有监督学习 无监督学习 半监督学习  机器学习分类维度二 分类 聚类 回归 关联规则  模型训练及测试 模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择 在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见) 模型的选择">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG18.png">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG19.png">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG20.png">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG24.png">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG27.png">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG25.png">
<meta property="og:image" content="http://yoursite.com/机器学习/WechatIMG26.png">
<meta property="og:updated_time" content="2019-11-01T23:34:54.011Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习">
<meta name="twitter:description" content="机器学习分类维度一 有监督学习 无监督学习 半监督学习  机器学习分类维度二 分类 聚类 回归 关联规则  模型训练及测试 模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择 在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见) 模型的选择">
<meta name="twitter:image" content="http://yoursite.com/机器学习/WechatIMG18.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/机器学习/"/>





  <title>机器学习 | Wenhua</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wenhua</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/机器学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenhua">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-31T08:18:24+08:00">
                2019-10-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ai/" itemprop="url" rel="index">
                    <span itemprop="name">ai</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="机器学习分类维度一"><a href="#机器学习分类维度一" class="headerlink" title="机器学习分类维度一"></a>机器学习分类维度一</h2><ul>
<li>有监督学习</li>
<li>无监督学习</li>
<li>半监督学习</li>
</ul>
<h2 id="机器学习分类维度二"><a href="#机器学习分类维度二" class="headerlink" title="机器学习分类维度二"></a>机器学习分类维度二</h2><ul>
<li>分类</li>
<li>聚类</li>
<li>回归</li>
<li>关联规则</li>
</ul>
<h2 id="模型训练及测试"><a href="#模型训练及测试" class="headerlink" title="模型训练及测试"></a>模型训练及测试</h2><ul>
<li>模型选择, 对特定任务最优建模方法的选择或者对特定模型最佳参数的选择</li>
<li>在训练数据集上运行模型(算法)并在测试数据集中测试效果, 迭代进行数据模型对修改, 这种方式称为交叉验证(将原始数据分为训练集和测试集, 使用训练集构建模型, 并使用测试集评估模型提供修改意见)</li>
<li>模型的选择会尽可能多的选择算法进行执行, 并比较执行结果 </li>
<li>模型的测试一般从以下几个方面来进行比较, 分别是准确率, 召回率, 精准率, F值 <ul>
<li>准确率(Accuracy) = 提取出的正确样本数/总样本数</li>
<li>召回率(Recall) = 正确的正例样本数/样本中的正例样本数- 覆盖率</li>
<li>精确度(Precision) = 正确的正例样本数/预测为正例的样本数</li>
<li>F值 = Precision <em> Recall </em> 2 / (Precision + Recall), 即F值是精准度和召回率的调和平均值</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr>
<td>真实值</td>
<td>正例</td>
<td>true positive真正例(A)</td>
<td>false negative假负例(B)</td>
</tr>
<tr>
<td></td>
<td>负例</td>
<td>false positive假正例(C)</td>
<td>true negative真负例(D)</td>
</tr>
</tbody>
</table>
</div>
<p>true positive(hit)真正例, 确实是正例<br>true negative(Correct Rejection)真负例, 确实是负例<br>false positive(False Alarm)假正例, 本来真实值是负例, 被预测为正例了, 虚报, 干扰报警<br>false negative(Miss)假负例, 本来真实值是正例, 被预测为负例了, 即没有预测出来</p>
<p>击中(Hit)(报准)和正确拒绝(Correct Rejection)是正确反应, 虚报(False Alarm)和漏报(Miss)是错误反应</p>
<p>A和D预测正确, B和C预测错误, 那么计算结果为:</p>
<script type="math/tex; mode=display">Accuracy = (A + D) / (A + B + C + D)</script><script type="math/tex; mode=display">Recall = A / (A + B)</script><script type="math/tex; mode=display">Precision = A / (A + C)</script><script type="math/tex; mode=display">\displaystyle F = \frac{Precision \ast Recall \ast 2}{Precision + Recall}</script><p>举个例子, 真实值中, 正例为80, 负例为20; 预测值中, 正例为90, 负例为10, 然而, 在模型的实际预测中, 原本有75个真正的正例, 有15个是假正例, 5个假负例和5个真负例, 如下图所示:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>预测值</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>正例90</td>
<td>负例10</td>
</tr>
<tr>
<td>真实值</td>
<td>正例80</td>
<td>真正例(A)75</td>
<td>假负例(B)5</td>
</tr>
<tr>
<td></td>
<td>负例20</td>
<td>假正例(C)15</td>
<td>真负例(D)5</td>
</tr>
</tbody>
</table>
</div>
<p>Accuracy和Recall是一对互斥的关系, Accuracy在增大的时候Recall是在减小的</p>
<p><a href="/ai/AI/precision和accuracy的区别/">precision和accuracy的区别</a></p>
<h2 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h2><p>ROC(Receiver Operating Characteristic), 描述的是分类混淆矩阵中FPR-TPR之间的相对变化情况.<br>纵轴是TPR(True Positive Rate), 横轴是FPR(False Positive Rate)</p>
<p>如果二元分类器输出的是对正样本对一个分类概率值, 当取不同阈值时会得到不同当混淆矩阵， 对应于ROC曲线上当一个点, ROC曲线就反应了FPR和TPR之间权衡当情况, 通俗的说, 即在TPR随着FPR递增的情况下, 谁增长的更快, 快多少的问题. TPR增长的越快, 曲线越往上弯曲, AUC就越大, 反应了模型的分类性能就越好. 当正负样本不平衡时, 这种模型评价方式比起一般当精确度评价方式的好处尤其显著. </p>
<h3 id="AUC-Area-Under-Curve"><a href="#AUC-Area-Under-Curve" class="headerlink" title="AUC(Area Under Curve)"></a>AUC(Area Under Curve)</h3><p>AUC被定义为ROC曲线下的面积, 显然这个面积的数值不会大于1. 由于ROC曲线一般都处于<code>y=x</code>这条直线的上方, 所以AUC的取之范围在0.5和1之间. AUC作为数值可以直观的评价分类器的好坏, 值越大越好.</p>
<p>AUC的值一般要求在0.7以上.</p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>回归结果度量</p>
<ul>
<li>explained_varicance_score, 可解释方差的回归评分函数</li>
<li>mean_absolute_error, 平均绝对误差</li>
<li>mean_squared_error, 平均平方误差</li>
</ul>
<h3 id="模型评估总结-分类算法评估方式"><a href="#模型评估总结-分类算法评估方式" class="headerlink" title="模型评估总结_分类算法评估方式"></a>模型评估总结_分类算法评估方式</h3><div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>scikit-learn函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>精确度</td>
<td>from sklearn.metrics import precision_score</td>
</tr>
<tr>
<td>Recall</td>
<td>召回率</td>
<td>from sklearn.metrics import recall_score</td>
</tr>
<tr>
<td>F1</td>
<td>F1指标</td>
<td>from sklearn.metrics import f1_score</td>
</tr>
<tr>
<td>Confusion Matrix</td>
<td>混淆矩阵</td>
<td>from sklearn.metrics import confusion_matrix</td>
</tr>
<tr>
<td>ROC</td>
<td>ROC曲线</td>
<td>from sklearn.metrics import roc</td>
</tr>
<tr>
<td>AUC</td>
<td>ROC曲线下的面积</td>
<td>from sklearn.metrics import auc</td>
</tr>
<tr>
<td>Mean Square Error(MSE, RMSE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_squared_error</td>
</tr>
<tr>
<td>Absolute Error(MAE, RAE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_absolute_error, median_absolute_error</td>
</tr>
<tr>
<td>R-Squared</td>
<td>平均方差</td>
<td>from sklearn.metrics import r2_score</td>
</tr>
</tbody>
</table>
</div>
<h2 id="数据清洗和转换"><a href="#数据清洗和转换" class="headerlink" title="数据清洗和转换"></a>数据清洗和转换</h2><ul>
<li>实际生产环境中机器学习比较耗时的一部分</li>
<li>大部分机器学习模型所处理的都是特征, 特征通常是输入变量所对应的可用于模型的数值表示</li>
<li>大部分情况下, 收集到的数据需要经过预处理之后才能够被算法所用, 预处理的操作包括以下几个部分:<ul>
<li>数据过滤</li>
<li>处理数据缺失</li>
<li>处理可能的异常, 错误或异常值</li>
<li>合并多个数据源数据</li>
<li>数据汇总</li>
</ul>
</li>
<li>对数据进行初步的预处理, 需要将其转换为一种适合机器学习模型的表示形式, 对许多模型来说, 这种表示就是包含数值数据的向量或者矩阵<ul>
<li>将类别数据编码成为对应的数值表示(一般使用1-of-k方法)-dumy</li>
<li>从文本数据中提取有用的数据(一般使用词袋法或者TF-IDF)</li>
<li>处理图像或者音频数据(像素, 声波, 音频, 振幅&lt;傅立叶变换&gt;)</li>
<li>数值数据转换为类别数据以减少变量的值, 比如年龄分段</li>
<li>对数值数据进行转换, 比如对数转换</li>
<li>对特征进行正则化, 标准化, 以保证同一模型的不同输入变量的值域相同</li>
<li>对现有变量进行组合或转换以生成新特征, 比如平均数, 可以做虚拟变量不断尝试</li>
</ul>
</li>
</ul>
<h3 id="类型特征转换1-of-k-哑编码"><a href="#类型特征转换1-of-k-哑编码" class="headerlink" title="类型特征转换1-of-k(哑编码)"></a>类型特征转换1-of-k(哑编码)</h3><ul>
<li>将非数值型的特征值转换为数值型的数据</li>
<li>假设变量的取值有k个, 如果对这些值用1到k编序, 则可用维度为k对向量来表示一个变量对值. 在这样的向量里, 该取值所对应的序号所在的元素为1, 其他元素均为0.</li>
</ul>
<h3 id="文本数据抽取"><a href="#文本数据抽取" class="headerlink" title="文本数据抽取"></a>文本数据抽取</h3><ul>
<li>将一个文本当作一个无序的数据集合, 文本特征可以采用文本中的词条T进行体现, 那么文本中出现的所有词条及其出现的次数就可以体现文档的特征</li>
<li>TF-IDF, 词条的重要性随着它在文件中出现的次数成正比增加, 但同时会随着它在语料库中出现的频率成反比下降;<ul>
<li>词条在文本中出现的次数越多, 表示该词条对该文本的重要性越高</li>
<li>词条在所有文本中出现的次数越少, 说明这个词条对文本的重要性越高</li>
<li>TF(词频)指某个词条在文本中出现的次数, 一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)</li>
<li>IDF(逆向文件频率)指一条词条重要性的度量, 一般计算方式为总文件数目除以包含该词语之文件的数目, 再将得到的商取对数得到. TF-IDF即TF*IDF</li>
</ul>
</li>
</ul>
<p><a href="https://blog.csdn.net/asialee_bird/article/details/81486700" target="_blank" rel="noopener">TF-IDF算法介绍及实现</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line">import math</span><br><span class="line">import operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loadDataSet():</span><br><span class="line">    dataset = [ [&apos;my&apos;, &apos;dog&apos;, &apos;has&apos;, &apos;flea&apos;, &apos;problems&apos;, &apos;help&apos;, &apos;please&apos;],    # 切分的词条</span><br><span class="line">                   [&apos;maybe&apos;, &apos;not&apos;, &apos;take&apos;, &apos;him&apos;, &apos;to&apos;, &apos;dog&apos;, &apos;park&apos;, &apos;stupid&apos;],</span><br><span class="line">                   [&apos;my&apos;, &apos;dalmation&apos;, &apos;is&apos;, &apos;so&apos;, &apos;cute&apos;, &apos;I&apos;, &apos;love&apos;, &apos;him&apos;],</span><br><span class="line">                   [&apos;stop&apos;, &apos;posting&apos;, &apos;stupid&apos;, &apos;worthless&apos;, &apos;garbage&apos;],</span><br><span class="line">                   [&apos;mr&apos;, &apos;licks&apos;, &apos;ate&apos;, &apos;my&apos;, &apos;steak&apos;, &apos;how&apos;, &apos;to&apos;, &apos;stop&apos;, &apos;him&apos;],</span><br><span class="line">                   [&apos;quit&apos;, &apos;buying&apos;, &apos;worthless&apos;, &apos;dog&apos;, &apos;food&apos;, &apos;stupid&apos;] ]</span><br><span class="line">    classVec = [0, 1, 0, 1, 0, 1]  # 类别标签向量，1代表好，0代表不好</span><br><span class="line">    return dataset, classVec</span><br><span class="line"></span><br><span class="line">def feature_select(list_words):</span><br><span class="line">    doc_frequency = defaultdict(int)</span><br><span class="line">    # print(type(doc_frequency))</span><br><span class="line">    for word_list in list_words:</span><br><span class="line">        for i in word_list:</span><br><span class="line">            # print(doc_frequency[i])</span><br><span class="line">            doc_frequency[i] += 1</span><br><span class="line">            # print(doc_frequency[i])</span><br><span class="line">            # print(&quot;===================&quot;)</span><br><span class="line"></span><br><span class="line">    print(doc_frequency, doc_frequency.values())</span><br><span class="line">    word_tf = &#123;&#125;</span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        word_tf[i] = doc_frequency[i] / sum(doc_frequency.values())</span><br><span class="line">  </span><br><span class="line">    doc_num = len(list_words)</span><br><span class="line">    word_idf = &#123;&#125;</span><br><span class="line">    word_doc = defaultdict(int)</span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        for j in list_words:</span><br><span class="line">            if i in j:</span><br><span class="line">                word_doc[i] += 1</span><br><span class="line">    </span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        word_idf[i] = math.log(doc_num/(word_doc[i] + 1))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    word_tf_idf = &#123;&#125;</span><br><span class="line">    for i in doc_frequency:</span><br><span class="line">        word_tf_idf[i] = word_tf[i] * word_idf[i]</span><br><span class="line"></span><br><span class="line">    # print(word_tf_idf)</span><br><span class="line">    dict_feature_select = sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)</span><br><span class="line">    return dict_feature_select</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&apos;__main__&apos;:</span><br><span class="line">    data_list,label_list=loadDataSet() #加载数据</span><br><span class="line">    # print(data_list, label_list)</span><br><span class="line">    features=feature_select(data_list) #所有词的TF-IDF值</span><br><span class="line">    print(features)</span><br><span class="line">    print(len(features))</span><br></pre></td></tr></table></figure>
<h2 id="回归算法"><a href="#回归算法" class="headerlink" title="回归算法"></a>回归算法</h2><p>回归算法是一种有监督算法, </p>
<ul>
<li>线性回归</li>
<li>Logistic回归</li>
<li>Softmax回归</li>
<li>梯度下降</li>
<li>特征抽取</li>
<li>线性回归案例</li>
</ul>
<h3 id="回归算法认知"><a href="#回归算法认知" class="headerlink" title="回归算法认知"></a>回归算法认知</h3><p>通过房屋面积预测房价, 如下图所示:<br><img src="/机器学习/WechatIMG18.png" width="300px"><br>如果影响房价的还有房间数量, 那么:<br><img src="/机器学习/WechatIMG19.png" width="300px"><br><img src="/机器学习/WechatIMG20.png" width="300px"><br>那么问题的解决依赖于如下的方程:</p>
<script type="math/tex; mode=display">\begin{align}
h_{\theta}(x) &= \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n\\ 
&=\theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n\\
&=\theta_{0}1 + \theta_1x_1 + \cdots + \theta_nx_n\\
&=\sum_{i=0}^{n}\theta_ix_i = \theta^Tx
\end{align}</script><p>最后就是要计算出$\theta$的值, 并选择最优的$\theta$值构成算法公式</p>
<p>那么它的似然函数为:</p>
<script type="math/tex; mode=display">y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}</script><ul>
<li>误差$\epsilon^{(i)}(i \le i \le n)$是独立同分布的, 服从均值为0, 方差为某定值$\sigma^2$的高斯分布(查看<a href="https://baike.baidu.com/item/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/829451?fr=aladdin" target="_blank" rel="noopener">中心极限定理</a>)</li>
<li>在实际中, 很多随机现象可以看作是众多因素的独立影响的综合反应, 往往服从正态分布</li>
</ul>
<h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><script type="math/tex; mode=display">y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}</script><script type="math/tex; mode=display">p(\epsilon^{(i)}) = \frac{1}{\sigma \sqrt{2\pi}} e^{(- \frac{(\epsilon^{(i)})^2}{2\sigma^2})}</script><script type="math/tex; mode=display">p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sigma \sqrt{2\pi}}exp^{(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})}</script><p>当给定某个样本值x的时候, 那么得到的实际值y的概率是多少</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>误差</th>
<th>概率</th>
<th>预测值是实际值的概率</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\epsilon \downarrow$</td>
<td>p $\rightarrow$ 1</td>
<td>预测值是实际值的概率越大</td>
</tr>
<tr>
<td>$\epsilon \uparrow$</td>
<td>p $\rightarrow$ 0</td>
<td>预测值是实际值的概率越大</td>
</tr>
</tbody>
</table>
</div>
<p>上述只是一个样本, 概率越等于1越好,  如果对m个样本的概率进行相乘得到联合概率(样本与样本之间独立的), 这样就得到一个似然函数</p>
<script type="math/tex; mode=display">\begin{align}
L(\theta) &= \prod_{i=1}^{m}p(y^{(i)} | x^{(i)}; \theta)\\
&=\prod_{i=1}^{m} \frac{1}{\sigma \sqrt{2\pi}}exp^{(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})}\\
\end{align}</script><h3 id="对数似然-目标函数及最小二乘法"><a href="#对数似然-目标函数及最小二乘法" class="headerlink" title="对数似然, 目标函数及最小二乘法"></a>对数似然, 目标函数及最小二乘法</h3><script type="math/tex; mode=display">\begin{align}
l(\theta) &= \log L(\theta)\\
&= \log\prod_{i=1}^{m} \frac{1}{\sigma \sqrt{2\pi}}exp^{(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})}\\
&= m\log \frac{1}{\sigma \sqrt{2\pi}} - \frac{1}{2\sigma^2} \sum_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2\\
\end{align}</script><script type="math/tex; mode=display">loss(y_i, \widehat{y_i}) = J(\theta) = \frac{1}{2}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><h3 id="theta-的求解过程"><a href="#theta-的求解过程" class="headerlink" title="$\theta$的求解过程"></a>$\theta$的求解过程</h3><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><script type="math/tex; mode=display">\theta_0x_0^{(1)} + \theta_1x_1^{(1)} + ... + \theta_nx_n^{(1)} = \widehat{y^{(1)}}</script><script type="math/tex; mode=display">\theta_0x_0^{(2)} + \theta_1x_1^{(2)} + ... + \theta_nx_n^{(2)} = \widehat{y^{(2)}}</script><script type="math/tex; mode=display">\widehat{y} = 

\begin{bmatrix}
{\widehat{y^{(1)}}}\\
{\widehat{y^{(2)}}}\\
{\vdots}\\
{\widehat{y^{(m)}}}\\
\end{bmatrix}
=
\begin{bmatrix}
{x_0^{(1)}}&{x_1^{(1)}}&{\cdots}&{x_n^{(1)}}\\
{x_0^{(2)}}&{x_1^{(2)}}&{\cdots}&{x_n^{(2)}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{x_0^{(m)}}&{x_1^{(m)}}&{\cdots}&{x_n^{(m)}}\\
\end{bmatrix}

\begin{bmatrix}
{\theta_0}\\
{\theta_1}\\
{\vdots}\\
{\theta_m}\\
\end{bmatrix}
= 
X\theta</script><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} = \frac{1}{2}(X\theta - Y)^T (X\theta - Y) \rightarrow min_{\theta}J(\theta)</script><p><strong>即让$J(\theta)$最小的时候, $\theta$的取值</strong>, 这就是最小二乘法</p>
<script type="math/tex; mode=display">\begin{align}
\nabla_{\theta}J(\theta) &= \nabla_{\theta} \frac{1}{2}(X\theta - Y)^T (X\theta - Y)\\
&= \nabla_{\theta} \frac{1}{2}(\theta^TX^T - Y^T)(X\theta - Y)\\
&= \nabla_{\theta} \frac{1}{2}(\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta + Y^TY)\\
&= \frac{1}{2}(X^TX\theta + (\theta^TX^TX)^T - X^TY - (Y^TX)^T)\\
&= \frac{1}{2}(2X^TX\theta -2X^TY )\\
&= X^TX\theta - X^TY\\
\end{align}</script><script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><h3 id="最小二乘法的参数最优解"><a href="#最小二乘法的参数最优解" class="headerlink" title="最小二乘法的参数最优解"></a>最小二乘法的参数最优解</h3><ul>
<li><p>参数解析式</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><p>X为特征矩阵, Y为目标属性, 如果一个矩阵可逆, 那么它的行列式大于0才可以</p>
</li>
<li><p>最小二乘法的使用要求矩阵$X^TX$是可逆的; 为了防止不可逆或者过拟合问题的存在, 可以增加额外的数据影响, 导致最终的矩阵是可逆的</p>
<script type="math/tex; mode=display">\theta = (X^TX + \lambda I)^{-1}X^TY</script></li>
</ul>
<p>证明如下:</p>
<script type="math/tex; mode=display">\because (A)^T(A) \ge 0</script><script type="math/tex; mode=display">\therefore (x\mu)^T(x\mu) \ge 0, (向量\mu \ne 0)</script><script type="math/tex; mode=display">\therefore \mu^Tx^Tx\mu \ge 0</script><p>而$x^Tx$是半正定矩阵, 要想$\mu^TA\mu &gt; 0$<br>而$\mu^Tx^Tx\mu + \mu^T\mu &gt; 0$</p>
<script type="math/tex; mode=display">\therefore \mu^T(x^Tx + \lambda I)\mu > 0</script><script type="math/tex; mode=display">\therefore X^TX + \lambda I是正定矩阵, 正定矩阵一定可逆</script><p>那么中间的一定是大于0的, 就一定是正定矩阵，而正定矩阵是可逆的</p>
<p>另外一种理解方式</p>
<script type="math/tex; mode=display">对于X\theta=Y, 要想求\theta, 必须要消掉X, 要消掉X就会用到X的逆X^{-1}</script><script type="math/tex; mode=display">要想求X^{-1}, 一定是方阵才行, \therefore X^TX\theta = X^TY</script><script type="math/tex; mode=display">X^TX是方阵, 然后再整体求逆, \therefore (X^TX)^{-1}X^TX\theta = (X^TX)^{-1}X^TY</script><h3 id="目标损失函数-loss-cost-function"><a href="#目标损失函数-loss-cost-function" class="headerlink" title="目标损失函数(loss/cost function)"></a>目标损失函数(loss/cost function)</h3><ul>
<li>0-1损失函数 $J(\theta) = \begin{cases}1, Y \ne f(X)\\ 0, Y = f(X) \end{cases}$</li>
<li>感知损失函数 $J(\theta) = \begin{cases}1, \vert Y - f(X) \vert &gt; t\\ 0, \vert Y - f(X) \vert \leq t\end{cases}$</li>
<li>平方和损失函数 <script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}</script></li>
<li>绝对值损失函数<script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m}{\vert h_{\theta}(x^{(i)}) - y^{(i)} \vert}</script></li>
<li>对数损失函数<script type="math/tex; mode=display">J(\theta) = \sum_{i=1}^{m}{(y^{(i)}\log h_{\theta}(x^{(i)}))}</script></li>
</ul>
<p><strong>0-1损失函数在分类中用的比较多</strong><br><strong>感知损失函数用的也比较多, 稍微一变就是SVM</strong><br><strong>对数损失函数会用在逻辑回归中</strong></p>
<h3 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h3><p><img src="/机器学习/WechatIMG24.png" width="300px"></p>
<p>误差分为期望误差($L(f)$)和方差误差($\Omega(f)$)<br><a href="https://baijiahao.baidu.com/s?id=1601092478839269810&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">偏差误差和方差之间的区别和联系</a></p>
<p>05.avi_30:00</p>
<p>多项式扩展就是将特征与特征之间进行融合，从而形成新的特征的一个过程; 从数学上来讲就是将低维度空间上的点映射到高维度空间中, 本身属于特征工程的一种操作.</p>
<p>如果模型在训练集上效果非常好, 而在训练集上效果不好, 那么认为这个时候存在过拟合的情况; 多项式扩展的时候, 如果指定的阶数比较大, 那么有可能导致过拟合, 从线性回归来讲, 认为训练出来的模型参数值越大, 就表示越存在过拟合的情况.</p>
<h3 id="线性回归的过拟合"><a href="#线性回归的过拟合" class="headerlink" title="线性回归的过拟合"></a>线性回归的过拟合</h3><ul>
<li>目标函数  $J(\theta) = \displaystyle \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}$</li>
<li>为了防止数据过拟合, 即$\theta$值的样本空间不能过大或者过小, 可以在目标函数增加一个平方和损失:<br>$J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda \displaystyle\sum_{i=1}^{n}\theta_{j}^2$, 前面的一项体现的是期望误差, 后面一项体现的是模型的复杂度误差</li>
<li><p>正则项(norm): $\lambda \displaystyle\sum_{i=1}^{n}\theta_{j}^2$, 此正则项叫做L2-norm, 此外还有L1-norm: $\lambda \displaystyle\sum_{i=1}^{n}\vert \theta_{j} \vert$</p>
<ul>
<li>L2-norm(Ridge回归, 岭回归): $J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda \displaystyle\sum_{i=1}^{n}\theta_{j}^2, \lambda &gt; 0$</li>
<li>L1-norm(LASSO回归): $J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda \displaystyle\sum_{i=1}^{n}\vert \theta_{j} \vert, \lambda &gt; 0$, LASSO回归中间很容易出现稀疏解, 即很多0的情况; L1-norm的求解方式为坐标轴下降法</li>
</ul>
</li>
</ul>
<p>为了解决过拟合问题, 可以选择在损失函数中加入惩罚项(对于系数过大的惩罚), 分为L1-norm和L2-norm</p>
<h3 id="模型效果判断"><a href="#模型效果判断" class="headerlink" title="模型效果判断"></a>模型效果判断</h3><ul>
<li>MSE, 误差平方和, 越趋近于0表示模型越拟合训练数据</li>
<li>RMSE, MSE的平方根, 作用同MSE</li>
<li>$R^2$, 取值范围$(-\infty, 1]$, 值越大表示模型越拟合训练数据; 最优解是1, 当模型预测为随机值的时候, 有可能为负; 若预测值恒为样本值期望, $R^2$为0</li>
<li>TSS, 总平方和, 表示样本之间的差异情况, 是伪方差的m倍</li>
<li>RSS, 残差平方和, 表示预测值和样本值之间的差异, 是MSE的m倍</li>
</ul>
<script type="math/tex; mode=display">MSE = \frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2</script><script type="math/tex; mode=display">RMSE = \sqrt{MSE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2}</script><script type="math/tex; mode=display">R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{m}(y_i - \hat{y_i})^2}{\sum_{i=1}^{m}{(y_i - \overline{y})^2}}</script><script type="math/tex; mode=display">\overline{y} = \frac{1}{m}\sum_{i=1}^my_i</script><h3 id="Elastic-Net"><a href="#Elastic-Net" class="headerlink" title="Elastic Net"></a>Elastic Net</h3><p>同时使用L1正则和L2正则的线性回归模型称为Elastic Net算法(弹性网络算法)</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2} + \lambda(p\sum_{j=1}^{n}{\vert \theta_{j} \vert} + (1-p)\sum_{j=1}^{n}{\theta^2}) \begin{cases}
\lambda > 0\\
p \in [0, 1]
\end{cases}</script><p>该公式中p表示选中L1-norm的概率是多少, 选中L2-norm的概率是多少</p>
<h3 id="机器学习调参"><a href="#机器学习调参" class="headerlink" title="机器学习调参"></a>机器学习调参</h3><ul>
<li>实际中对于各种算法模型, 需要获取$\theta$, $\lambda$, $p$的值, $\theta$的求解其实就是算法模型的求解, 一般不需要开发人员的参与(算法已经实现), 主要需要求解的是$\lambda$和$p$的值(超参), 此过程叫做<strong>调参</strong></li>
<li>交叉验证, 将训练数据分为多份, 其中一份进行数据验证并获取最优的超参$\lambda$和$p$, 比如十指交叉验证和五指交叉验证(scikit-learn中默认)</li>
</ul>
<p>注: $y_i$为样本值, $\hat{y_i}$为预测值, $\overline{y_i}$为均值</p>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><ul>
<li>目标函数$\theta$求解, $J(\theta) = \displaystyle \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_{\theta}{(x^{(i)})} - y^{(i)})^2}$</li>
<li>初始化$\theta$(随机初始化, 可以初始化为0)</li>
<li>沿着负梯度方向迭代, 更新后的$\theta$使$J(\theta)$更小, $\theta = \theta - \alpha \cdot \displaystyle \frac{\partial J(\theta)}{\partial \theta}$, $\alpha$为学习率或者步长</li>
</ul>
<p>求解如下:</p>
<script type="math/tex; mode=display">\begin{align}\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j}\displaystyle \frac{1}{2}{(h_{\theta}{(x)} - y)^2}\\
&= \frac{\partial}{\partial \theta_j} \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_{\theta}{(x^{(i)})} - y^{(i)})^2}\\
&= \frac{1}{2} \cdot 2 (h_{\theta}(x) - y) \frac{\partial}{\partial \theta_j} (h_{\theta}{(x)} - y)\\
&= (h_{\theta}(x) - y) \frac{\partial}{\partial \theta_j} \displaystyle \sum_{i=1}^{m}(h_{\theta}{(x^{(i)})} - y^{(i)})\\
&= (h_{\theta}(x) - y) \frac{\partial}{\partial \theta_j} \displaystyle \sum_{i=1}^{m}(\theta_i x_i - y^{(i)})\\
&= (h_{\theta}(x) - y) x_j\\
\end{align}</script><h4 id="批量梯度下降法-BGD"><a href="#批量梯度下降法-BGD" class="headerlink" title="批量梯度下降法(BGD)"></a>批量梯度下降法(BGD)</h4><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} J(\theta) = (h_{\theta}(x) - y) x_j</script><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_j} = \displaystyle \sum_{i=1}^{m}(h_{\theta}{(x^{(i)})} - y^{(i)})x_j^{(i)}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha \displaystyle \sum_{i=1}^{m}{(h_{\theta}{(x^{(i)})} - y^{(i)})x_j^{(i)}}</script><h4 id="随机梯度下降法-SGD"><a href="#随机梯度下降法-SGD" class="headerlink" title="随机梯度下降法(SGD)"></a>随机梯度下降法(SGD)</h4><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} J(\theta) = (h_{\theta}(x) - y) x_j</script><script type="math/tex; mode=display">for i=1 to \space m, \theta_j = \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}</script><h4 id="线性回归的扩展"><a href="#线性回归的扩展" class="headerlink" title="线性回归的扩展"></a>线性回归的扩展</h4><ul>
<li>线性回归针对的是$\theta$而言, 对于样本本身, 样本可以是非线性的<br><img src="/机器学习/WechatIMG27.png" width="300px"></li>
</ul>
<h3 id="局部加权回归"><a href="#局部加权回归" class="headerlink" title="局部加权回归"></a>局部加权回归</h3><p><img src="/机器学习/WechatIMG25.png" width="300px"></p>
<p>普通线性回归损失函数为:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><p>局部加权回归损失函数为:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\displaystyle\sum_{i=1}^{m} w^{(i)}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><ul>
<li>$w^{(i)}$是权重, 根据预测的点与数据集中的点的距离来为数据集中的点赋权值, 当某点离要预测的点越远, 其权重越小, 否则越大. 常用值选择公式为:<script type="math/tex; mode=display">w^{(i)} = exp(- \frac{(x^{(i)} - \bar{x})^2}{2k^2})</script>该函数为指数衰减函数, 其中$k$为波长参数, 它控制了权值随距离下降的速率</li>
</ul>
<p><strong>一般实际工作中, 当线性模型的参数接近0的时候, 我们认为当前参数对应的那个特征属性在模型判断中没有太大的决策信息, 所以对于这样的属性我们可以删除; 一般情况下, 如果是手动删除的话, 选择小于$1e-4$的特征属性</strong></p>
<h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p>Logistic/Sigmoid函数, $\displaystyle p = h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$, p是个概率值</p>
<script type="math/tex; mode=display">g(z) = \frac{1}{1 + e^{-z}}</script><script type="math/tex; mode=display">\begin{align}
g^\prime(z) = &(\frac{1}{1 + e^{-z}})^\prime = \frac{e^{-z}}{(1 + e^{-z})^2}\\
&=\frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{(1 + e^{-z})} = \frac{1}{1 + e^{-z}} \cdot (1 - \frac{1}{1 + e^{-z}})\\
&=g(z) \cdot (1 - g(z))\\
\end{align}</script><script type="math/tex; mode=display">y = \begin{cases}0\\
1\end{cases}</script><script type="math/tex; mode=display">\hat{y} = \begin{cases}1, P(\hat{y} = 1) > p\\
0, P(\hat{y} = 0) > p\end{cases}</script><p><img src="/机器学习/WechatIMG26.png" width="300px"></p>
<h3 id="Logistic回归及似然函数"><a href="#Logistic回归及似然函数" class="headerlink" title="Logistic回归及似然函数"></a>Logistic回归及似然函数</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y=1</th>
<th>y=0</th>
</tr>
</thead>
<tbody>
<tr>
<td>$p(y \mid x)$</td>
<td>$\theta$</td>
<td>$1 - \theta$</td>
</tr>
</tbody>
</table>
</div>
<p>假设: </p>
<script type="math/tex; mode=display">P(y=1|x;\theta) = h_\theta(x)</script><script type="math/tex; mode=display">P(y=0|x;\theta) = 1 - h_\theta(x)</script><script type="math/tex; mode=display">P(y|x;\theta) = h_\theta(x) \ast (1 - h_\theta(x)) = (h_\theta(x))^y(1 - h_\theta(x))^{(1-y)}</script><p>似然函数:</p>
<script type="math/tex; mode=display">\begin{align}L(\theta) &= p(\overrightarrow y \mid X;\theta) = \displaystyle \prod_{i=1}^{m}{p(y^{(i)}|x^{(i)}; \theta)}\\
&=\displaystyle \prod_{i=1}^{m} (h_\theta(x^{(i)}))^y{^{(i)}}(1 - h_\theta(x^{(i)}))^{(1-y^{(i)})}\\
\end{align}</script><p>最大似然/极大似然函数的随机梯度<br>对数似然函数为:</p>
<script type="math/tex; mode=display">l(\theta) = logL(\theta) = \displaystyle \sum_{i=1}^{m}(y^{(i)}log h_\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)})))</script><script type="math/tex; mode=display">\begin{align}\frac{\partial l(\theta)}{\partial \theta_j} &= \displaystyle \sum_{i=1}^{m}(\frac{y^{(i)}}{h_\theta(x^{(i)})} - \frac{1 - y^{(i)}}{1 - h_\theta(x^{(i)}) }) \cdot \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j}\\
&= \displaystyle \sum_{i=1}^{m}(\frac{y^{(i)}}{g(\theta^Tx^{(i)})} - \frac{1 - y^{(i)}}{1 - g(\theta^Tx^{(i)}) }) \cdot \frac{\partial g(\theta^Tx^{(i)})}{\partial \theta_j} \\
&= \displaystyle \sum_{i=1}^{m}(\frac{y^{(i)}}{g(\theta^Tx^{(i)})} - \frac{1 - y^{(i)}}{1 - g(\theta^Tx^{(i)}) }) \cdot g(\theta^Tx^{(i)})(1 - g(\theta^Tx^{(i)})) \cdot \frac{\partial \theta^Tx^{(i)}}{\partial \theta_j} \\
&=\displaystyle \sum_{i=1}^{m}(y^{(i)}(1 - g(\theta^Tx^{(i)}) - g(\theta^Tx^{(i)})(1 - y^{(i)})) \cdot \frac{\partial \theta^Tx^{(i)}}{\partial \theta_j}\\
&=\displaystyle \sum_{i=1}^{m}(y^{(i)}(1 - g(\theta^Tx^{(i)}) - g(\theta^Tx^{(i)})(1 - y^{(i)})) \cdot x_{j}^{(i)}\\
&=\displaystyle \sum_{i=1}^{m}(y^{(i)} - g(\theta^Tx^{(i)})) \cdot x_{j}^{(i)}\\
\end{align}</script><h3 id="Logistic回归-theta-参数求解"><a href="#Logistic回归-theta-参数求解" class="headerlink" title="Logistic回归$\theta$参数求解"></a>Logistic回归$\theta$参数求解</h3><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha \displaystyle \sum_{i=1}^{m}(y^{(i)} - g(\theta^Tx^{(i)})) \cdot x_{j}^{(i)}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha (y^{(i)} - g(\theta^Tx^{(i)})) \cdot x_{j}^{(i)}</script><h3 id="极大似然估计与Logistic回归损失函数"><a href="#极大似然估计与Logistic回归损失函数" class="headerlink" title="极大似然估计与Logistic回归损失函数"></a>极大似然估计与Logistic回归损失函数</h3><script type="math/tex; mode=display">L(\theta) = \displaystyle \prod_{i=1}^{m}{p(y^{(i)}|x^{(i)}; \theta)} = \displaystyle \prod_{i=1}^{m}{p_{i}^{y^{(i)}}(1-p_i)^{(1-y^{(i)})}}, \quad p_i = h_\theta(x^{(i)}) = \frac{1}{1 + e^{\displaystyle - \theta^{T} x^{(i)}}}</script><script type="math/tex; mode=display">l(\theta) = lnL(\theta) = \displaystyle \sum_{i=1}^{m} ln[p_{i}^{y^{(i)}}(1-p_i)^{(1-y^{(i)})}]</script><script type="math/tex; mode=display">\begin{align}loss &= - l(\theta)\\
&= - \displaystyle \sum_{i=1}^{m}[y^{(i)}ln(p_i) + (1-y^{(i)})ln(1-p_i)] \\
&= \displaystyle \sum_{i=1}^{m}[- y^{(i)}ln(h_\theta(x^{(i)})) - (1-y^{(i)})ln(1-h_\theta(x^{(i)}))] \\
\end{align}</script><h3 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h3><ul>
<li>softmax回归是logistic回归的一般化, 适用于K分类问题, 第K类的参数为向量$\theta_k$, 组成的二维矩阵为$\theta_{k \ast n}$</li>
<li>softmax函数的本质是将一个K维的任意实数向量压缩(映射)成另一个K维的实数向量, 其中向量中的每个元素取值都介于(0, 1)之间</li>
<li>softmax回归概率函数为:<script type="math/tex; mode=display">p(y=k|x;\theta) = \displaystyle \frac{e^{\theta_{k}^{T}x}}{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x}}}, \quad k = 1, 2, ..., K</script></li>
</ul>
<h3 id="softmax算法原理"><a href="#softmax算法原理" class="headerlink" title="softmax算法原理"></a>softmax算法原理</h3><script type="math/tex; mode=display">p(y=k|x;\theta) = \frac{e^{\theta_{k}^{T}x}}{\displaystyle \sum_{l=1}^{K}{e^{\theta_{l}^{T}x}}}, \quad k = 1, 2, ..., K</script><script type="math/tex; mode=display">h_\theta(x) = 
\begin{bmatrix}
p(y^{(i)}=1|x^{(i)}; \theta)\\
p(y^{(i)}=2|x^{(i)}; \theta)\\
{\vdots}\\
p(y^{(i)}=k|x^{(i)}; \theta)\\
\end{bmatrix} = \frac{1}{\displaystyle \sum_{j=1}^{k}{e^{\theta_{j}^{T}x^{(i)}}}}
\begin{bmatrix}
e^{\theta_{1}^{T}x}\\
e^{\theta_{2}^{T}x}\\
{\vdots}\\
e^{\theta_{k}^{T}x}\\
\end{bmatrix} \Rightarrow \theta = \begin{bmatrix}
{\theta_{11}}&{\theta_{12}}&{\cdots}&{\theta_{1n}}\\
{\theta_{21}}&{\theta_{22}}&{\cdots}&{\theta_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{\theta_{k1}}&{\theta_{k2}}&{\cdots}&{\theta_{kn}}\\
\end{bmatrix}</script><h3 id="softmax算法损失函数"><a href="#softmax算法损失函数" class="headerlink" title="softmax算法损失函数"></a>softmax算法损失函数</h3><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^{m} \sum_{j=1}^{k} I(y^{(i)}=j) ln(\displaystyle \frac{e^{\theta_{j}^{T}x^{(i)}}}{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}}}}), \quad I(y^{(i)} = j) = \begin{cases}
1, \quad y^{(i)}=j\\
0, \quad y^{(i)} \ne j\\
\end{cases}</script><h3 id="softmax算法梯度下降法求解"><a href="#softmax算法梯度下降法求解" class="headerlink" title="softmax算法梯度下降法求解"></a>softmax算法梯度下降法求解</h3><script type="math/tex; mode=display">\begin{align} \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j}[- I(y^{(i)}=j) ln(\displaystyle \frac{\displaystyle e^{\theta_{j}^{T}x^{(i)}}}{ \displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}} } })]\\
&= \frac{\partial}{\partial \theta_j} [- I(y^{(i)}=j) (\theta_{j}^{T}x^{(i)} - ln(\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}} }))]\\
&= - I(y^{(i)}=j) (x^{(i)} - \frac{ e^{\theta_{j}^{T}x^{(i)}} \cdot x^{(i)} }{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}}}})\\
&= - I(y^{(i)}=j) (1 - \frac{ e^{\theta_{j}^{T}x^{(i)}} }{\displaystyle \sum_{l=1}^{k}{e^{\theta_{l}^{T}x^{(i)}}}}) x^{(i)}   \\
\end{align}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha \displaystyle \sum_{i=1}^{m} I(y^{(i)}=j)(1 - p(y^{(i)}=j|x^{(i)};\theta))x^{(i)}</script><script type="math/tex; mode=display">\theta_j = \theta_j + \alpha I(y^{(i)}=j)(1 - p(y^{(i)}=j|x^{(i)};\theta))x^{(i)}</script><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><ul>
<li>线性模型一般用于回归问题, Logistic和 softmax模型一般用于分类问题</li>
<li>求$\theta$的主要方式是梯度下降法, 梯度下降法是参数优化的重要手段, 主要是SGD, 适用于在线学习以及跳出局部极小值</li>
<li>Logistic和softmax回归是实践中解决分类问题的最重要的方法</li>
<li>广义线性模型对样本要求不必要服从正态分布, 只需要服从指数分布簇(二项分布, 泊松分布, 伯努利分布, 指数分布即可); 广义线性模型的自变量可以是连续的也可以是离散的</li>
</ul>
<h3 id="鸢尾花数据"><a href="#鸢尾花数据" class="headerlink" title="鸢尾花数据"></a>鸢尾花数据</h3><p><a href="https://blog.csdn.net/heuguangxu/article/details/80426437" target="_blank" rel="noopener">参考案例</a></p>
<h4 id="ravel和flatten"><a href="#ravel和flatten" class="headerlink" title="ravel和flatten"></a>ravel和flatten</h4><p>ravel是将多维数据转换为一维数据</p>
<h3 id="线性回归-多项式扩展"><a href="#线性回归-多项式扩展" class="headerlink" title="线性回归+多项式扩展"></a>线性回归+多项式扩展</h3><ul>
<li>如果数据本身不是线性关系, 那么直接使用线性回归模型效果不会体太好, 存在欠拟合</li>
<li>数据在低维空间中不是线性关系, 但是如果将数据映射到高维空间的时候, 数据就有可能变成线性关系, 从而就可以使用线性回归</li>
<li>如果映射的维度特别高, 那么数据就会完全变成线性的, 从而训练出来的模型会非常的契合训练数据; 但是实际上的数据可能会和训练数据存在一定的差距, 从而可能会导致模型在其他数据集上的效果不佳, 可能存在过拟合</li>
</ul>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><ul>
<li>表现形式: 模型在训练集上效果非常好, 但是在测试集上效果不好</li>
<li>产生原因: 进入算法模型训练的这个数据集特别契合算法模型, 导致训练出来的模型非常完美, 但是实际的数据一定存在和训练集数据不一致的地方, 这个不一致就导致模型在其他数据集上效果不佳</li>
<li>解决方案: L1-norm和L2-norm</li>
</ul>
<h3 id="Logistic回归-1"><a href="#Logistic回归-1" class="headerlink" title="Logistic回归"></a>Logistic回归</h3><ul>
<li>本质上是一个二分类算法, 计算的是样本X属于一个类别的概率p, 样本属于另外一个类别的概率为1-p</li>
<li>最终认为样本X属于概率最大的那一个类别 <h3 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h3></li>
<li>与Logistic回归的区别在于, softmax是一个多分类算法, 需要计算样本属于某一个类别的概率, 最终认为样本属于概率最大的那一个类别</li>
<li>softmax会为每个类别训练一个参数$\theta$向量, 所以在softmax中需要求解的参数其实是一个由k个向量组成的$\theta$矩阵</li>
</ul>
<h3 id="持久化-管道-模型效果评估-交叉验证"><a href="#持久化-管道-模型效果评估-交叉验证" class="headerlink" title="持久化, 管道, 模型效果评估, 交叉验证"></a>持久化, 管道, 模型效果评估, 交叉验证</h3><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><ul>
<li>理论/原理: 认为”物以类聚, 人以群分”, 相同/近似样本在样本空间中是比较接近的, 所以可以使用和当前样本比较接近的的其他样本的目标属性作为当前样本的预测值</li>
<li>预测规则:<ul>
<li>分类: 多数投票或者加权多数投票</li>
<li>回归: 平均值或者加权平均值</li>
<li>权重一般选择是与距离成反比</li>
</ul>
</li>
<li>相似度度量<br>需要找相似的样本, 认为样本的特征向量在空间中的点之间的距离体现了样本之间的相似性, 越近的点越相似, 一般使用欧几里得距离</li>
<li>寻找最近邻的样本<ul>
<li>暴力的方式, 计算所有样本到当前样本的距离, 然后再获取最近的k个样本</li>
<li>KD-Tree方式, 通过构建KD-Tree, 减少计算量</li>
</ul>
</li>
</ul>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>考虑: 虽然抽样数据在一定程度上体现了样本数据的特性, 所以我们用样本数据模型来预测它的全体数据. 但是数据与数据之间是存在一定的差异性的, 那么可以认为在当前数据集的模型有可能会出现不拟合生产环境中数据的情况 -&gt; 过拟合;在决策树中, 进行划分特征属性选择的时候, 如果选择最优, 表示这个划分在当前数据集上一定是最优的, 但是不一定在全体数据集上最优; 在随机森林中, 如果每个决策树都是选择最优的进行划分的话, 就会导致所有子模型(内部的决策树)很大程度/概率上会使用相同的划分属性进行数据的划分, 就会特别容易导致过拟合, 所以在随机森林中, 选择划分属性的时候一般使用随机的方式选择</p>
<p>如果涉及到权重的话, 一般都是将错误率/正确率/距离/MSE/MAE等指标进行转换得到的</p>
<h3 id="随机森林-Random-Forest-推广算法"><a href="#随机森林-Random-Forest-推广算法" class="headerlink" title="随机森林(Random Forest)推广算法"></a>随机森林(Random Forest)推广算法</h3><p>RF算法在实际应用中具有比较好的特性, 应用也比较广泛, 主要应用在分类, 回归, 特征转换, 异常点检测等, 常见的RF变种算法有:</p>
<ul>
<li>Extra Tree</li>
<li>Totally Random Trees Embedding(TRTE)</li>
<li>Isolation Forest</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/求解二元函数最小值/" rel="next" title="求解二元函数最小值">
                <i class="fa fa-chevron-left"></i> 求解二元函数最小值
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/梯度下降求解二元函数/" rel="prev" title="梯度下降求解二元函数">
                梯度下降求解二元函数 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">64</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习分类维度一"><span class="nav-number">1.</span> <span class="nav-text">机器学习分类维度一</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习分类维度二"><span class="nav-number">2.</span> <span class="nav-text">机器学习分类维度二</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练及测试"><span class="nav-number">3.</span> <span class="nav-text">模型训练及测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ROC"><span class="nav-number">4.</span> <span class="nav-text">ROC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AUC-Area-Under-Curve"><span class="nav-number">4.1.</span> <span class="nav-text">AUC(Area Under Curve)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型评估"><span class="nav-number">4.2.</span> <span class="nav-text">模型评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型评估总结-分类算法评估方式"><span class="nav-number">4.3.</span> <span class="nav-text">模型评估总结_分类算法评估方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据清洗和转换"><span class="nav-number">5.</span> <span class="nav-text">数据清洗和转换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#类型特征转换1-of-k-哑编码"><span class="nav-number">5.1.</span> <span class="nav-text">类型特征转换1-of-k(哑编码)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文本数据抽取"><span class="nav-number">5.2.</span> <span class="nav-text">文本数据抽取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回归算法"><span class="nav-number">6.</span> <span class="nav-text">回归算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#回归算法认知"><span class="nav-number">6.1.</span> <span class="nav-text">回归算法认知</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#似然函数"><span class="nav-number">6.2.</span> <span class="nav-text">似然函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对数似然-目标函数及最小二乘法"><span class="nav-number">6.3.</span> <span class="nav-text">对数似然, 目标函数及最小二乘法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#theta-的求解过程"><span class="nav-number">6.4.</span> <span class="nav-text">$\theta$的求解过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最小二乘法的参数最优解"><span class="nav-number">6.5.</span> <span class="nav-text">最小二乘法的参数最优解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标损失函数-loss-cost-function"><span class="nav-number">6.6.</span> <span class="nav-text">目标损失函数(loss/cost function)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#误差"><span class="nav-number">6.7.</span> <span class="nav-text">误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归的过拟合"><span class="nav-number">6.8.</span> <span class="nav-text">线性回归的过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型效果判断"><span class="nav-number">6.9.</span> <span class="nav-text">模型效果判断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Elastic-Net"><span class="nav-number">6.10.</span> <span class="nav-text">Elastic Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习调参"><span class="nav-number">6.11.</span> <span class="nav-text">机器学习调参</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降算法"><span class="nav-number">6.12.</span> <span class="nav-text">梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#批量梯度下降法-BGD"><span class="nav-number">6.12.1.</span> <span class="nav-text">批量梯度下降法(BGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度下降法-SGD"><span class="nav-number">6.12.2.</span> <span class="nav-text">随机梯度下降法(SGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性回归的扩展"><span class="nav-number">6.12.3.</span> <span class="nav-text">线性回归的扩展</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#局部加权回归"><span class="nav-number">6.13.</span> <span class="nav-text">局部加权回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic回归"><span class="nav-number">7.</span> <span class="nav-text">Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic回归及似然函数"><span class="nav-number">7.1.</span> <span class="nav-text">Logistic回归及似然函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic回归-theta-参数求解"><span class="nav-number">7.2.</span> <span class="nav-text">Logistic回归$\theta$参数求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#极大似然估计与Logistic回归损失函数"><span class="nav-number">7.3.</span> <span class="nav-text">极大似然估计与Logistic回归损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax回归"><span class="nav-number">7.4.</span> <span class="nav-text">Softmax回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax算法原理"><span class="nav-number">7.5.</span> <span class="nav-text">softmax算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax算法损失函数"><span class="nav-number">7.6.</span> <span class="nav-text">softmax算法损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax算法梯度下降法求解"><span class="nav-number">7.7.</span> <span class="nav-text">softmax算法梯度下降法求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary"><span class="nav-number">7.8.</span> <span class="nav-text">summary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#鸢尾花数据"><span class="nav-number">7.9.</span> <span class="nav-text">鸢尾花数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ravel和flatten"><span class="nav-number">7.9.1.</span> <span class="nav-text">ravel和flatten</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归-多项式扩展"><span class="nav-number">7.10.</span> <span class="nav-text">线性回归+多项式扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合"><span class="nav-number">7.11.</span> <span class="nav-text">过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic回归-1"><span class="nav-number">7.12.</span> <span class="nav-text">Logistic回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax回归"><span class="nav-number">7.13.</span> <span class="nav-text">softmax回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#持久化-管道-模型效果评估-交叉验证"><span class="nav-number">7.14.</span> <span class="nav-text">持久化, 管道, 模型效果评估, 交叉验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN"><span class="nav-number">7.15.</span> <span class="nav-text">KNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树"><span class="nav-number">8.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集成学习"><span class="nav-number">9.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林-Random-Forest-推广算法"><span class="nav-number">9.1.</span> <span class="nav-text">随机森林(Random Forest)推广算法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
